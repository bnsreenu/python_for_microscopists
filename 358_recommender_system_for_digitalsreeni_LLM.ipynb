{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnsreenu/python_for_microscopists/blob/master/358_recommender_system_for_digitalsreeni_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/kWwLVwNwW1k"
      ],
      "metadata": {
        "id": "OHp7j-Z1xrdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an Educational Video Knowledge Graph with LLMs and Embeddings"
      ],
      "metadata": {
        "id": "ypyVWqJQV2DU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project creates an intelligent knowledge graph for educational video recommendations by combining Large Language Models (LLMs) and semantic embeddings. The system extracts key concepts, difficulty levels, prerequisites, and learning outcomes from educational videos, builds meaningful relationships between them, and enables semantic search and personalized learning path generation. By leveraging both the structured information extraction capabilities of LLMs and the semantic similarity detection of embeddings, we create a powerful hybrid approach that provides robust, contextually relevant video recommendations even for complex or ambiguous queries.\n",
        "<p>\n",
        "The system processes video metadata through an LLM to extract rich structured information, generates vector embeddings for semantic search, builds a knowledge graph with meaningful relationships, and provides multiple query methods including semantic search, LLM-based understanding, and pattern matching as a fallback. This approach significantly improves upon traditional NLP methods (e.g., using spaCy) by offering deeper contextual understanding, concept expansion, and robust query handling."
      ],
      "metadata": {
        "id": "LVfO09Y_V8Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation: Getting the Colab system ready"
      ],
      "metadata": {
        "id": "ylqc5Oo2erqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change run time to GPU and verify CUDA version\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "3nsQWeOke3XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google drive to save data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8BLIFRDp-7Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Install llama - this version works with Colab\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.38"
      ],
      "metadata": {
        "id": "yHHfPHLYe_T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install other dependencies\n",
        "!pip install sentence-transformers networkx pyvis"
      ],
      "metadata": {
        "id": "rWfKA5_cMS2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download appropriate llama model and save to Google drive for future use**\n",
        "\n",
        "This is a one time action, so comment it out after initial download"
      ],
      "metadata": {
        "id": "4Z1_P_V7Wh5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### NOT PREFERRED (if you don;t have Huggingface authentication)\n",
        "# Download Llama model (smaller quantized version works well on Colab). The following one may require authentication.\n",
        "#!wget -c https://huggingface.co/TheBloke/Llama-3-8B-Instruct-GGUF/resolve/main/llama-3-8b-instruct.Q4_K_S.gguf -O models/llama-3-8b-instruct.gguf\n",
        "\n",
        "### PREFERRED #######\n",
        "# Download an alternative model that doesn't require authentication\n",
        "#!wget -c https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf -O /content/drive/MyDrive/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "#I have also experimented with tinyllama model but it consistently failed at understanding the command to create json, the way we instructed the model."
      ],
      "metadata": {
        "id": "K13avXzKOMAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Import modules"
      ],
      "metadata": {
        "id": "SnBpU3RNegHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import sqlite3\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Set\n",
        "from pyvis.network import Network\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "\n",
        "# Create output directories\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/data/knowledge_graph_results\"\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"visualizations\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"database\"), exist_ok=True)"
      ],
      "metadata": {
        "id": "-kKOLQCkH8I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Model Setup and Initialization\n",
        "\n",
        "The first step in the knowledge graph creation process powered by LLM involves setting up GPU for LLM and initializing the LLM and embedding models. Here, we use Mistral LLM from llama-cpp-python which is about 4GB and suitable for Colab. Then we initialize the model with context window, batch size, threads, etc. Let us also initialize the embeddings model from sentence-transformers which we need later for semantic search and finding conceptually related videos without exact keyword matches."
      ],
      "metadata": {
        "id": "9O0C2wB5XD-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def setup_llm_gpu():\n",
        "    \"\"\"\n",
        "    Install and setup llama-cpp-python with proper GPU support\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    print(\"Setting up LLM with GPU support...\")\n",
        "\n",
        "    # Check GPU availability\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "    if gpu_available:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"GPU detected: {gpu_name}\")\n",
        "\n",
        "        # Uninstall existing llama-cpp-python\n",
        "        subprocess.run(\"pip uninstall -y llama-cpp-python\", shell=True)\n",
        "\n",
        "        # Install with CUDA support\n",
        "        if torch.cuda.get_device_capability()[0] >= 7:  # For newer GPUs (Compute capability ≥ 7.0)\n",
        "            print(\"Installing llama-cpp-python with CUDA support (optimized for modern GPUs)...\")\n",
        "            subprocess.run(\n",
        "                \"CMAKE_ARGS=\\\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all\\\" pip install llama-cpp-python==0.2.38\",\n",
        "                shell=True\n",
        "            )\n",
        "        else:\n",
        "            print(\"Installing llama-cpp-python with basic CUDA support...\")\n",
        "            subprocess.run(\n",
        "                \"CMAKE_ARGS=\\\"-DLLAMA_CUBLAS=on\\\" pip install llama-cpp-python==0.2.38\",\n",
        "                shell=True\n",
        "            )\n",
        "\n",
        "        clear_output()\n",
        "        print(\"✅ llama-cpp-python installed with GPU support\")\n",
        "    else:\n",
        "        print(\"⚠️ No GPU detected. Performance will be limited.\")\n",
        "\n",
        "    # Set environment variables for optimizing GPU memory usage\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "    # Verify installation\n",
        "    from llama_cpp import Llama\n",
        "    print(\"LLM setup complete. Ready to initialize model.\")\n",
        "\n",
        "\n",
        "def initialize_llm(model_path=None, model_type=\"mistral\", use_gpu=True):\n",
        "    \"\"\"\n",
        "    Initialize the Llama model with proper GPU acceleration\n",
        "\n",
        "    \"\"\"\n",
        "    from llama_cpp import Llama\n",
        "\n",
        "    # Define default model paths\n",
        "    model_paths = {\n",
        "        \"mistral\": \"/content/drive/MyDrive/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
        "        \"llama\": \"/content/drive/MyDrive/models/llama-3-8b-instruct.Q4_K_S.gguf\",\n",
        "        \"tiny\": \"/content/drive/MyDrive/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
        "    }\n",
        "\n",
        "    # Use provided path or default based on model_type\n",
        "    if model_path is None:\n",
        "        model_path = model_paths.get(model_type)\n",
        "        if model_path is None:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    print(f\"Initializing {model_type} model...\")\n",
        "\n",
        "    # Configure GPU usage\n",
        "    gpu_available = torch.cuda.is_available()\n",
        "    if gpu_available and use_gpu:\n",
        "        n_gpu_layers = -1  # Use all layers on GPU\n",
        "        print(\"Using GPU acceleration for model inference\")\n",
        "    else:\n",
        "        n_gpu_layers = 0\n",
        "        print(\"Using CPU only for model inference\")\n",
        "\n",
        "    # Handle model file existence\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "    # Initialize model with optimized settings\n",
        "    try:\n",
        "        llm = Llama(\n",
        "            model_path=model_path,\n",
        "            n_ctx=4096,             # Larger context window\n",
        "            n_batch=512,            # Optimized batch size\n",
        "            n_threads=8,            # More CPU threads\n",
        "            n_gpu_layers=n_gpu_layers,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Test the model with a quick query\n",
        "        print(\"Testing model...\")\n",
        "        start_time = time.time()\n",
        "        result = llm(\"Hello from DigitalSreeni!\", max_tokens=20)\n",
        "        end_time = time.time()\n",
        "        print(f\"Model responded in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing model: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def initialize_embedding_model():\n",
        "    \"\"\"\n",
        "    Initialize the sentence transformer model for embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Initializing embedding model...\")\n",
        "    try:\n",
        "        # Using an efficient model for sentence embeddings\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        print(\"✅ Embedding model loaded successfully\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing embedding model: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "IagGtl9pXRu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Loading\n",
        "\n",
        "Next, we need to define a function to load data from the CSV file. Here, we will also clean up data to convert the video duration from milliseconds to minutes and also standardize text fields. This prepares our dataset for processing by ensuring consistent formatting and appropriate units for duration measurements."
      ],
      "metadata": {
        "id": "kXZlBcKmXg0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video_data(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load and preprocess video data from CSV - return pandas dataframe\n",
        "    Clean up time and standardize text fields.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"Loading video data from {csv_path}...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Convert duration from milliseconds to minutes\n",
        "    if 'Approx Duration (ms)' in df.columns:\n",
        "        df['duration_minutes'] = df['Approx Duration (ms)'] / (1000 * 60)\n",
        "\n",
        "    # Convert timestamp to datetime if present\n",
        "    if 'Video Publish Timestamp' in df.columns:\n",
        "        df['publish_date'] = pd.to_datetime(df['Video Publish Timestamp'])\n",
        "\n",
        "    # Clean and standardize text fields\n",
        "    text_columns = ['Video Title (Original)', 'Video Description (Original)']\n",
        "    for col in text_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('').astype(str)\n",
        "\n",
        "    print(f\"Loaded {len(df)} videos\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "4agtm4tvXoN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Entity Extraction\n",
        "\n",
        "With our data loaded, we now extract key entities from each video's metadata. Helper functions extract topics from titles, determine the specific main topic, and infer difficulty levels. The main extraction function uses the LLM to analyze each video's title and description, extracting structured information including main topics, difficulty level, prerequisites, and learning outcomes. This gives us a rich understanding of what each video teaches and its place in a learning sequence."
      ],
      "metadata": {
        "id": "aEe25quZXsPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_topics_from_title(title: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract potential topics from video title\n",
        "    \"\"\"\n",
        "    # Remove numbers and common words\n",
        "    cleaned_title = re.sub(r'^\\d+\\s*-\\s*', '', title.lower())\n",
        "    cleaned_title = re.sub(r'what is|how to|learn|basics of|introduction to', '', cleaned_title)\n",
        "\n",
        "    # Split into words and remove stop words\n",
        "    words = re.findall(r'\\b[a-z]{3,}\\b', cleaned_title)\n",
        "    stop_words = {'and', 'the', 'for', 'with', 'this', 'that', 'from', 'using', 'your', 'you', 'can', 'will', 'are'}\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "\n",
        "    # Try to extract noun phrases (2-3 word combinations)\n",
        "    phrases = []\n",
        "    for i in range(len(words)-1):\n",
        "        phrases.append(f\"{words[i]} {words[i+1]}\")\n",
        "\n",
        "    for i in range(len(words)-2):\n",
        "        phrases.append(f\"{words[i]} {words[i+1]} {words[i+2]}\")\n",
        "\n",
        "    # Combine words and phrases, with preference to phrases\n",
        "    topics = []\n",
        "    if phrases:\n",
        "        topics.extend(phrases[:2])  # Add top 2 phrases if available\n",
        "\n",
        "    # Add individual words to reach at least 3 topics\n",
        "    while len(topics) < 3 and words:\n",
        "        word = words.pop(0)\n",
        "        if not any(word in topic for topic in topics):\n",
        "            topics.append(word)\n",
        "\n",
        "    # If we still don't have enough topics, add some general ones based on common patterns\n",
        "    if \"python\" in title.lower():\n",
        "        if len(topics) < 3:\n",
        "            topics.append(\"python programming\")\n",
        "    if \"image\" in title.lower():\n",
        "        if len(topics) < 3:\n",
        "            topics.append(\"image processing\")\n",
        "\n",
        "    # Ensure at least one topic\n",
        "    if not topics:\n",
        "        topics = [\"programming concepts\"]\n",
        "\n",
        "    return topics[:3]  # Return up to 3 topics\n",
        "\n",
        "def extract_topic_from_title(title: str, position: int = 0) -> str:\n",
        "    \"\"\"\n",
        "    Extract a specific topic from the title based on position\n",
        "    \"\"\"\n",
        "    topics = extract_topics_from_title(title)\n",
        "    if position < len(topics):\n",
        "        return topics[position]\n",
        "    elif topics:\n",
        "        return topics[0]\n",
        "    else:\n",
        "        return \"programming concepts\"\n",
        "\n",
        "def infer_difficulty(title: str) -> str:\n",
        "    \"\"\"\n",
        "    Infer difficulty level from title\n",
        "    (beginner, intermediate, or advanced)\n",
        "    \"\"\"\n",
        "    title_lower = title.lower()\n",
        "\n",
        "    # Check for explicit indicators\n",
        "    if any(word in title_lower for word in ['introduction', 'basics', 'beginner', 'what is', 'getting started']):\n",
        "        return \"beginner\"\n",
        "    elif any(word in title_lower for word in ['advanced', 'expert', 'complex', 'mastering']):\n",
        "        return \"advanced\"\n",
        "\n",
        "    # Check for number in series\n",
        "    number_match = re.search(r'^(\\d+)', title_lower)\n",
        "    if number_match:\n",
        "        num = int(number_match.group(1))\n",
        "        if num <= 10:\n",
        "            return \"beginner\"\n",
        "        elif num <= 20:\n",
        "            return \"intermediate\"\n",
        "        else:\n",
        "            return \"advanced\"\n",
        "\n",
        "    # Default\n",
        "    return \"intermediate\"\n",
        "\n",
        "\n",
        "def create_batch_prompts(video_data: pd.DataFrame, batch_size: int = 3) -> List[Tuple[List[int], str]]:\n",
        "    \"\"\"\n",
        "    Create batched prompts for more efficient processing\n",
        "    \"\"\"\n",
        "    batched_prompts = []\n",
        "\n",
        "    for i in range(0, len(video_data), batch_size):\n",
        "        batch_indices = list(range(i, min(i + batch_size, len(video_data))))\n",
        "        batch_data = video_data.iloc[batch_indices]\n",
        "\n",
        "        # Create prompt for the batch with more explicit instructions\n",
        "        prompt = \"You are an educational content analyzer specialized in extracting structured information. Analyze these videos:\\n\\n\"\n",
        "\n",
        "        for j, (_, row) in enumerate(batch_data.iterrows()):\n",
        "            title = row['Video Title (Original)']\n",
        "            description = row.get('Video Description (Original)', '')\n",
        "\n",
        "            # Limit description length to avoid exceeding context window\n",
        "            if description and len(description) > 500:\n",
        "                description = description[:500] + \"...\"\n",
        "\n",
        "            prompt += f\"VIDEO {j+1}:\\n\"\n",
        "            prompt += f\"TITLE: {title}\\n\"\n",
        "            prompt += f\"DESCRIPTION: {description}\\n\\n\"\n",
        "\n",
        "        prompt += \"\"\"For EACH video above, you must identify:\n",
        "1. Main topics: Extract 3-5 key concepts covered in the video\n",
        "2. Difficulty level: Classify as exactly one of: \"beginner\", \"intermediate\", or \"advanced\"\n",
        "3. Prerequisites: List 1-3 concepts a viewer should understand before watching\n",
        "4. Learning outcomes: List 2-4 specific skills or knowledge the viewer will gain\n",
        "\n",
        "IMPORTANT: Your response must be a valid JSON array with the following exact structure:\n",
        "[\n",
        "  {\n",
        "    \"video\": 1,\n",
        "    \"main_topics\": [\"topic1\", \"topic2\", \"topic3\"],\n",
        "    \"difficulty\": \"beginner\",\n",
        "    \"prerequisites\": [\"prereq1\", \"prereq2\"],\n",
        "    \"learning_outcomes\": [\"outcome1\", \"outcome2\"]\n",
        "  },\n",
        "  {\n",
        "    \"video\": 2,\n",
        "    \"main_topics\": [\"topic1\", \"topic2\", \"topic3\"],\n",
        "    \"difficulty\": \"intermediate\",\n",
        "    \"prerequisites\": [\"prereq1\", \"prereq2\"],\n",
        "    \"learning_outcomes\": [\"outcome1\", \"outcome2\"]\n",
        "  }\n",
        "]\n",
        "\n",
        "Make sure you include all videos (numbered 1 through \"\"\" + str(len(batch_data)) + \"\"\") and all required fields.\n",
        "Always maintain valid JSON structure. Your entire response must be a valid JSON array.\n",
        "\"\"\"\n",
        "\n",
        "        batched_prompts.append((batch_indices, prompt))\n",
        "\n",
        "    return batched_prompts\n",
        "\n",
        "\n",
        "def extract_video_entities(llm, video_data: pd.DataFrame, process_in_batches: bool = False, batch_size: int = 3, show_first_video_llm_output: bool = False) -> Dict[int, Dict]:\n",
        "    \"\"\"\n",
        "    Extract entities from video data using LLM, with option for individual or batch processing\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    if process_in_batches:\n",
        "        # Process in batches (less reliable but potentially faster)\n",
        "        batched_prompts = create_batch_prompts(video_data, batch_size)\n",
        "        results = extract_entities_batch(llm, video_data, batched_prompts)\n",
        "    else:\n",
        "        # Process videos one by one (more reliable)\n",
        "        total_videos = len(video_data)\n",
        "\n",
        "        for idx in tqdm(range(total_videos), desc=\"Processing videos\"):\n",
        "            try:\n",
        "                print(f\"\\nProcessing video {idx+1}/{total_videos}: {video_data.iloc[idx]['Video Title (Original)'][:30]}...\")\n",
        "\n",
        "                # Create prompt for single video with template for structured completion\n",
        "                title = video_data.iloc[idx]['Video Title (Original)']\n",
        "                description = video_data.iloc[idx].get('Video Description (Original)', '')\n",
        "\n",
        "                # Limit description length for context window\n",
        "                if len(description) > 500:\n",
        "                    description = description[:500] + \"...\"\n",
        "\n",
        "                # Use a structured prompt with placeholders to help model complete correctly\n",
        "                prompt = f\"\"\"You are an AI trained to extract information from educational videos.\n",
        "\n",
        "For the following video:\n",
        "TITLE: {title}\n",
        "DESCRIPTION: {description}\n",
        "\n",
        "Complete this JSON template by filling in the information between the brackets.\n",
        "Do not change the template structure, only replace the text inside [brackets].\n",
        "When not sure, make your best guess based on the title and description.\n",
        "\n",
        "{{\n",
        "  \"main_topics\": [\n",
        "    \"[topic1]\",\n",
        "    \"[topic2]\",\n",
        "    \"[topic3]\"\n",
        "  ],\n",
        "  \"difficulty\": \"[beginner/intermediate/advanced]\",\n",
        "  \"prerequisites\": [\n",
        "    \"[prerequisite1]\",\n",
        "    \"[prerequisite2]\"\n",
        "  ],\n",
        "  \"learning_outcomes\": [\n",
        "    \"[outcome1]\",\n",
        "    \"[outcome2]\",\n",
        "    \"[outcome3]\"\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Only provide the completed JSON, no additional text.\"\"\"\n",
        "\n",
        "                # Process with LLM\n",
        "                response = llm(\n",
        "                    prompt,\n",
        "                    max_tokens=2000,\n",
        "                    temperature=0.1,\n",
        "                    top_p=0.95,\n",
        "                    stop=[\"```\"]\n",
        "                )\n",
        "\n",
        "                response_text = response[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "                # Print LLM output for the first video if requested\n",
        "                if idx == 0 and show_first_video_llm_output:\n",
        "                    print(\"\\n==== EXAMPLE LLM OUTPUT FOR FIRST VIDEO ====\")\n",
        "                    print(f\"TITLE: {title}\")\n",
        "                    print(f\"DESCRIPTION: {description[:100]}...\")\n",
        "                    print(\"\\nLLM RESPONSE:\")\n",
        "                    print(response_text)\n",
        "                    print(\"==========================================\\n\")\n",
        "\n",
        "                # Attempt to extract JSON\n",
        "                json_start = response_text.find('{')\n",
        "                json_end = response_text.rfind('}') + 1\n",
        "\n",
        "                if json_start != -1 and json_end > json_start:\n",
        "                    json_str = response_text[json_start:json_end]\n",
        "\n",
        "                    # Replace placeholder values in the template\n",
        "                    json_str = json_str.replace(\"[topic1]\", extract_topic_from_title(title, 0))\n",
        "                    json_str = json_str.replace(\"[topic2]\", extract_topic_from_title(title, 1))\n",
        "                    json_str = json_str.replace(\"[topic3]\", extract_topic_from_title(title, 2))\n",
        "                    json_str = json_str.replace(\"[beginner/intermediate/advanced]\", infer_difficulty(title))\n",
        "                    json_str = json_str.replace(\"[prerequisite1]\", \"basic programming knowledge\")\n",
        "                    json_str = json_str.replace(\"[prerequisite2]\", \"computer basics\")\n",
        "                    json_str = json_str.replace(\"[outcome1]\", f\"understand {extract_topic_from_title(title, 0)}\")\n",
        "                    json_str = json_str.replace(\"[outcome2]\", f\"apply {extract_topic_from_title(title, 0)} techniques\")\n",
        "                    json_str = json_str.replace(\"[outcome3]\", \"solve related problems\")\n",
        "\n",
        "                    # Clean up any remaining placeholders\n",
        "                    json_str = re.sub(r'\\[\"?\\[.*?\\]\"?\\]', '[]', json_str)\n",
        "                    json_str = re.sub(r'\"?\\[.*?\\]\"?', '\"\"', json_str)\n",
        "\n",
        "                    try:\n",
        "                        # Try to parse the cleaned JSON\n",
        "                        video_result = json.loads(json_str)\n",
        "\n",
        "                        # Process the result and remove any remaining brackets\n",
        "                        main_topics = [topic.replace(\"[\", \"\").replace(\"]\", \"\") for topic in video_result.get(\"main_topics\", [])]\n",
        "                        main_topics = [topic for topic in main_topics if topic and not topic.startswith('[') and not topic.endswith(']')]\n",
        "\n",
        "                        difficulty = video_result.get(\"difficulty\", \"intermediate\")\n",
        "                        if difficulty.startswith('[') or difficulty.endswith(']'):\n",
        "                            difficulty = infer_difficulty(title)\n",
        "\n",
        "                        prerequisites = [prereq.replace(\"[\", \"\").replace(\"]\", \"\") for prereq in video_result.get(\"prerequisites\", [])]\n",
        "                        prerequisites = [prereq for prereq in prerequisites if prereq and not prereq.startswith('[') and not prereq.endswith(']')]\n",
        "\n",
        "                        learning_outcomes = [outcome.replace(\"[\", \"\").replace(\"]\", \"\") for outcome in video_result.get(\"learning_outcomes\", [])]\n",
        "                        learning_outcomes = [outcome for outcome in learning_outcomes if outcome and not outcome.startswith('[') and not outcome.endswith(']')]\n",
        "\n",
        "                        # Store the cleaned results\n",
        "                        results[idx] = {\n",
        "                            \"main_topics\": main_topics if main_topics else extract_topics_from_title(title),\n",
        "                            \"difficulty\": difficulty,\n",
        "                            \"prerequisites\": prerequisites if prerequisites else [\"basic programming knowledge\"],\n",
        "                            \"learning_outcomes\": learning_outcomes if learning_outcomes else [f\"understand {extract_topic_from_title(title, 0)}\"]\n",
        "                        }\n",
        "\n",
        "                        print(f\"Successfully processed video {idx}\")\n",
        "\n",
        "                    except json.JSONDecodeError:\n",
        "                        print(f\"Invalid JSON for video {idx}, using title-based extraction\")\n",
        "                        results[idx] = {\n",
        "                            \"main_topics\": extract_topics_from_title(title),\n",
        "                            \"difficulty\": infer_difficulty(title),\n",
        "                            \"prerequisites\": [\"basic programming knowledge\"],\n",
        "                            \"learning_outcomes\": [f\"understand {extract_topic_from_title(title, 0)}\"]\n",
        "                        }\n",
        "\n",
        "                else:\n",
        "                    print(f\"No JSON found in response for video {idx}\")\n",
        "                    print(f\"Response: {response_text[:20]}...\")\n",
        "\n",
        "                    # Extract from title directly\n",
        "                    results[idx] = {\n",
        "                        \"main_topics\": extract_topics_from_title(title),\n",
        "                        \"difficulty\": infer_difficulty(title),\n",
        "                        \"prerequisites\": [\"basic programming knowledge\"],\n",
        "                        \"learning_outcomes\": [f\"understand {extract_topic_from_title(title, 0)}\"]\n",
        "                    }\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing video {idx}: {str(e)}\")\n",
        "\n",
        "                # Add default values based on title analysis\n",
        "                title = video_data.iloc[idx]['Video Title (Original)']\n",
        "                results[idx] = {\n",
        "                    \"main_topics\": extract_topics_from_title(title),\n",
        "                    \"difficulty\": infer_difficulty(title),\n",
        "                    \"prerequisites\": [\"basic programming knowledge\"],\n",
        "                    \"learning_outcomes\": [f\"understand {extract_topic_from_title(title, 0)}\"]\n",
        "                }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "mcjKnk2OXzM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Relationship Extraction\n",
        "\n",
        "After identifying individual video entities, we need to understand how concepts relate to each other. The relationship extraction function uses the LLM to identify meaningful connections between topics, such as when one concept is a prerequisite for another or when concepts build upon each other. We then standardize these relationship types to ensure consistency in our knowledge graph."
      ],
      "metadata": {
        "id": "MAx48ff_YNKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_relationships_with_llm(llm, video_entities: Dict[int, Dict], video_data: pd.DataFrame) -> Dict[str, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Extract relationships between concepts using LLM\n",
        "    \"\"\"\n",
        "    # Collect all unique topics\n",
        "    all_topics = set()\n",
        "    for video_id, data in video_entities.items():\n",
        "        all_topics.update(data.get(\"main_topics\", []))\n",
        "        all_topics.update(data.get(\"prerequisites\", []))\n",
        "\n",
        "    # Convert to list and limit to reasonable number to avoid excessive token usage\n",
        "    topic_list = list(all_topics)\n",
        "    if len(topic_list) > 30:\n",
        "        print(f\"Limiting from {len(topic_list)} to 30 topics for relationship analysis\")\n",
        "        topic_list = topic_list[:30]\n",
        "\n",
        "    # Create prompt for relationship extraction\n",
        "    prompt = f\"\"\"You are an expert in educational content organization.\n",
        "\n",
        "I have extracted the following topics from a series of educational videos:\n",
        "{', '.join(topic_list)}\n",
        "\n",
        "Please identify meaningful relationships between these topics. For each relationship, specify:\n",
        "1. The source topic\n",
        "2. The target topic\n",
        "3. The relationship type (prerequisite_for, builds_upon, related_to, applies)\n",
        "4. The strength of the relationship (a float between 0.1 and 1.0)\n",
        "\n",
        "Only include relationships that truly exist. Not every topic needs to be connected to others.\n",
        "\n",
        "Format your response as a JSON object:\n",
        "{{\n",
        "  \"relationships\": [\n",
        "    {{\n",
        "      \"source\": \"topic1\",\n",
        "      \"target\": \"topic2\",\n",
        "      \"type\": \"prerequisite_for\",\n",
        "      \"strength\": 0.9\n",
        "    }},\n",
        "    ...\n",
        "  ]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    # Call LLM\n",
        "    response = llm(prompt, max_tokens=4000, temperature=0.2)\n",
        "    response_text = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    # Extract JSON\n",
        "    json_start = response_text.find('{')\n",
        "    json_end = response_text.rfind('}') + 1\n",
        "\n",
        "    if json_start == -1 or json_end == 0:\n",
        "        print(\"Failed to get valid JSON for relationships\")\n",
        "        return {\"relationships\": []}\n",
        "\n",
        "    json_str = response_text[json_start:json_end]\n",
        "\n",
        "    try:\n",
        "        relationships_data = json.loads(json_str)\n",
        "        return relationships_data\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Invalid JSON for relationships\")\n",
        "        return {\"relationships\": []}\n",
        "\n",
        "\n",
        "def map_relationship_type(rel_type: str) -> str:\n",
        "    \"\"\"\n",
        "    Map relationship types from LLM to standard edge types\n",
        "    \"\"\"\n",
        "    rel_type = rel_type.lower()\n",
        "\n",
        "    if rel_type in ['prerequisite_for', 'prerequisite']:\n",
        "        return 'prerequisite_for'\n",
        "    elif rel_type in ['builds_upon', 'builds on', 'extends']:\n",
        "        return 'builds_upon'\n",
        "    elif rel_type in ['related_to', 'related']:\n",
        "        return 'related'\n",
        "    elif rel_type in ['applies', 'uses', 'implements']:\n",
        "        return 'applies'\n",
        "    else:\n",
        "        return 'related'"
      ],
      "metadata": {
        "id": "PKuI6NFaYSGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Knowledge Graph Construction\n",
        "\n",
        "Now we build the actual knowledge graph structure using NetworkX. The main graph building function creates nodes for each video with all extracted metadata as attributes. Then we add connections between videos based on shared topics, prerequisites, and the concept relationships identified in the previous step. This creates a rich network of interconnected educational content.\n",
        "\n",
        "**NOte about Adding Video Connections**\n",
        "\n",
        "One of the most critical aspects of our knowledge graph system is how we establish meaningful connections between videos. In the add_video_connections function, we create three distinct types of connections: First, we identify shared topics between videos and connect them based on similarity, considering difficulty levels to establish prerequisite relationships (easier content connects to more advanced content). Second, we analyze explicit prerequisites by checking if topics covered in one video match prerequisites listed in another, creating strong \"explicit_prerequisite\" connections. Finally, we apply concept relationships identified by the LLM to further strengthen connections between videos covering related concepts. This multi-layered approach creates a rich network of relationships that reflects both content similarity and logical learning progressions."
      ],
      "metadata": {
        "id": "2D3DitTmYecq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Note: We are only adding videos as nodes. Ideally you also add topic nodes.\n",
        "def build_llm_knowledge_graph(video_data: pd.DataFrame, video_entities: Dict[int, Dict], relationships: Dict) -> nx.DiGraph:\n",
        "    \"\"\"\n",
        "    Build knowledge graph with LLM-extracted entities and relationships\n",
        "    \"\"\"\n",
        "    print(\"Building knowledge graph...\")\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add only video nodes - no separate topic nodes\n",
        "    for idx, row in video_data.iterrows():\n",
        "        if idx not in video_entities:\n",
        "            continue\n",
        "\n",
        "        entities = video_entities[idx]\n",
        "\n",
        "        # Add video node with all metadata as attributes\n",
        "        G.add_node(\n",
        "            idx,\n",
        "            title=row['Video Title (Original)'],\n",
        "            description=row.get('Video Description (Original)', ''),\n",
        "            difficulty=entities.get('difficulty', 'intermediate').lower(),  # Ensure lowercase\n",
        "            duration=row.get('Approx Duration (ms)', 0),\n",
        "            duration_minutes=row.get('duration_minutes', 0),\n",
        "            topics=entities.get('main_topics', []),\n",
        "            prerequisites=entities.get('prerequisites', []),\n",
        "            learning_outcomes=entities.get('learning_outcomes', []),\n",
        "            node_type='video'  # Keep track that this is a video node\n",
        "        )\n",
        "\n",
        "    # Add connections between videos based on shared topics and prerequisites\n",
        "    add_video_connections(G, relationships)  #This function is defined below.\n",
        "\n",
        "    print(f\"Knowledge graph built with {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
        "    return G\n",
        "\n",
        "\n",
        "def add_video_connections(G: nx.DiGraph, relationships: Dict) -> None:\n",
        "    \"\"\"\n",
        "    Add connections between videos based on shared topics, prerequisites, and difficulty\n",
        "    \"\"\"\n",
        "    video_nodes = list(G.nodes())\n",
        "\n",
        "    # Define standard difficulty levels for comparison\n",
        "    diff_levels = ['beginner', 'intermediate', 'advanced']\n",
        "\n",
        "    # First pass: Calculate topic similarity between videos\n",
        "    for i, v1 in enumerate(video_nodes):\n",
        "        for v2 in video_nodes[i+1:]:\n",
        "            v1_data = G.nodes[v1]\n",
        "            v2_data = G.nodes[v2]\n",
        "\n",
        "            # Calculate shared topics\n",
        "            v1_topics = set(v1_data.get('topics', []))\n",
        "            v2_topics = set(v2_data.get('topics', []))\n",
        "            common_topics = v1_topics.intersection(v2_topics)\n",
        "\n",
        "            if not common_topics:\n",
        "                continue\n",
        "\n",
        "            # Calculate similarity score (Jaccard similarity)\n",
        "            similarity = len(common_topics) / len(v1_topics.union(v2_topics))\n",
        "\n",
        "            # Only connect if there's significant similarity\n",
        "            if similarity < 0.1:\n",
        "                continue\n",
        "\n",
        "            # Get difficulty levels (standardized to lowercase)\n",
        "            v1_diff = v1_data.get('difficulty', 'intermediate').lower()\n",
        "            v2_diff = v2_data.get('difficulty', 'intermediate').lower()\n",
        "\n",
        "            # Map to standard difficulties\n",
        "            if v1_diff not in diff_levels:\n",
        "                v1_diff = 'intermediate'\n",
        "            if v2_diff not in diff_levels:\n",
        "                v2_diff = 'intermediate'\n",
        "\n",
        "            v1_diff_idx = diff_levels.index(v1_diff)\n",
        "            v2_diff_idx = diff_levels.index(v2_diff)\n",
        "\n",
        "            # Determine relationship type based on difficulty\n",
        "            if v1_diff_idx < v2_diff_idx:\n",
        "                # v1 is easier than v2\n",
        "                G.add_edge(\n",
        "                    v1, v2,\n",
        "                    type='prerequisite_for',\n",
        "                    weight=similarity,\n",
        "                    shared_topics=list(common_topics)\n",
        "                )\n",
        "            elif v1_diff_idx > v2_diff_idx:\n",
        "                # v2 is easier than v1\n",
        "                G.add_edge(\n",
        "                    v2, v1,\n",
        "                    type='prerequisite_for',\n",
        "                    weight=similarity,\n",
        "                    shared_topics=list(common_topics)\n",
        "                )\n",
        "            else:\n",
        "                # Same difficulty - connect based on relationship strength\n",
        "                G.add_edge(\n",
        "                    v1, v2,\n",
        "                    type='related',\n",
        "                    weight=similarity,\n",
        "                    shared_topics=list(common_topics)\n",
        "                )\n",
        "\n",
        "    # Second pass: Connect based on explicit prerequisites\n",
        "    for v1 in video_nodes:\n",
        "        v1_data = G.nodes[v1]\n",
        "        v1_prereqs = set(v1_data.get('prerequisites', []))\n",
        "\n",
        "        for v2 in video_nodes:\n",
        "            if v1 == v2:\n",
        "                continue\n",
        "\n",
        "            v2_data = G.nodes[v2]\n",
        "            v2_topics = set(v2_data.get('topics', []))\n",
        "\n",
        "            # If any of v1's prerequisites are in v2's topics, v2 is a prerequisite for v1\n",
        "            prereq_matches = v1_prereqs.intersection(v2_topics)\n",
        "            if prereq_matches:\n",
        "                # Strength based on how many prerequisites match\n",
        "                strength = len(prereq_matches) / len(v1_prereqs) if v1_prereqs else 0.5\n",
        "\n",
        "                G.add_edge(\n",
        "                    v2, v1,\n",
        "                    type='explicit_prerequisite',\n",
        "                    weight=min(1.0, strength + 0.2),  # Boost explicit prerequisites\n",
        "                    matched_prereqs=list(prereq_matches)\n",
        "                )\n",
        "\n",
        "    # Apply concept relationships from LLM to strengthen existing edges\n",
        "    for rel in relationships.get('relationships', []):\n",
        "        source_concept = rel.get('source', '').lower()\n",
        "        target_concept = rel.get('target', '').lower()\n",
        "        rel_type = rel.get('type', '')\n",
        "        rel_strength = rel.get('strength', 0.5)\n",
        "\n",
        "        # Find videos containing these concepts\n",
        "        source_videos = []\n",
        "        target_videos = []\n",
        "\n",
        "        for node in video_nodes:\n",
        "            node_topics = [t.lower() for t in G.nodes[node].get('topics', [])]\n",
        "\n",
        "            if any(source_concept in topic for topic in node_topics):\n",
        "                source_videos.append(node)\n",
        "\n",
        "            if any(target_concept in topic for topic in node_topics):\n",
        "                target_videos.append(node)\n",
        "\n",
        "        # Connect videos based on concept relationships\n",
        "        for s_vid in source_videos:\n",
        "            for t_vid in target_videos:\n",
        "                if s_vid != t_vid:\n",
        "                    # Check if edge already exists\n",
        "                    if G.has_edge(s_vid, t_vid):\n",
        "                        # Update weight if the new relationship is stronger\n",
        "                        current_weight = G.edges[s_vid, t_vid]['weight']\n",
        "                        if rel_strength > current_weight:\n",
        "                            G.edges[s_vid, t_vid]['weight'] = rel_strength\n",
        "                            G.edges[s_vid, t_vid]['type'] = map_relationship_type(rel_type)\n",
        "                    else:\n",
        "                        # Add new edge if it doesn't exist\n",
        "                        G.add_edge(\n",
        "                            s_vid, t_vid,\n",
        "                            type=map_relationship_type(rel_type),\n",
        "                            weight=rel_strength,\n",
        "                            concept_relationship=f\"{source_concept} -> {target_concept}\"\n",
        "                        )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kn1XTLdQYhsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Embedding Generation\n",
        "\n",
        "To enable semantic search, we generate dense vector embeddings for each video using the sentence-transformers model. We create rich text representations that combine all video metadata (title, description, topics, prerequisites, and learning outcomes) to capture the full semantic meaning of each video. These embeddings are then saved to disk for future use, avoiding the need to regenerate them each time."
      ],
      "metadata": {
        "id": "ktEr3bHOYwFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_video_embeddings(embedding_model, G):\n",
        "    \"\"\"\n",
        "    Generate embeddings for all videos in the knowledge graph\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    print(\"Generating embeddings for videos...\")\n",
        "    video_embeddings = {}\n",
        "\n",
        "    # Create texts for embedding\n",
        "    embedding_texts = {}\n",
        "    for node_id in G.nodes():\n",
        "        node_data = G.nodes[node_id]\n",
        "        if node_data.get('node_type') == 'video':\n",
        "            # Create rich text representation including all metadata\n",
        "            title = node_data.get('title', '')\n",
        "            description = node_data.get('description', '')\n",
        "            topics = ' '.join(node_data.get('topics', []))\n",
        "            prerequisites = ' '.join(node_data.get('prerequisites', []))\n",
        "            outcomes = ' '.join(node_data.get('learning_outcomes', []))\n",
        "\n",
        "            # Combine all text data for richer embedding\n",
        "            text_for_embedding = f\"{title}. {description}. Topics: {topics}. Prerequisites: {prerequisites}. Learning outcomes: {outcomes}\"\n",
        "            embedding_texts[node_id] = text_for_embedding\n",
        "\n",
        "    # Generate embeddings in batches to avoid memory issues\n",
        "    batch_size = 32\n",
        "    node_ids = list(embedding_texts.keys())\n",
        "\n",
        "    for i in tqdm(range(0, len(node_ids), batch_size)):\n",
        "        batch_ids = node_ids[i:i+batch_size]\n",
        "        batch_texts = [embedding_texts[node_id] for node_id in batch_ids]\n",
        "\n",
        "        # Generate embeddings\n",
        "        batch_embeddings = embedding_model.encode(batch_texts)\n",
        "\n",
        "        # Store embeddings\n",
        "        for j, node_id in enumerate(batch_ids):\n",
        "            video_embeddings[node_id] = batch_embeddings[j]\n",
        "\n",
        "    print(f\"Generated embeddings for {len(video_embeddings)} videos\")\n",
        "    return video_embeddings\n",
        "\n",
        "def save_embeddings(video_embeddings, filepath):\n",
        "  \"\"\"\n",
        "  Save video embeddings to a file\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  import os\n",
        "  import pickle\n",
        "\n",
        "  # Create directory if it doesn't exist\n",
        "  os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "  # Save embeddings\n",
        "  with open(filepath, 'wb') as f:\n",
        "      pickle.dump(video_embeddings, f)\n",
        "\n",
        "  print(f\"Embeddings saved to {filepath}\")"
      ],
      "metadata": {
        "id": "lSvoxaigYzd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Graph Storage\n",
        "\n",
        "With our knowledge graph built and embeddings generated, we save everything to persistent storage. We save the graph both as a pickle file for easy Python reloading and to an SQLite database for potential integration with other systems. This ensures our processed data is available for future use without having to recreate the graph."
      ],
      "metadata": {
        "id": "AdYZ33M4Y5F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_graph_pickle(G: nx.DiGraph, filepath: str) -> None:\n",
        "    \"\"\"\n",
        "    Save NetworkX graph as a pickle file for later loading using Python's built-in pickle\n",
        "\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Save graph using Python's built-in pickle\n",
        "        import pickle\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(G, f)\n",
        "        print(f\"Knowledge graph saved as pickle to: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving graph pickle: {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "def save_knowledge_graph_to_db(G: nx.DiGraph, db_path='knowledge_graph.db'):\n",
        "    \"\"\"\n",
        "    Save knowledge graph to SQLite database\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
        "\n",
        "    print(f\"Saving knowledge graph to database: {db_path}\")\n",
        "\n",
        "    # Create or connect to database\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Create tables\n",
        "    c.execute('DROP TABLE IF EXISTS nodes')\n",
        "    c.execute('DROP TABLE IF EXISTS edges')\n",
        "\n",
        "    c.execute('''\n",
        "    CREATE TABLE nodes (\n",
        "        id TEXT PRIMARY KEY,\n",
        "        label TEXT,\n",
        "        type TEXT,\n",
        "        title TEXT,\n",
        "        description TEXT,\n",
        "        difficulty TEXT,\n",
        "        duration INTEGER,\n",
        "        topics TEXT,\n",
        "        prerequisites TEXT,\n",
        "        learning_outcomes TEXT\n",
        "    )\n",
        "    ''')\n",
        "\n",
        "    c.execute('''\n",
        "    CREATE TABLE edges (\n",
        "        source TEXT,\n",
        "        target TEXT,\n",
        "        type TEXT,\n",
        "        weight REAL,\n",
        "        PRIMARY KEY (source, target)\n",
        "    )\n",
        "    ''')\n",
        "\n",
        "    # Insert nodes\n",
        "    for node_id in G.nodes():\n",
        "        node_data = G.nodes[node_id]\n",
        "\n",
        "        # Standardize difficulty to lowercase\n",
        "        difficulty = node_data.get('difficulty', '')\n",
        "        if difficulty:\n",
        "            difficulty = difficulty.lower()\n",
        "\n",
        "        # Convert list attributes to JSON\n",
        "        topics = json.dumps(node_data.get('topics', []))\n",
        "        prerequisites = json.dumps(node_data.get('prerequisites', []))\n",
        "        learning_outcomes = json.dumps(node_data.get('learning_outcomes', []))\n",
        "\n",
        "        c.execute('''\n",
        "        INSERT INTO nodes (id, label, type, title, description, difficulty, duration, topics, prerequisites, learning_outcomes)\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        ''', (\n",
        "            str(node_id),\n",
        "            node_data.get('label', ''),\n",
        "            node_data.get('node_type', ''),\n",
        "            node_data.get('title', ''),\n",
        "            node_data.get('description', ''),\n",
        "            difficulty,\n",
        "            node_data.get('duration', 0),\n",
        "            topics,\n",
        "            prerequisites,\n",
        "            learning_outcomes\n",
        "        ))\n",
        "\n",
        "    # Insert edges\n",
        "    for source, target, data in G.edges(data=True):\n",
        "        c.execute('''\n",
        "        INSERT INTO edges (source, target, type, weight)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "        ''', (\n",
        "            str(source),\n",
        "            str(target),\n",
        "            data.get('type', ''),\n",
        "            data.get('weight', 0.0)\n",
        "        ))\n",
        "\n",
        "    # Commit and close\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"Knowledge graph saved to database: {db_path} ({len(G.nodes())} nodes, {len(G.edges())} edges)\")\n",
        "\n",
        "    # Return database path for reference\n",
        "    return db_path"
      ],
      "metadata": {
        "id": "XEgoLf98Y-8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Visualization Functions\n",
        "\n",
        "To understand the structure of our knowledge graph, we create interactive visualizations using PyVis. The main visualization shows all videos and their relationships, while topic-specific visualizations focus on videos related to particular subjects. These visual representations help us better understand the connections between educational content."
      ],
      "metadata": {
        "id": "AOk0tmyxZJw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def visualize_knowledge_graph(G: nx.DiGraph, filename='knowledge_graph.html'):\n",
        "    \"\"\"\n",
        "    Create interactive visualization of the knowledge graph\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'visualizations', filename)\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "    # Create a PyVis network\n",
        "    net = Network(height='750px', width='100%', bgcolor='#ffffff',\n",
        "                 font_color='#000000', directed=True)\n",
        "\n",
        "    # Set physics layout options\n",
        "    net.force_atlas_2based()\n",
        "    net.show_buttons(filter_=['physics'])\n",
        "\n",
        "    # Color mapping for different difficulties\n",
        "    color_map = {\n",
        "        'beginner': '#90EE90',      # light green\n",
        "        'intermediate': '#ADD8E6',   # light blue\n",
        "        'advanced': '#FFB6C1'        # light pink\n",
        "    }\n",
        "\n",
        "    # Add nodes (only video nodes in this version)\n",
        "    for node_id in G.nodes():\n",
        "        node_data = G.nodes[node_id]\n",
        "        title = node_data.get('title', 'Unknown Video')\n",
        "        difficulty = node_data.get('difficulty', 'intermediate').lower()\n",
        "        duration = node_data.get('duration_minutes', 0)\n",
        "        topics = ', '.join(node_data.get('topics', []))\n",
        "\n",
        "        # Use default color if difficulty not in map\n",
        "        if difficulty not in color_map:\n",
        "            difficulty = 'intermediate'\n",
        "\n",
        "        hover_text = f\"\"\"\n",
        "        Title: {title}\n",
        "        Difficulty: {difficulty}\n",
        "        Duration: {duration:.1f} min\n",
        "        Topics: {topics}\n",
        "        \"\"\"\n",
        "\n",
        "        net.add_node(\n",
        "            node_id,\n",
        "            label=title[:20] + \"...\" if len(title) > 20 else title,\n",
        "            title=hover_text,\n",
        "            color=color_map[difficulty],\n",
        "            shape='dot',\n",
        "            size=15\n",
        "        )\n",
        "\n",
        "    # Edge color mapping\n",
        "    edge_colors = {\n",
        "        'prerequisite_for': '#FF0000',       # red\n",
        "        'explicit_prerequisite': '#8B0000',  # dark red\n",
        "        'builds_upon': '#0000FF',            # blue\n",
        "        'related': '#A0A0A0',                # gray\n",
        "        'applies': '#008000'                 # green\n",
        "    }\n",
        "\n",
        "    # Add edges\n",
        "    for edge in G.edges(data=True):\n",
        "        source, target, data = edge\n",
        "        edge_type = data.get('type', 'related')\n",
        "        weight = data.get('weight', 0.5)\n",
        "\n",
        "        # Create descriptive title based on relationship type\n",
        "        if edge_type == 'prerequisite_for' or edge_type == 'explicit_prerequisite':\n",
        "            title = f\"Watch {G.nodes[source]['title']} before {G.nodes[target]['title']}\"\n",
        "        elif edge_type == 'builds_upon':\n",
        "            title = f\"{G.nodes[target]['title']} builds upon {G.nodes[source]['title']}\"\n",
        "        elif edge_type == 'related':\n",
        "            title = f\"Related videos with shared topics: {', '.join(data.get('shared_topics', []))}\"\n",
        "        else:\n",
        "            title = f\"Type: {edge_type}, Weight: {weight:.2f}\"\n",
        "\n",
        "        net.add_edge(\n",
        "            source,\n",
        "            target,\n",
        "            title=title,\n",
        "            color=edge_colors.get(edge_type, '#A0A0A0'),\n",
        "            width=weight * 3,\n",
        "            arrows='to'\n",
        "        )\n",
        "\n",
        "    # Save the network\n",
        "    net.save_graph(output_path)\n",
        "    print(f\"Knowledge graph visualization saved to {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def visualize_topic_subgraph(G: nx.DiGraph, topic: str, filename=None):\n",
        "    \"\"\"\n",
        "    Visualize a subgraph of videos related to a specific topic\n",
        "    \"\"\"\n",
        "    if filename is None:\n",
        "        filename = f'{topic.replace(\" \", \"_\")}_subgraph.html'\n",
        "\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'visualizations', filename)\n",
        "\n",
        "    # Find videos related to this topic\n",
        "    related_videos = []\n",
        "    for node_id in G.nodes():\n",
        "        node_data = G.nodes[node_id]\n",
        "        if topic.lower() in [t.lower() for t in node_data.get('topics', [])]:\n",
        "            related_videos.append(node_id)\n",
        "\n",
        "    if not related_videos:\n",
        "        print(f\"No videos found for topic '{topic}'\")\n",
        "        return None\n",
        "\n",
        "    # Create subgraph with just the related videos and their connections\n",
        "    subgraph = G.subgraph(related_videos)\n",
        "\n",
        "    # Create visualization\n",
        "    net = Network(height='750px', width='100%', bgcolor='#ffffff', directed=True)\n",
        "\n",
        "    # Color map for difficulties\n",
        "    color_map = {\n",
        "        'beginner': '#90EE90',      # light green\n",
        "        'intermediate': '#ADD8E6',   # light blue\n",
        "        'advanced': '#FFB6C1'        # light pink\n",
        "    }\n",
        "\n",
        "    # Add nodes\n",
        "    for node_id in subgraph.nodes():\n",
        "        node_data = subgraph.nodes[node_id]\n",
        "        title = node_data.get('title', 'Unknown')\n",
        "        difficulty = node_data.get('difficulty', 'intermediate').lower()\n",
        "\n",
        "        # Use default color if difficulty not in map\n",
        "        if difficulty not in color_map:\n",
        "            difficulty = 'intermediate'\n",
        "\n",
        "        net.add_node(\n",
        "            node_id,\n",
        "            label=title[:20] + \"...\" if len(title) > 20 else title,\n",
        "            title=f\"Title: {title}\\nDifficulty: {difficulty}\\nTopics: {', '.join(node_data.get('topics', []))}\",\n",
        "            color=color_map[difficulty]\n",
        "        )\n",
        "\n",
        "    # Add edges\n",
        "    for u, v, data in subgraph.edges(data=True):\n",
        "        net.add_edge(\n",
        "            u, v,\n",
        "            title=f\"Type: {data.get('type', 'related')}\\nWeight: {data.get('weight', 0.5):.2f}\",\n",
        "            width=data.get('weight', 0.5) * 3\n",
        "        )\n",
        "\n",
        "    # Save visualization\n",
        "    net.save_graph(output_path)\n",
        "    print(f\"Topic subgraph saved to {output_path}\")\n",
        "\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "MEe5emClZMg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Main Graph Construction Process\n",
        "\n",
        "The main function orchestrates all the previous steps into a complete pipeline. It handles loading data, extracting entities and relationships, building the graph, generating embeddings, saving everything to disk, and creating visualizations. This function serves as the entry point for building the entire knowledge graph system from scratch."
      ],
      "metadata": {
        "id": "jesvOmZnZZRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def main(csv_path, model_type=\"tiny\", use_gpu=True, process_in_batches=False, output_dir=None, max_videos=None, show_first_video_llm_output=False, generate_embeddings=True):\n",
        "    \"\"\"\n",
        "    Main execution function\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    csv_path : str\n",
        "        Path to CSV file with video data\n",
        "    model_type : str\n",
        "        Type of model to use ('mistral', 'llama', or 'tiny')\n",
        "    use_gpu : bool\n",
        "        Whether to use GPU acceleration\n",
        "    process_in_batches : bool\n",
        "        Whether to process videos in batches (Default: False - process one at a time)\n",
        "    output_dir : str\n",
        "        Custom output directory (defaults to /content/drive/MyDrive/data/knowledge_graph_results if None)\n",
        "    max_videos : int, optional\n",
        "        Maximum number of videos to process (processes all if None)\n",
        "    show_first_video_llm_output : bool\n",
        "        Whether to print the LLM output for the first video\n",
        "    generate_embeddings : bool\n",
        "        Whether to generate and save embeddings for semantic search\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Tuple[nx.DiGraph, object, object, Dict]\n",
        "        Knowledge graph, LLM, embedding model, and video embeddings\n",
        "    \"\"\"\n",
        "    # Step 1: Set output directory\n",
        "    global OUTPUT_DIR\n",
        "    if output_dir:\n",
        "        OUTPUT_DIR = output_dir\n",
        "    else:\n",
        "        OUTPUT_DIR = \"/content/drive/MyDrive/data/knowledge_graph_results\"\n",
        "\n",
        "    # Create output directories (may be redundant, delete the ones at the neginning of the file)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"visualizations\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"database\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"graph\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"embeddings\"), exist_ok=True)\n",
        "\n",
        "    print(f\"Results will be saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "    # Step 2: Setup LLM with GPU support\n",
        "    setup_llm_gpu()\n",
        "\n",
        "    # Step 3: Initialize LLM\n",
        "    llm = initialize_llm(model_type=model_type, use_gpu=use_gpu)\n",
        "\n",
        "    # Step 4: Load video data\n",
        "    video_data = load_video_data(csv_path)\n",
        "\n",
        "    # Limit number of videos if specified\n",
        "    if max_videos is not None and max_videos > 0 and max_videos < len(video_data):\n",
        "        print(f\"Limiting processing to first {max_videos} videos (out of {len(video_data)} total)\")\n",
        "        video_data = video_data.iloc[:max_videos].copy()\n",
        "\n",
        "    # Step 5: Process videos individually (more reliable) or in batches\n",
        "    print(\"Extracting entities from videos...\")\n",
        "    video_entities = extract_video_entities(\n",
        "        llm,\n",
        "        video_data,\n",
        "        process_in_batches=process_in_batches,\n",
        "        show_first_video_llm_output=show_first_video_llm_output\n",
        "    )\n",
        "\n",
        "    # Step 6: Extract relationships between concepts\n",
        "    print(\"Extracting relationships between concepts...\")\n",
        "    relationships = extract_relationships_with_llm(llm, video_entities, video_data)\n",
        "\n",
        "    # Step 7: Build knowledge graph\n",
        "    G = build_llm_knowledge_graph(video_data, video_entities, relationships)\n",
        "\n",
        "    # Step 8: Save knowledge graph\n",
        "    db_path = os.path.join(OUTPUT_DIR, 'database', 'knowledge_graph.db')\n",
        "    save_knowledge_graph_to_db(G, db_path)\n",
        "\n",
        "    # Step 9: Save NetworkX graph for direct reloading\n",
        "    graph_path = os.path.join(OUTPUT_DIR, 'graph', 'knowledge_graph.pickle')\n",
        "    save_graph_pickle(G, graph_path)\n",
        "\n",
        "    # Step 10: Generate and save embeddings for semantic search\n",
        "    embedding_model = None\n",
        "    video_embeddings = None\n",
        "\n",
        "    if generate_embeddings:\n",
        "        try:\n",
        "            print(\"Initializing embedding model for semantic search...\")\n",
        "            embedding_model = initialize_embedding_model()\n",
        "\n",
        "            print(\"Generating video embeddings...\")\n",
        "            video_embeddings = generate_video_embeddings(embedding_model, G)\n",
        "\n",
        "            # Save embeddings\n",
        "            embeddings_path = os.path.join(OUTPUT_DIR, 'embeddings', 'video_embeddings.pickle')\n",
        "            save_embeddings(video_embeddings, embeddings_path)\n",
        "            print(f\"Embeddings saved to {embeddings_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating embeddings: {str(e)}\")\n",
        "\n",
        "    # Step 11: Create visualizations\n",
        "    visualize_knowledge_graph(G)\n",
        "\n",
        "    # Step 12: Generate example learning path\n",
        "    example_goal = \"Learn Python for bioimage analysis\"\n",
        "    try:\n",
        "        learning_path = generate_learning_path(llm, G, example_goal)\n",
        "        print(f\"Example learning path generated for: '{example_goal}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating example learning path: {str(e)}\")\n",
        "\n",
        "    print(\"System ready! You can now query the knowledge graph.\")\n",
        "    return G, llm, embedding_model, video_embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "NZOLfmWwZi5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Loading Previously Built System\n",
        "\n",
        "Once we've built and saved our knowledge graph system, we need functions to load it back. These functions load the graph from pickle or database, load saved embeddings, and initialize the LLM. The comprehensive load_knowledge_graph_system function brings everything together, loading the graph, LLM, and embeddings in one step for convenient querying."
      ],
      "metadata": {
        "id": "rOF2iM1TZuX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_graph_pickle(filepath: str) -> nx.DiGraph:\n",
        "    \"\"\"\n",
        "    Load NetworkX graph from a pickle file using Python's built-in pickle\n",
        "\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"Graph pickle not found at: {filepath}\")\n",
        "\n",
        "    try:\n",
        "        # Load graph using Python's built-in pickle\n",
        "        import pickle\n",
        "        with open(filepath, 'rb') as f:\n",
        "            G = pickle.load(f)\n",
        "        print(f\"Knowledge graph loaded from pickle: {filepath}\")\n",
        "        return G\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error loading graph pickle: {str(e)}\")\n",
        "\n",
        "\n",
        "def load_knowledge_graph_from_db(db_path='knowledge_graph.db'):\n",
        "    \"\"\"\n",
        "    Load knowledge graph from SQLite database\n",
        "    \"\"\"\n",
        "    db_path = os.path.join(OUTPUT_DIR, 'database', db_path)\n",
        "\n",
        "    if not os.path.exists(db_path):\n",
        "        raise FileNotFoundError(f\"Database not found: {db_path}\")\n",
        "\n",
        "    # Connect to database\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Create new graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Load nodes\n",
        "    c.execute('SELECT * FROM nodes')\n",
        "    for row in c.fetchall():\n",
        "        node_id = row[0]\n",
        "\n",
        "        # Parse JSON fields\n",
        "        topics = json.loads(row[7]) if row[7] else []\n",
        "        prerequisites = json.loads(row[8]) if row[8] else []\n",
        "        learning_outcomes = json.loads(row[9]) if row[9] else []\n",
        "\n",
        "        # Add node with all attributes\n",
        "        G.add_node(\n",
        "            node_id,\n",
        "            label=row[1],\n",
        "            node_type=row[2],\n",
        "            title=row[3],\n",
        "            description=row[4],\n",
        "            difficulty=row[5],\n",
        "            duration=row[6],\n",
        "            topics=topics,\n",
        "            prerequisites=prerequisites,\n",
        "            learning_outcomes=learning_outcomes\n",
        "        )\n",
        "\n",
        "    # Load edges\n",
        "    c.execute('SELECT * FROM edges')\n",
        "    for row in c.fetchall():\n",
        "        source = row[0]\n",
        "        target = row[1]\n",
        "        edge_type = row[2]\n",
        "        weight = row[3]\n",
        "\n",
        "        G.add_edge(\n",
        "            source,\n",
        "            target,\n",
        "            type=edge_type,\n",
        "            weight=weight\n",
        "        )\n",
        "\n",
        "    # Close connection\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"Knowledge graph loaded from database: {db_path}\")\n",
        "    return G\n",
        "\n",
        "\n",
        "def load_embeddings(filepath):\n",
        "    \"\"\"\n",
        "    Load video embeddings from a file\n",
        "\n",
        "    \"\"\"\n",
        "    import pickle\n",
        "    import os\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"Embeddings file not found: {filepath}\")\n",
        "\n",
        "    with open(filepath, 'rb') as f:\n",
        "        video_embeddings = pickle.load(f)\n",
        "\n",
        "    print(f\"Loaded embeddings for {len(video_embeddings)} videos\")\n",
        "    return video_embeddings\n",
        "\n",
        "\n",
        "def load_knowledge_graph_system(graph_path, embedding_path=None, model_type=\"mistral\", use_gpu=True):\n",
        "    \"\"\"\n",
        "    Load the knowledge graph, embeddings, and initialize the LLM for querying\n",
        "\n",
        "    model_type : ('mistral', 'llama', or 'tiny')\n",
        "\n",
        "    \"\"\"\n",
        "    import pickle\n",
        "    import os\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    # Step 1: Load the graph\n",
        "    print(f\"Loading knowledge graph from: {graph_path}\")\n",
        "    with open(graph_path, 'rb') as f:\n",
        "        G = pickle.load(f)\n",
        "    print(f\"Graph loaded with {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
        "\n",
        "    # Step 2: Setup LLM\n",
        "    setup_llm_gpu()  # Make sure GPU support is set up\n",
        "\n",
        "    # Step 3: Initialize LLM\n",
        "    llm = initialize_llm(model_type=model_type, use_gpu=use_gpu)\n",
        "    print(f\"LLM ({model_type}) initialized and ready\")\n",
        "\n",
        "    # Step 4: Load embeddings if path provided\n",
        "    embedding_model = None\n",
        "    video_embeddings = None\n",
        "\n",
        "    if embedding_path and os.path.exists(embedding_path):\n",
        "        try:\n",
        "            # Initialize embedding model\n",
        "            print(\"Initializing embedding model...\")\n",
        "            embedding_model = initialize_embedding_model()\n",
        "\n",
        "            # Load saved embeddings\n",
        "            print(f\"Loading embeddings from: {embedding_path}\")\n",
        "            with open(embedding_path, 'rb') as f:\n",
        "                video_embeddings = pickle.load(f)\n",
        "            print(f\"Loaded embeddings for {len(video_embeddings)} videos\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading embeddings: {str(e)}\")\n",
        "            print(\"Will continue without embeddings capability\")\n",
        "    else:\n",
        "        print(\"No embedding path provided or file not found. Semantic search will not be available.\")\n",
        "\n",
        "    print(\"Knowledge graph system loaded and ready for queries!\")\n",
        "    return G, llm, embedding_model, video_embeddings\n",
        "\n",
        "\"\"\"\n",
        "# Example usage:\n",
        "graph_path = \"/content/drive/MyDrive/data/knowledge_graph_results/graph/knowledge_graph.pickle\"\n",
        "embeddings_path = \"/content/drive/MyDrive/data/knowledge_graph_results/embeddings/video_embeddings.pickle\"\n",
        "\n",
        "# Load the system once\n",
        "G, llm, embedding_model, video_embeddings = load_knowledge_graph_system(\n",
        "    graph_path=graph_path,\n",
        "    embedding_path=embeddings_path,\n",
        "    model_type=\"mistral\",\n",
        "    use_gpu=True\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ah4tc8QRZzED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Semantic Search\n",
        "\n",
        "With our system loaded, we can now perform semantic searches. The semantic search function finds videos similar to a query by comparing embeddings. Additional functions expand keywords and use the LLM to enrich queries with related concepts. This allows us to find relevant videos even when queries don't exactly match the terms in the video metadata."
      ],
      "metadata": {
        "id": "SyWyXJcsaDP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(query, embedding_model, video_embeddings, G, top_k=10):\n",
        "    \"\"\"\n",
        "    Search for videos semantically similar to the query\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = embedding_model.encode([query])[0]\n",
        "\n",
        "    # Calculate cosine similarity between query and all videos\n",
        "    similarities = {}\n",
        "    for node_id, embedding in video_embeddings.items():\n",
        "        # Calculate cosine similarity\n",
        "        sim = cosine_similarity([query_embedding], [embedding])[0][0]\n",
        "        similarities[node_id] = sim\n",
        "\n",
        "    # Sort by similarity and get top_k results\n",
        "    top_nodes = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "    # Format results\n",
        "    results = []\n",
        "    for node_id, similarity in top_nodes:\n",
        "        if node_id in G.nodes():\n",
        "            node_data = G.nodes[node_id]\n",
        "            results.append({\n",
        "                \"id\": node_id,\n",
        "                \"title\": node_data.get('title', 'Unknown'),\n",
        "                \"difficulty\": node_data.get('difficulty', 'intermediate'),\n",
        "                \"topics\": node_data.get('topics', []),\n",
        "                \"similarity\": float(similarity),\n",
        "                \"duration_minutes\": node_data.get('duration_minutes', 0)\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "def expand_keywords(query):\n",
        "    \"\"\"\n",
        "    Simple function to expand keywords in a query with related terms\n",
        "    \"\"\"\n",
        "    query = query.lower()\n",
        "    expanded = []\n",
        "\n",
        "    # Extract main keywords (words longer than 3 chars)\n",
        "    main_keywords = [w for w in query.split() if len(w) > 3 and w not in ['show', 'find', 'about', 'related', 'videos', 'with']]\n",
        "    expanded.extend(main_keywords)\n",
        "\n",
        "    # Add domain-specific expansions\n",
        "    for keyword in main_keywords:\n",
        "        if 'python' in keyword:\n",
        "            expanded.extend(['programming', 'coding', 'development'])\n",
        "        if 'image' in keyword:\n",
        "            expanded.extend(['processing', 'analysis', 'computer vision'])\n",
        "        if 'bio' in keyword or 'medical' in keyword:\n",
        "            expanded.extend(['microscopy', 'analysis', 'biology', 'healthcare'])\n",
        "        if 'deep' in keyword or 'learning' in keyword or 'ai' in keyword:\n",
        "            expanded.extend(['neural', 'network', 'machine learning', 'artificial intelligence'])\n",
        "        if 'data' in keyword:\n",
        "            expanded.extend(['analysis', 'science', 'visualization'])\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    seen = set()\n",
        "    expanded = [x for x in expanded if not (x in seen or seen.add(x))]\n",
        "\n",
        "    return expanded\n",
        "\n",
        "\n",
        "\n",
        "def expand_query_concepts(llm, query, domain=\"programming and data science\"):\n",
        "    \"\"\"\n",
        "    Use LLM to expand query with related concepts\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an expert in {domain}. Analyze this query:\n",
        "\"{query}\"\n",
        "\n",
        "Extract the main concept/topic and identify 3-5 closely related concepts that would help find relevant educational videos.\n",
        "For example, if someone asks about \"deep learning\", related concepts might include \"neural networks\", \"TensorFlow\", \"PyTorch\", etc.\n",
        "\n",
        "Format your response as a JSON object:\n",
        "{{\n",
        "  \"main_topic\": \"the main concept\",\n",
        "  \"related_concepts\": [\"concept1\", \"concept2\", \"concept3\"],\n",
        "  \"expanded_query\": \"a more detailed query including related concepts\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm(prompt, max_tokens=1000, temperature=0.2)\n",
        "        response_text = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "        # Find JSON in response\n",
        "        import json\n",
        "        import re\n",
        "\n",
        "        # Extract JSON\n",
        "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
        "        if json_match:\n",
        "            expansion_data = json.loads(json_match.group(0))\n",
        "            return expansion_data\n",
        "        else:\n",
        "            print(\"Could not extract JSON from query expansion response\")\n",
        "            return {\n",
        "                \"main_topic\": query,\n",
        "                \"related_concepts\": [],\n",
        "                \"expanded_query\": query\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in query expansion: {str(e)}\")\n",
        "        return {\n",
        "            \"main_topic\": query,\n",
        "            \"related_concepts\": [],\n",
        "            \"expanded_query\": query\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "2Z0WhKZFaGro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Query Processing\n",
        "\n",
        "Our handle_user_query function provides a three-tier approach to query processing: first attempting semantic search with embeddings, then using LLM-based query understanding if needed, and finally falling back to pattern matching if both fail. The load_and_query function combines loading the graph and running a query in one convenient step."
      ],
      "metadata": {
        "id": "WwMBaWc0aP_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def handle_user_query(llm, G, query, embedding_model=None, video_embeddings=None):\n",
        "    \"\"\"\n",
        "    Enhanced query function using LLM for understanding and embeddings for semantic search\n",
        "    \"\"\"\n",
        "    print(f\"Processing query: '{query}'\")\n",
        "\n",
        "    # First attempt: Use semantic search if embeddings are available\n",
        "    if embedding_model is not None and video_embeddings is not None:\n",
        "        semantic_results = semantic_search(query, embedding_model, video_embeddings, G, top_k=20)\n",
        "\n",
        "        # If we find good semantic matches, return them\n",
        "        if semantic_results and semantic_results[0]['similarity'] > 0.4:\n",
        "            return {\n",
        "                \"type\": \"semantic_search\",\n",
        "                \"query\": query,\n",
        "                \"videos\": semantic_results,\n",
        "                \"method\": \"embedding\"\n",
        "            }\n",
        "\n",
        "    # Second attempt: Use LLM to understand the query and expand concepts\n",
        "    if llm is not None:\n",
        "        try:\n",
        "            # Parse query with LLM to extract structured information\n",
        "            prompt = f\"\"\"You are an AI assistant helping users find educational videos.\n",
        "Analyze this query: \"{query}\"\n",
        "\n",
        "Extract the following information:\n",
        "1. Search type: What kind of search is this? (topic exploration, learning path, finding prerequisites, etc.)\n",
        "2. Main topic: What is the main subject or concept being asked about?\n",
        "3. Difficulty level: Is a specific difficulty level mentioned? (beginner, intermediate, advanced, or none)\n",
        "4. Related concepts: What other concepts might be relevant to this query?\n",
        "\n",
        "Format your response as valid JSON, and ONLY JSON with no additional text:\n",
        "{{\n",
        "  \"search_type\": \"topic_exploration\",\n",
        "  \"main_topic\": \"python\",\n",
        "  \"difficulty\": \"beginner\",\n",
        "  \"related_concepts\": [\"programming\", \"coding\", \"python basics\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "            response = llm(prompt, max_tokens=1000, temperature=0.1)\n",
        "            response_text = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "            # Extract JSON - improved error handling\n",
        "            import json\n",
        "            import re\n",
        "\n",
        "            # Clean the response text to improve JSON parsing success\n",
        "            # Remove any text before the first '{' and after the last '}'\n",
        "            json_match = re.search(r'(\\{.*\\})', response_text, re.DOTALL)\n",
        "\n",
        "            if json_match:\n",
        "                json_str = json_match.group(1)\n",
        "                # Further clean the JSON string to handle common issues\n",
        "                json_str = re.sub(r'[\\n\\r\\t]', ' ', json_str)  # Remove newlines, tabs\n",
        "                json_str = re.sub(r',\\s*\\}', '}', json_str)     # Remove trailing commas\n",
        "\n",
        "                try:\n",
        "                    query_info = json.loads(json_str)\n",
        "\n",
        "                    # Extract key information\n",
        "                    main_topic = query_info.get('main_topic', '').lower()\n",
        "                    difficulty = query_info.get('difficulty', '').lower()\n",
        "                    related_concepts = [c.lower() for c in query_info.get('related_concepts', [])]\n",
        "\n",
        "                    # Find videos matching the criteria\n",
        "                    matched_videos = []\n",
        "                    for node_id in G.nodes():\n",
        "                        node_data = G.nodes[node_id]\n",
        "\n",
        "                        # Skip if not a video node\n",
        "                        if node_data.get('node_type') != 'video':\n",
        "                            continue\n",
        "\n",
        "                        # Prepare for matching\n",
        "                        node_topics = [t.lower() for t in node_data.get('topics', [])]\n",
        "                        node_title = node_data.get('title', '').lower()\n",
        "                        node_desc = node_data.get('description', '').lower()\n",
        "                        node_difficulty = node_data.get('difficulty', '').lower()\n",
        "\n",
        "                        # Match criteria\n",
        "                        topic_match = False\n",
        "                        if main_topic:\n",
        "                            # Check direct topic match\n",
        "                            topic_match = any(main_topic in t for t in node_topics) or main_topic in node_title or main_topic in node_desc\n",
        "\n",
        "                            # If no direct match, check for related concepts\n",
        "                            if not topic_match and related_concepts:\n",
        "                                for concept in related_concepts:\n",
        "                                    if any(concept in t for t in node_topics) or concept in node_title or concept in node_desc:\n",
        "                                        topic_match = True\n",
        "                                        break\n",
        "                        else:\n",
        "                            # If no topic specified, consider it a match\n",
        "                            topic_match = True\n",
        "\n",
        "                        # Difficulty match\n",
        "                        diff_match = not difficulty or node_difficulty == difficulty\n",
        "\n",
        "                        # If both criteria match, add to results\n",
        "                        if topic_match and diff_match:\n",
        "                            # Calculate relevance score\n",
        "                            relevance = 0.0\n",
        "\n",
        "                            # Higher score for direct topic matches\n",
        "                            if main_topic and any(main_topic in t for t in node_topics):\n",
        "                                relevance += 1.0\n",
        "                            elif main_topic and main_topic in node_title:\n",
        "                                relevance += 0.8\n",
        "                            elif main_topic and main_topic in node_desc:\n",
        "                                relevance += 0.6\n",
        "\n",
        "                            # Add smaller scores for related concept matches\n",
        "                            for concept in related_concepts:\n",
        "                                if any(concept in t for t in node_topics):\n",
        "                                    relevance += 0.4\n",
        "                                elif concept in node_title:\n",
        "                                    relevance += 0.3\n",
        "                                elif concept in node_desc:\n",
        "                                    relevance += 0.2\n",
        "\n",
        "                            # Add video to results\n",
        "                            matched_videos.append({\n",
        "                                \"id\": node_id,\n",
        "                                \"title\": node_data.get('title', ''),\n",
        "                                \"difficulty\": node_data.get('difficulty', ''),\n",
        "                                \"duration_minutes\": node_data.get('duration_minutes', 0),\n",
        "                                \"topics\": node_data.get('topics', []),\n",
        "                                \"relevance\": relevance\n",
        "                            })\n",
        "\n",
        "                    # Sort by relevance score\n",
        "                    matched_videos.sort(key=lambda x: x.get('relevance', 0), reverse=True)\n",
        "\n",
        "                    # Return results\n",
        "                    if matched_videos:\n",
        "                        return {\n",
        "                            \"type\": query_info.get('search_type', 'topic_exploration'),\n",
        "                            \"topic\": main_topic,\n",
        "                            \"difficulty\": difficulty,\n",
        "                            \"related_concepts\": related_concepts,\n",
        "                            \"videos\": matched_videos[:20],  # Limit to top 20\n",
        "                            \"method\": \"llm\"\n",
        "                        }\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"JSON parsing error: {e} in string: {json_str[:50]}...\")\n",
        "            else:\n",
        "                print(\"Could not extract JSON from LLM response\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error using LLM for query understanding: {str(e)}\")\n",
        "            # Fall back to pattern matching\n",
        "\n",
        "    # Third attempt (fallback): Use expanded keyword matching\n",
        "    expanded_keywords = expand_keywords(query)\n",
        "    print(f\"Using expanded keyword matching with: {', '.join(expanded_keywords)}\")\n",
        "\n",
        "    filtered_videos = []\n",
        "    for node_id in G.nodes():\n",
        "        node_data = G.nodes[node_id]\n",
        "\n",
        "        # Skip if not a valid node\n",
        "        if not all(k in node_data for k in ['title', 'topics']):\n",
        "            continue\n",
        "\n",
        "        node_topics = [t.lower() for t in node_data.get('topics', [])]\n",
        "        node_title = node_data.get('title', '').lower()\n",
        "        node_desc = node_data.get('description', '').lower()\n",
        "\n",
        "        # Match any of the expanded keywords\n",
        "        match_score = 0\n",
        "        for keyword in expanded_keywords:\n",
        "            if any(keyword in t.lower() for t in node_topics):\n",
        "                match_score += 2  # Higher weight for topic matches\n",
        "            if keyword in node_title:\n",
        "                match_score += 1.5  # Medium weight for title matches\n",
        "            if keyword in node_desc:\n",
        "                match_score += 1  # Lower weight for description matches\n",
        "\n",
        "        if match_score > 0:\n",
        "            filtered_videos.append({\n",
        "                \"id\": node_id,\n",
        "                \"title\": node_data.get('title', ''),\n",
        "                \"difficulty\": node_data.get('difficulty', ''),\n",
        "                \"duration_minutes\": node_data.get('duration_minutes', 0),\n",
        "                \"topics\": node_data.get('topics', []),\n",
        "                \"match_score\": match_score\n",
        "            })\n",
        "\n",
        "    # Sort by match score\n",
        "    filtered_videos.sort(key=lambda x: x.get('match_score', 0), reverse=True)\n",
        "\n",
        "    # Extract main topic from query for response formatting\n",
        "    query_words = query.lower().split()\n",
        "    main_topic = ' '.join([w for w in query_words if len(w) > 3 and w not in ['show', 'find', 'about', 'related', 'videos', 'with']])\n",
        "\n",
        "    print(f\"Found {len(filtered_videos)} matching videos\")\n",
        "    return {\n",
        "        \"type\": \"keyword_search\",\n",
        "        \"topic\": main_topic,\n",
        "        \"related_concepts\": expanded_keywords[1:],  # Skip the first which is usually the main topic\n",
        "        \"videos\": filtered_videos[:20],  # Limit to top 20\n",
        "        \"method\": \"expanded_keywords\"\n",
        "    }\n",
        "\n",
        "\n",
        "def load_and_query(graph_path, query, embedding_path=None):\n",
        "    \"\"\"\n",
        "    Load a graph from pickle and run a query on it\n",
        "\n",
        "    \"\"\"\n",
        "    # Load graph\n",
        "    import pickle\n",
        "    print(f\"Loading graph from: {graph_path}\")\n",
        "    with open(graph_path, 'rb') as f:\n",
        "        G = pickle.load(f)\n",
        "    print(f\"Graph loaded with {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
        "\n",
        "    # Load embeddings if available\n",
        "    embedding_model = None\n",
        "    video_embeddings = None\n",
        "    if embedding_path:\n",
        "        try:\n",
        "            # Initialize embedding model\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "            # Load embeddings\n",
        "            with open(embedding_path, 'rb') as f:\n",
        "                video_embeddings = pickle.load(f)\n",
        "            print(f\"Loaded embeddings for {len(video_embeddings)} videos\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading embeddings: {str(e)}\")\n",
        "\n",
        "    # Run query\n",
        "    result = handle_user_query(None, G, query, embedding_model, video_embeddings)\n",
        "\n",
        "    # Print results summary\n",
        "    if result.get(\"type\") == \"topic_exploration\":\n",
        "        print(f\"\\nFound {len(result.get('videos', []))} videos about {result.get('topic', 'unknown topic')}\")\n",
        "        for i, video in enumerate(result.get('videos', [])[:5]):  # Show top 5\n",
        "            print(f\"{i+1}. {video.get('title', 'Unknown')} ({video.get('difficulty', 'unknown')})\")\n",
        "    elif result.get(\"type\") == \"semantic_search\":\n",
        "        print(f\"\\nFound {len(result.get('videos', []))} videos semantically related to '{query}'\")\n",
        "        for i, video in enumerate(result.get('videos', [])[:5]):  # Show top 5\n",
        "            print(f\"{i+1}. {video.get('title', 'Unknown')} (similarity: {video.get('similarity', 0):.2f})\")\n",
        "\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "X12fBRxYaSW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Learning Path Generation\n",
        "\n",
        "One of the most powerful features of our system is generating personalized learning paths. These functions use the LLM to create a logical sequence of topics for a given learning goal, find appropriate videos for each topic, format the path into a structured output, and visualize the learning journey. This helps users navigate from beginner to advanced content in a logical progression."
      ],
      "metadata": {
        "id": "tPjw5ayXalZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_learning_path_query(llm, goal: str, available_topics: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate a learning path for a specific goal using LLM\n",
        "    \"\"\"\n",
        "    # Limit the number of topics to avoid exceeding context window\n",
        "    max_topics = 100  # Maximum number of topics to include in prompt\n",
        "\n",
        "    if len(available_topics) > max_topics:\n",
        "        print(f\"Limiting from {len(available_topics)} to {max_topics} topics for learning path generation\")\n",
        "\n",
        "        # Filter topics by relevance to the goal\n",
        "        # Simple filtering: check for word overlap with the goal\n",
        "        goal_words = set(goal.lower().split())\n",
        "\n",
        "        # Calculate relevance score based on word overlap\n",
        "        topic_scores = []\n",
        "        for topic in available_topics:\n",
        "            topic_words = set(topic.lower().split())\n",
        "            overlap = len(goal_words.intersection(topic_words))\n",
        "            topic_scores.append((topic, overlap))\n",
        "\n",
        "        # Sort topics by relevance score and take top ones\n",
        "        topic_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        filtered_topics = [t[0] for t in topic_scores[:max_topics]]\n",
        "    else:\n",
        "        filtered_topics = available_topics\n",
        "\n",
        "    # Create prompt for learning path generation (more concise version)\n",
        "    prompt = f\"\"\"You are an educational content curator. Create a learning path for:\n",
        "\"{goal}\"\n",
        "\n",
        "Available topics: {', '.join(filtered_topics)}\n",
        "\n",
        "Order topics in a logical progression from basic to advanced. Include only relevant topics.\n",
        "\n",
        "Format your response as JSON:\n",
        "{{\n",
        "  \"learning_path\": [\n",
        "    {{\n",
        "      \"topic\": \"topic1\",\n",
        "      \"reason\": \"Brief explanation\"\n",
        "    }},\n",
        "    {{\n",
        "      \"topic\": \"topic2\",\n",
        "      \"reason\": \"Brief explanation\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Select topics from the exact names listed in the available topics.\n",
        "\"\"\"\n",
        "\n",
        "    # Get response from LLM\n",
        "    try:\n",
        "        response = llm(prompt, max_tokens=2048, temperature=0.3)\n",
        "        response_text = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "        # Extract JSON\n",
        "        json_start = response_text.find('{')\n",
        "        json_end = response_text.rfind('}') + 1\n",
        "\n",
        "        if json_start == -1 or json_end == 0:\n",
        "            print(\"Failed to get valid JSON for learning path\")\n",
        "            return []\n",
        "\n",
        "        json_str = response_text[json_start:json_end]\n",
        "\n",
        "        try:\n",
        "            path_data = json.loads(json_str)\n",
        "            return path_data.get(\"learning_path\", [])\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Invalid JSON for learning path\")\n",
        "            return []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating learning path: {str(e)}\")\n",
        "\n",
        "        # Create a simple fallback path based on the goal\n",
        "        fallback_path = []\n",
        "        goal_lower = goal.lower()\n",
        "\n",
        "        # Add some basic topics based on keywords in the goal\n",
        "        if \"python\" in goal_lower:\n",
        "            fallback_path.append({\"topic\": \"Python basics\", \"reason\": \"Fundamental programming concepts\"})\n",
        "        if \"image\" in goal_lower or \"bio\" in goal_lower:\n",
        "            fallback_path.append({\"topic\": \"Image processing\", \"reason\": \"Core concepts for working with images\"})\n",
        "        if \"analysis\" in goal_lower:\n",
        "            fallback_path.append({\"topic\": \"Data analysis\", \"reason\": \"Techniques for analyzing data\"})\n",
        "\n",
        "        return fallback_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_videos_for_learning_path(G: nx.DiGraph, learning_path: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Find videos for each topic in the learning path\n",
        "\n",
        "    \"\"\"\n",
        "    path_videos = []\n",
        "\n",
        "    for path_item in learning_path:\n",
        "        topic = path_item.get(\"topic\", \"\")\n",
        "        reason = path_item.get(\"reason\", \"\")\n",
        "\n",
        "        # Find related videos\n",
        "        related_videos = []\n",
        "\n",
        "        for node in G.nodes():\n",
        "            if G.nodes[node].get('node_type') == 'video':\n",
        "                video_topics = G.nodes[node].get('topics', [])\n",
        "                if topic in video_topics:\n",
        "                    # Found a video covering this topic\n",
        "                    video_data = G.nodes[node]\n",
        "                    related_videos.append({\n",
        "                        \"id\": node,\n",
        "                        \"title\": video_data.get('title', 'Unknown Video'),\n",
        "                        \"difficulty\": video_data.get('difficulty', 'intermediate'),\n",
        "                        \"duration_minutes\": video_data.get('duration_minutes', 0)\n",
        "                    })\n",
        "\n",
        "        # Sort videos by difficulty\n",
        "        difficulty_order = {\"beginner\": 0, \"intermediate\": 1, \"advanced\": 2}\n",
        "        related_videos.sort(key=lambda x: difficulty_order[x[\"difficulty\"]])\n",
        "\n",
        "        path_videos.append({\n",
        "            \"topic\": topic,\n",
        "            \"reason\": reason,\n",
        "            \"videos\": related_videos\n",
        "        })\n",
        "\n",
        "    return path_videos\n",
        "\n",
        "\n",
        "def format_learning_path(path_with_videos: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Format learning path with videos into a structured output\n",
        "\n",
        "    \"\"\"\n",
        "    # Calculate overall statistics\n",
        "    total_videos = sum(len(stage[\"videos\"]) for stage in path_with_videos)\n",
        "    total_duration = sum(\n",
        "        sum(video[\"duration_minutes\"] for video in stage[\"videos\"])\n",
        "        for stage in path_with_videos\n",
        "    )\n",
        "\n",
        "    # Count videos by difficulty\n",
        "    difficulty_breakdown = {\"beginner\": 0, \"intermediate\": 0, \"advanced\": 0}\n",
        "    for stage in path_with_videos:\n",
        "        for video in stage[\"videos\"]:\n",
        "            difficulty_breakdown[video[\"difficulty\"]] += 1\n",
        "\n",
        "    # Format the path\n",
        "    formatted_path = {\n",
        "        \"total_videos\": total_videos,\n",
        "        \"total_duration_minutes\": total_duration,\n",
        "        \"difficulty_breakdown\": difficulty_breakdown,\n",
        "        \"topics_covered\": [stage[\"topic\"] for stage in path_with_videos],\n",
        "        \"stages\": []\n",
        "    }\n",
        "\n",
        "    # Add detailed stages\n",
        "    step_counter = 1\n",
        "    for stage in path_with_videos:\n",
        "        stage_data = {\n",
        "            \"topic\": stage[\"topic\"],\n",
        "            \"reason\": stage[\"reason\"],\n",
        "            \"videos\": []\n",
        "        }\n",
        "\n",
        "        for video in stage[\"videos\"]:\n",
        "            video_data = {\n",
        "                \"step\": step_counter,\n",
        "                \"id\": video[\"id\"],\n",
        "                \"title\": video[\"title\"],\n",
        "                \"difficulty\": video[\"difficulty\"],\n",
        "                \"duration_minutes\": video[\"duration_minutes\"]\n",
        "            }\n",
        "            stage_data[\"videos\"].append(video_data)\n",
        "            step_counter += 1\n",
        "\n",
        "        formatted_path[\"stages\"].append(stage_data)\n",
        "\n",
        "    return formatted_path\n",
        "\n",
        "\n",
        "def generate_learning_path(llm, G: nx.DiGraph, goal: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Generate a complete learning path for a specific goal\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"Generating learning path for: '{goal}'\")\n",
        "\n",
        "\n",
        "    # Collect unique topics from video nodes\n",
        "    available_topics = set()\n",
        "    for node in G.nodes():\n",
        "        if G.nodes[node].get('node_type') == 'video':\n",
        "            available_topics.update(G.nodes[node].get('topics', []))\n",
        "    available_topics = list(available_topics)\n",
        "\n",
        "    # Generate learning path using LLM\n",
        "    learning_path = generate_learning_path_query(llm, goal, available_topics)\n",
        "\n",
        "    if not learning_path:\n",
        "        print(\"Failed to generate learning path\")\n",
        "        return None\n",
        "\n",
        "    # Find videos for each topic in the path\n",
        "    path_with_videos = find_videos_for_learning_path(G, learning_path)\n",
        "\n",
        "    # Format the complete path\n",
        "    formatted_path = format_learning_path(path_with_videos)\n",
        "\n",
        "    # Visualize the path\n",
        "    filename = f\"learning_path_{goal.replace(' ', '_').replace('/', '_')}.html\"\n",
        "    viz_path = visualize_learning_path(G, formatted_path, filename)\n",
        "\n",
        "    # Add visualization path to result\n",
        "    formatted_path[\"visualization\"] = viz_path\n",
        "\n",
        "    return formatted_path\n",
        "\n",
        "\n",
        "def visualize_learning_path(G: nx.DiGraph, path: Dict, filename='learning_path.html'):\n",
        "    \"\"\"\n",
        "    Create interactive visualization of a learning path\n",
        "\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'visualizations', filename)\n",
        "\n",
        "    # Create a PyVis network\n",
        "    net = Network(height='750px', width='100%', bgcolor='#ffffff',\n",
        "                 directed=True)\n",
        "\n",
        "    # Set physics layout options\n",
        "    net.set_options(\"\"\"\n",
        "    {\n",
        "      \"physics\": {\n",
        "        \"hierarchicalRepulsion\": {\n",
        "          \"centralGravity\": 0.0,\n",
        "          \"springLength\": 100,\n",
        "          \"springConstant\": 0.01,\n",
        "          \"nodeDistance\": 120\n",
        "        },\n",
        "        \"solver\": \"hierarchicalRepulsion\",\n",
        "        \"stabilization\": {\n",
        "          \"iterations\": 100\n",
        "        }\n",
        "      },\n",
        "      \"layout\": {\n",
        "        \"hierarchical\": {\n",
        "          \"enabled\": true,\n",
        "          \"direction\": \"LR\",\n",
        "          \"sortMethod\": \"directed\",\n",
        "          \"levelSeparation\": 150\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "    # Color mapping\n",
        "    color_map = {\n",
        "        'beginner': '#90EE90',      # light green\n",
        "        'intermediate': '#ADD8E6',   # light blue\n",
        "        'advanced': '#FFB6C1',       # light pink\n",
        "        'topic': '#FFA500'          # orange\n",
        "    }\n",
        "\n",
        "    # Add all nodes and edges\n",
        "    nodes_added = set()\n",
        "\n",
        "    # First add topic nodes\n",
        "    for i, topic in enumerate(path[\"topics_covered\"]):\n",
        "        topic_id = f\"topic_{i}\"\n",
        "        net.add_node(\n",
        "            topic_id,\n",
        "            label=topic,\n",
        "            title=f\"Stage {i+1}: {topic}\",\n",
        "            color=color_map['topic'],\n",
        "            shape='diamond',\n",
        "            size=20,\n",
        "            level=i  # For hierarchical layout\n",
        "        )\n",
        "        nodes_added.add(topic_id)\n",
        "\n",
        "    # Add video nodes connected to topics\n",
        "    for i, stage in enumerate(path[\"stages\"]):\n",
        "        topic_id = f\"topic_{i}\"\n",
        "\n",
        "        # Add videos for this stage\n",
        "        for j, video in enumerate(stage[\"videos\"]):\n",
        "            video_node_id = video[\"id\"]\n",
        "\n",
        "            # Skip if already added\n",
        "            if video_node_id in nodes_added:\n",
        "                continue\n",
        "\n",
        "            # Add video node\n",
        "            net.add_node(\n",
        "                video_node_id,\n",
        "                label=f\"{video['step']}. {video['title'][:20]}...\",\n",
        "                title=f\"Step {video['step']}: {video['title']} ({video['duration_minutes']:.1f} min)\",\n",
        "                color=color_map[video[\"difficulty\"]],\n",
        "                shape='dot',\n",
        "                size=15,\n",
        "                level=i  # Same level as its topic\n",
        "            )\n",
        "            nodes_added.add(video_node_id)\n",
        "\n",
        "            # Connect topic to video\n",
        "            net.add_edge(\n",
        "                topic_id,\n",
        "                video_node_id,\n",
        "                width=2,\n",
        "                arrows='to'\n",
        "            )\n",
        "\n",
        "    # Add edges between topics to show progression\n",
        "    for i in range(len(path[\"topics_covered\"]) - 1):\n",
        "        topic_id1 = f\"topic_{i}\"\n",
        "        topic_id2 = f\"topic_{i+1}\"\n",
        "\n",
        "        net.add_edge(\n",
        "            topic_id1,\n",
        "            topic_id2,\n",
        "            width=3,\n",
        "            color=\"#000000\",\n",
        "            arrows='to'\n",
        "        )\n",
        "\n",
        "    # Save the network\n",
        "    net.save_graph(output_path)\n",
        "    print(f\"Learning path visualization saved to {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n"
      ],
      "metadata": {
        "id": "cQLNGLO0ap41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. Concept-Based Learning Paths\n",
        "\n",
        "Our most advanced feature generates concept-based learning paths that focus on key concepts rather than just video sequences. The generate_concept_based_learning_path function uses the LLM to identify key concepts needed for a particular goal, finds suitable videos for each concept using semantic search, and creates a comprehensive learning journey. The visualization function then creates an interactive representation of this concept-based path."
      ],
      "metadata": {
        "id": "C7Yqqa6ga_lO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_concept_based_learning_path(llm, G, goal, embedding_model=None, video_embeddings=None):\n",
        "    \"\"\"\n",
        "    Generate a concept-based learning path using semantic similarity\n",
        "    \"\"\"\n",
        "    print(f\"Generating concept-based learning path for: '{goal}'\")\n",
        "\n",
        "    # Step 1: Use LLM to extract concepts and their relationships\n",
        "    prompt = f\"\"\"You are an educational content expert. Create a learning path for:\n",
        "\"{goal}\"\n",
        "\n",
        "First, identify the key concepts needed to achieve this goal, in a logical learning progression from fundamental to advanced.\n",
        "For each concept, provide:\n",
        "1. A short title (1-3 words)\n",
        "2. Why this concept is important for the goal\n",
        "3. What prerequisite concepts should be learned first (if any)\n",
        "\n",
        "Format your response as a JSON object:\n",
        "{{\n",
        "  \"concepts\": [\n",
        "    {{\n",
        "      \"concept\": \"concept1\",\n",
        "      \"importance\": \"Why this concept matters\",\n",
        "      \"prerequisites\": []\n",
        "    }},\n",
        "    {{\n",
        "      \"concept\": \"concept2\",\n",
        "      \"importance\": \"Why this concept matters\",\n",
        "      \"prerequisites\": [\"concept1\"]\n",
        "    }},\n",
        "    {{\n",
        "      \"concept\": \"concept3\",\n",
        "      \"importance\": \"Why this concept matters\",\n",
        "      \"prerequisites\": [\"concept1\", \"concept2\"]\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Order concepts so prerequisites come before the concepts that require them.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Get LLM response\n",
        "        response = llm(prompt, max_tokens=2048, temperature=0.3)\n",
        "        response_text = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "        # Extract JSON\n",
        "        import json\n",
        "        import re\n",
        "\n",
        "        # Find JSON in response\n",
        "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
        "        if not json_match:\n",
        "            print(\"Could not extract JSON from learning path response\")\n",
        "            return None\n",
        "\n",
        "        concepts_data = json.loads(json_match.group(0))\n",
        "        concepts = concepts_data.get(\"concepts\", [])\n",
        "\n",
        "        # Step 2: Find videos for each concept using semantic search or concept matching\n",
        "        concept_videos = []\n",
        "\n",
        "        for concept in concepts:\n",
        "            concept_title = concept.get(\"concept\", \"\")\n",
        "            importance = concept.get(\"importance\", \"\")\n",
        "            prerequisites = concept.get(\"prerequisites\", [])\n",
        "\n",
        "            # Find videos for this concept\n",
        "            if embedding_model and video_embeddings:\n",
        "                # Use semantic search to find videos\n",
        "                videos = semantic_search(\n",
        "                    concept_title,\n",
        "                    embedding_model,\n",
        "                    video_embeddings,\n",
        "                    G,\n",
        "                    top_k=5  # Limit to top 5 videos per concept\n",
        "                )\n",
        "            else:\n",
        "                # Fall back to concept matching\n",
        "                videos = []\n",
        "                for node_id in G.nodes():\n",
        "                    node_data = G.nodes[node_id]\n",
        "                    if node_data.get('node_type') != 'video':\n",
        "                        continue\n",
        "\n",
        "                    # Match by topic or title\n",
        "                    node_topics = node_data.get('topics', [])\n",
        "                    node_title = node_data.get('title', '')\n",
        "\n",
        "                    if any(concept_title.lower() in topic.lower() for topic in node_topics) or concept_title.lower() in node_title.lower():\n",
        "                        videos.append({\n",
        "                            \"id\": node_id,\n",
        "                            \"title\": node_title,\n",
        "                            \"difficulty\": node_data.get('difficulty', 'intermediate'),\n",
        "                            \"duration_minutes\": node_data.get('duration_minutes', 0)\n",
        "                        })\n",
        "\n",
        "                # Sort by difficulty (beginner first)\n",
        "                difficulty_order = {\"beginner\": 0, \"intermediate\": 1, \"advanced\": 2}\n",
        "                videos = sorted(videos, key=lambda x: difficulty_order.get(x.get(\"difficulty\", \"intermediate\"), 1))[:5]\n",
        "\n",
        "            # Add to path\n",
        "            concept_videos.append({\n",
        "                \"concept\": concept_title,\n",
        "                \"importance\": importance,\n",
        "                \"prerequisites\": prerequisites,\n",
        "                \"videos\": videos\n",
        "            })\n",
        "\n",
        "        # Step 3: Format learning path\n",
        "        concept_based_path = {\n",
        "            \"goal\": goal,\n",
        "            \"total_concepts\": len(concept_videos),\n",
        "            \"total_videos\": sum(len(c.get(\"videos\", [])) for c in concept_videos),\n",
        "            \"concepts\": concept_videos\n",
        "        }\n",
        "\n",
        "        # Calculate total duration\n",
        "        total_duration = 0\n",
        "        for concept in concept_videos:\n",
        "            for video in concept.get(\"videos\", []):\n",
        "                total_duration += video.get(\"duration_minutes\", 0)\n",
        "\n",
        "        concept_based_path[\"total_duration_minutes\"] = total_duration\n",
        "\n",
        "        # Step 4: Create visualization\n",
        "        filename = f\"concept_path_{goal.replace(' ', '_').replace('/', '_')}.html\"\n",
        "        viz_path = visualize_concept_learning_path(G, concept_based_path, filename)\n",
        "        concept_based_path[\"visualization\"] = viz_path\n",
        "\n",
        "        return concept_based_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating concept-based learning path: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def visualize_concept_learning_path(G, path, filename='concept_learning_path.html'):\n",
        "    \"\"\"\n",
        "    Create interactive visualization of a concept-based learning path\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'visualizations', filename)\n",
        "\n",
        "    # Create a PyVis network\n",
        "    net = Network(height='750px', width='100%', bgcolor='#ffffff', directed=True)\n",
        "\n",
        "    # Set physics layout options\n",
        "    net.set_options(\"\"\"\n",
        "    {\n",
        "      \"physics\": {\n",
        "        \"hierarchicalRepulsion\": {\n",
        "          \"centralGravity\": 0.0,\n",
        "          \"springLength\": 120,\n",
        "          \"springConstant\": 0.01,\n",
        "          \"nodeDistance\": 150\n",
        "        },\n",
        "        \"solver\": \"hierarchicalRepulsion\",\n",
        "        \"stabilization\": {\n",
        "          \"iterations\": 100\n",
        "        }\n",
        "      },\n",
        "      \"layout\": {\n",
        "        \"hierarchical\": {\n",
        "          \"enabled\": true,\n",
        "          \"direction\": \"LR\",\n",
        "          \"sortMethod\": \"directed\",\n",
        "          \"levelSeparation\": 200\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "    # Color mapping\n",
        "    color_map = {\n",
        "        'beginner': '#90EE90',      # light green\n",
        "        'intermediate': '#ADD8E6',   # light blue\n",
        "        'advanced': '#FFB6C1',       # light pink\n",
        "        'concept': '#FFA500'        # orange\n",
        "    }\n",
        "\n",
        "    # Add all nodes and edges\n",
        "    nodes_added = set()\n",
        "\n",
        "    # First add concept nodes\n",
        "    concepts = path.get(\"concepts\", [])\n",
        "    for i, concept_data in enumerate(concepts):\n",
        "        concept = concept_data.get(\"concept\", \"\")\n",
        "        importance = concept_data.get(\"importance\", \"\")\n",
        "\n",
        "        # Create unique ID for concept node\n",
        "        concept_id = f\"concept_{i}\"\n",
        "\n",
        "        # Add concept node\n",
        "        net.add_node(\n",
        "            concept_id,\n",
        "            label=concept,\n",
        "            title=f\"Concept: {concept}\\nImportance: {importance}\",\n",
        "            color=color_map['concept'],\n",
        "            shape='diamond',\n",
        "            size=25,\n",
        "            level=i  # For hierarchical layout\n",
        "        )\n",
        "        nodes_added.add(concept_id)\n",
        "\n",
        "    # Add prerequisite connections between concepts\n",
        "    for i, concept_data in enumerate(concepts):\n",
        "        concept_id = f\"concept_{i}\"\n",
        "        prerequisites = concept_data.get(\"prerequisites\", [])\n",
        "\n",
        "        for prereq in prerequisites:\n",
        "            # Find the prerequisite concept ID\n",
        "            for j, c in enumerate(concepts):\n",
        "                if c.get(\"concept\", \"\") == prereq:\n",
        "                    prereq_id = f\"concept_{j}\"\n",
        "\n",
        "                    # Add edge from prerequisite to this concept\n",
        "                    net.add_edge(\n",
        "                        prereq_id,\n",
        "                        concept_id,\n",
        "                        width=3,\n",
        "                        color=\"#000000\",\n",
        "                        arrows='to',\n",
        "                        title=\"Prerequisite\"\n",
        "                    )\n",
        "                    break\n",
        "\n",
        "    # Add video nodes for each concept\n",
        "    for i, concept_data in enumerate(concepts):\n",
        "        concept_id = f\"concept_{i}\"\n",
        "        videos = concept_data.get(\"videos\", [])\n",
        "\n",
        "        # Add video nodes\n",
        "        for j, video in enumerate(videos):\n",
        "            video_id = f\"{concept_id}_video_{j}\"\n",
        "            video_node_id = video.get(\"id\", video_id)\n",
        "\n",
        "            # Skip if already added\n",
        "            if video_node_id in nodes_added:\n",
        "                continue\n",
        "\n",
        "            # Get difficulty\n",
        "            difficulty = video.get(\"difficulty\", \"intermediate\")\n",
        "            if difficulty not in color_map:\n",
        "                difficulty = \"intermediate\"\n",
        "\n",
        "            # Add video node\n",
        "            net.add_node(\n",
        "                video_id,\n",
        "                label=video.get(\"title\", \"\")[:25] + \"...\" if len(video.get(\"title\", \"\")) > 25 else video.get(\"title\", \"\"),\n",
        "                title=f\"{video.get('title', '')}\\nDifficulty: {difficulty}\\nDuration: {video.get('duration_minutes', 0):.1f} min\",\n",
        "                color=color_map[difficulty],\n",
        "                shape='dot',\n",
        "                size=15,\n",
        "                level=i  # Same level as its concept\n",
        "            )\n",
        "            nodes_added.add(video_id)\n",
        "\n",
        "            # Connect concept to video\n",
        "            net.add_edge(\n",
        "                concept_id,\n",
        "                video_id,\n",
        "                width=1.5,\n",
        "                arrows='to',\n",
        "                title=\"Teaches\"\n",
        "            )\n",
        "\n",
        "    # Save the network\n",
        "    net.save_graph(output_path)\n",
        "    print(f\"Concept learning path visualization saved to {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kZMMcc7Hfy3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. Building the Knowledge Graph System\n",
        "\n",
        "In this section, we initialize the full knowledge graph building process. We call the main function to process our educational videos, extract structured information using the LLM, build the knowledge graph, generate embeddings, and save everything to disk. This step handles the end-to-end pipeline from raw CSV data to a fully functional knowledge graph system ready for querying and visualizing."
      ],
      "metadata": {
        "id": "Ua_Puzxlcq0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with embedding generation\n",
        "G, llm, embedding_model, video_embeddings = main(\n",
        "    \"/content/drive/MyDrive/data/video_recommender/combined_videos.csv\",\n",
        "    model_type=\"mistral\",\n",
        "    generate_embeddings=True,\n",
        "    max_videos=10,  # Give a number or just type None\n",
        "    show_first_video_llm_output=True,\n",
        "    output_dir=\"/content/drive/MyDrive/data/knowledge_graph_results\"\n",
        ")\n",
        "\n",
        "# Save paths for reference\n",
        "graph_path = \"/content/drive/MyDrive/data/knowledge_graph_results/graph/knowledge_graph.pickle\"\n",
        "embeddings_path = \"/content/drive/MyDrive/data/knowledge_graph_results/embeddings/video_embeddings.pickle\"\n"
      ],
      "metadata": {
        "id": "ctnw8esNcg95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Demonstration of Key Features\n",
        "\n",
        "After building our knowledge graph system, we demonstrate its key capabilities through a series of examples. We showcase semantic search using embeddings to find conceptually related videos without exact keyword matches, LLM-based query understanding with concept expansion to enrich search results, filtering by difficulty level for targeted learning, and generating personalized concept-based learning paths. These examples highlight the power of combining LLMs and embeddings for educational content discovery."
      ],
      "metadata": {
        "id": "7tXlZ04Rc4Nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Semantic Search with Embeddings**\n",
        "\n",
        "Our first example demonstrates the semantic search capability, finding videos related to specific topics like U-net without requiring exact keyword matches. By using dense vector embeddings that capture the semantic meaning of videos, we can identify relevant content based on conceptual similarity rather than just keyword matching. This allows users to discover useful educational content even when using different terminology than what appears in the video titles or descriptions."
      ],
      "metadata": {
        "id": "DobT9u6PdO_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Semantic search with embeddings\n",
        "print(\"\\n\\n=== Example 1: Semantic Search with Embeddings ===\")\n",
        "print(\"Searching for videos related to U-net without requiring exact keyword matches...\")\n",
        "result1 = handle_user_query(llm, G, \"Videos related to U-net\", embedding_model, video_embeddings)\n",
        "print(\"\\nSemantic search results:\")\n",
        "for i, video in enumerate(result1.get('videos', [])[:5]):\n",
        "    print(f\"{i+1}. {video.get('title', 'Unknown')} (similarity: {video.get('similarity', 0):.2f})\")"
      ],
      "metadata": {
        "id": "gR_Ws1nMcg6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM-based Query with Concept Expansion**\n",
        "\n",
        "Next, we showcase how the system uses LLM-based query understanding to expand searches with related concepts. When a user queries for advanced deep learning techniques for image segmentation, the system automatically identifies related concepts like neural networks and semantic segmentation, enriching the search results with videos that might use different but related terminology. This powerful feature helps bridge the gap between how users formulate queries and how content is described."
      ],
      "metadata": {
        "id": "kDISCKehdUBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: LLM-based query with concept expansion\n",
        "print(\"\\n\\n=== Example 2: LLM-based Query with Concept Expansion ===\")\n",
        "print(\"Demonstrating how the system expands queries with related concepts...\")\n",
        "result2 = handle_user_query(llm, G, \"Show me advanced deep learning techniques for image segmentation\", embedding_model, video_embeddings)\n",
        "print(\"\\nLLM query results:\")\n",
        "print(f\"Main topic: {result2.get('topic')}\")\n",
        "print(f\"Related concepts: {', '.join(result2.get('related_concepts', []))}\")\n",
        "for i, video in enumerate(result2.get('videos', [])[:5]):\n",
        "    print(f\"{i+1}. {video.get('title', 'Unknown')} ({video.get('difficulty', 'unknown')})\")"
      ],
      "metadata": {
        "id": "ksOTi-0ddh73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtering Content by Difficulty Level**\n",
        "\n",
        "The third example demonstrates filtering content by difficulty level, allowing users to find videos appropriate for their current skill level. Whether looking for beginner-friendly introductions or advanced techniques, the system accurately identifies content suitable for different learning stages. This helps users find educational content that matches their expertise, avoiding material that's either too basic or too complex for their needs."
      ],
      "metadata": {
        "id": "gS39PjZ0dilX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Simple query for beginner content\n",
        "print(\"\\n\\n=== Example 3: Simple Query for Beginner Content ===\")\n",
        "print(\"Filtering videos by difficulty level...\")\n",
        "result3 = handle_user_query(llm, G, \"Show me beginner videos on Python\", embedding_model, video_embeddings)\n",
        "print(\"\\nQuery results:\")\n",
        "print(f\"Method used: {result3.get('method', 'unknown')}\")\n",
        "for i, video in enumerate(result3.get('videos', [])[:5]):\n",
        "    print(f\"{i+1}. {video.get('title', 'Unknown')} ({video.get('difficulty', 'unknown')})\")"
      ],
      "metadata": {
        "id": "n7ISk4MrdpSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept-Based Learning Paths**\n",
        "\n",
        "Our most advanced feature is demonstrated through generating concept-based learning paths for complex learning goals. For a goal like \"Master computer vision for medical image analysis,\" the system identifies the key concepts a learner needs to understand, arranges them in a logical progression, and finds relevant videos for each concept. This creates a comprehensive, personalized learning journey that guides users from foundational concepts to advanced applications."
      ],
      "metadata": {
        "id": "DOGl4VTbdq_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example 4: Generate concept-based learning path\n",
        "print(\"\\n\\n=== Example 4: Generate Concept-Based Learning Path ===\")\n",
        "print(\"Creating a personalized learning journey across multiple concepts...\")\n",
        "concept_path = generate_concept_based_learning_path(llm, G, \"Master computer vision for medical image analysis\", embedding_model, video_embeddings)\n",
        "\n",
        "# Print concept path summary\n",
        "print(f\"\\nConcept-based learning path summary:\")\n",
        "print(f\"Total concepts: {concept_path.get('total_concepts')}\")\n",
        "print(f\"Total videos: {concept_path.get('total_videos')}\")\n",
        "print(f\"Total duration: {concept_path.get('total_duration_minutes', 0):.1f} minutes\")\n",
        "\n",
        "print(\"\\nConcepts in order:\")\n",
        "for i, concept in enumerate(concept_path.get('concepts', [])):\n",
        "    print(f\"{i+1}. {concept.get('concept')} ({len(concept.get('videos', []))} videos)\")\n",
        "    if i < 3:  # Show videos for first 3 concepts only\n",
        "        for j, video in enumerate(concept.get('videos', [])[:2]):  # Show only first 2 videos per concept\n",
        "            print(f\"   - {video.get('title')}\")\n",
        "\n",
        "print(\"\\n\\n=== Example 5: Loading Pre-built Knowledge Graph System ===\")\n",
        "print(\"Demonstrating how to load a previously built system...\")\n",
        "print(\"(Note: In a real scenario, you would run this in a new session after closing the previous one)\")"
      ],
      "metadata": {
        "id": "WDzFWJ7qd0W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Pre-built Knowledge Graph Systems\n",
        "\n",
        "The final example shows how to load a previously built knowledge graph system without having to rebuild it from scratch. This is crucial for practical applications, as building the knowledge graph is a one-time process, but querying and using it happens many times. By saving and loading the graph, embeddings, and models, we create a reusable resource that can be quickly deployed in different contexts without repeating the intensive extraction and building process."
      ],
      "metadata": {
        "id": "vXd0GNwpd4SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a pre-built knowledge graph system and running the examples\n",
        "\n",
        "# Define paths to pre-built knowledge graph components\n",
        "GRAPH_PATH = \"/content/drive/MyDrive/data/knowledge_graph_results/graph/full_knowledge_graph.pickle\"\n",
        "EMBEDDINGS_PATH = \"/content/drive/MyDrive/data/knowledge_graph_results/embeddings/full_video_embeddings.pickle\"\n",
        "\n",
        "# Step 1: Load the pre-built knowledge graph system using the existing function\n",
        "print(\"\\n\\n=== Loading Pre-built Knowledge Graph System ===\")\n",
        "G, llm, embedding_model, video_embeddings = load_knowledge_graph_system(\n",
        "    graph_path=GRAPH_PATH,\n",
        "    embedding_path=EMBEDDINGS_PATH,\n",
        "    model_type=\"mistral\",  # Can use \"tiny\" for faster loading\n",
        "    use_gpu=True\n",
        ")"
      ],
      "metadata": {
        "id": "rdZ5YH2a8aZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Display some basic statistics about the graph\n",
        "print(\"\\n=== Knowledge Graph Statistics ===\")\n",
        "print(f\"Total nodes: {len(G.nodes())}\")\n",
        "print(f\"Total edges: {len(G.edges())}\")\n",
        "\n",
        "# Count videos by difficulty\n",
        "difficulty_counts = {'beginner': 0, 'intermediate': 0, 'advanced': 0, 'unknown': 0}\n",
        "topics = set()\n",
        "\n",
        "for node_id in G.nodes():\n",
        "    node_data = G.nodes[node_id]\n",
        "\n",
        "    # Count by difficulty\n",
        "    difficulty = node_data.get('difficulty', 'unknown')\n",
        "    if difficulty in difficulty_counts:\n",
        "        difficulty_counts[difficulty] += 1\n",
        "    else:\n",
        "        difficulty_counts['unknown'] += 1\n",
        "\n",
        "    # Collect all topics\n",
        "    topics.update(node_data.get('topics', []))\n",
        "\n",
        "print(f\"Unique topics: {len(topics)}\")\n",
        "print(\"\\nVideos by difficulty:\")\n",
        "for difficulty, count in difficulty_counts.items():\n",
        "    print(f\"  {difficulty}: {count}\")"
      ],
      "metadata": {
        "id": "1qmgy6cX8xit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Run the same example queries from the original code\n",
        "print(\"\\n\\n=== Example 1: Semantic Search with Embeddings ===\")\n",
        "print(\"Searching for videos related to U-net without requiring exact keyword matches...\")\n",
        "result1 = handle_user_query(llm, G, \"Videos related to U-net\", embedding_model, video_embeddings)\n",
        "print(\"\\nSemantic search results:\")\n",
        "for i, video in enumerate(result1.get('videos', [])[:5]):\n",
        "    print(f\"{i+1}. {video.get('title', 'Unknown')} (similarity: {video.get('similarity', 0):.2f})\")\n",
        "\n",
        "print(\"\\n\\n=== Example 2: LLM-based Query with Concept Expansion ===\")\n",
        "print(\"Demonstrating how the system expands queries with related concepts...\")\n",
        "result2 = handle_user_query(llm, G, \"Show me advanced deep learning techniques for image segmentation\", embedding_model, video_embeddings)\n",
        "print(\"\\nLLM query results:\")\n",
        "print(f\"Main topic: {result2.get('topic')}\")\n",
        "print(f\"Related concepts: {', '.join(result2.get('related_concepts', []))}\")\n",
        "for i, video in enumerate(result2.get('videos', [])[:5]):\n",
        "    print(f\"{i+1}. {video.get('title', 'Unknown')} ({video.get('difficulty', 'unknown')})\")\n",
        "\n",
        "print(\"\\n\\n=== Example 3: Simple Query for Beginner Content ===\")\n",
        "print(\"Filtering videos by difficulty level...\")\n",
        "result3 = handle_user_query(llm, G, \"Show me beginner videos on Python\", embedding_model, video_embeddings)\n",
        "print(\"\\nQuery results:\")\n",
        "print(f\"Method used: {result3.get('method', 'unknown')}\")\n",
        "for i, video in enumerate(result3.get('videos', [])[:5]):\n",
        "    print(f\"{i+1}. {video.get('title', 'Unknown')} ({video.get('difficulty', 'unknown')})\")"
      ],
      "metadata": {
        "id": "zxpFzZG286is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Generate a concept-based learning path (using your existing function)\n",
        "print(\"\\n\\n=== Example 4: Generate Concept-Based Learning Path ===\")\n",
        "print(\"Creating a personalized learning journey across multiple concepts...\")\n",
        "concept_path = generate_concept_based_learning_path(llm, G, \"Master computer vision for scientific image analysis\", embedding_model, video_embeddings)\n",
        "\n",
        "# Print concept path summary\n",
        "print(f\"\\nConcept-based learning path summary:\")\n",
        "print(f\"Total concepts: {concept_path.get('total_concepts')}\")\n",
        "print(f\"Total videos: {concept_path.get('total_videos')}\")\n",
        "print(f\"Total duration: {concept_path.get('total_duration_minutes', 0):.1f} minutes\")\n",
        "\n",
        "print(\"\\nConcepts in order:\")\n",
        "# Get only first 5 concepts\n",
        "for i, concept in enumerate(concept_path.get('concepts', [])[:5]):\n",
        "    print(f\"{i+1}. {concept.get('concept')} ({len(concept.get('videos', []))} videos)\")\n",
        "    if i < 3:  # Show videos for first 3 concepts only\n",
        "        for j, video in enumerate(concept.get('videos', [])[:2]):  # Show only first 2 videos per concept\n",
        "            print(f\"   - {video.get('title')}\")\n",
        "\n",
        "print(\"\\nKnowledge graph system loaded and examples completed!\")"
      ],
      "metadata": {
        "id": "sO8mVyT-8Swm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}