{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LW2GR3arDUFyH6LmM7vAGFfxTgBw_3I6",
      "authorship_tag": "ABX9TyO0dtxqfkDOrI9tGZp8KpM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnsreenu/python_for_microscopists/blob/master/357_recommender_system_for_digitalsreeni_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/v2ru1mF58I8"
      ],
      "metadata": {
        "id": "8cUQBsIY0mmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# A very high level overview (detailed explanation in the next text block)\n",
        "\n",
        "In this project, we build a knowledge graph-based recommendation system for educational videos where the nodes represent individual videos and edges capture their relationships. We start by processing video metadata (titles, descriptions, durations) using NLP techniques - specifically, we use spaCy for extracting key concepts and SentenceTransformer for generating semantic embeddings. These embeddings help us understand video relationships as we can calculate similarity scores between them. When building the graph with NetworkX, we create edges based on both semantic similarity and logical prerequisites, while nodes store various attributes like difficulty level, topics covered, and duration.\n",
        "\n",
        "<p>\n",
        "\n",
        "After building the graph, we perform various analyses and visualizations using NetworkX and PyVis - including generating learning paths, visualizing topic relationships, and analyzing video prerequisites. Then, for demo purposes, we serialize and save this graph structure into a SQLite database, storing nodes, edges, and embeddings in separate tables. This is just for learning purposes only so you can learn how to serialize the graph, load it back and build a graph again. After loading the database, to perform further analysis, we don't actually query the SQL database directly - instead, we load the data back into a NetworkX graph structure using load_recommendation_system(). We do have some basic SQL query functions (query_videos_by_topic, get_video_prerequisites), again for learning purposes. The main analytical work like finding learning paths and analyzing relationships still happens using NetworkX methods after reconstructing the graph from the database. This approach suggests we might want to reconsider either making better use of SQL capabilities or exploring graph databases that could maintain the graph structure natively.\n"
      ],
      "metadata": {
        "id": "mWxgYko0b494"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an Educational Content Recommender System Using Knowledge Graphs and NLP\n",
        "\n",
        "This code is designed to illustrate the power of knowledge graphs especially when combined with NLP as a great recommender system for educational content. I will be using my own DigitalSreeni youtube channel videos for this example. First, I got the list of all my videos, their titles and duration from Google Takeout. This is the file we start by importing. Then, we embark on a multi-step process to transform this raw video data into an intelligent recommendation system.\n",
        "\n",
        "## Initial Data Processing\n",
        "\n",
        "The first stage involves loading and preprocessing the video data from the CSV file. Note that I have downloaded my YouTube video titles and meatadata using the Google Takeout tool. The `load_video_data` function handles this initial step, where we:\n",
        "1. Convert video durations from milliseconds to minutes for better readability\n",
        "2. Convert timestamp strings to proper datetime objects for temporal analysis\n",
        "3. Prepare the data for further processing\n",
        "\n",
        "## Text Processing and Feature Extraction\n",
        "\n",
        "The next stage is processing the video titles and descriptions using Natural Language Processing (NLP). This is where the automation magic begins to happen. If you checked out the previous tutorial, this is where we manually defined our graph using python learning topic names. Let us extract these details using NLP. In the `process_video_data` function, we use several NLP techniques:\n",
        "\n",
        "### 1. Text Cleaning\n",
        "- We clean all text by converting to lowercase and removing special characters\n",
        "- Handle any missing values (NaN) in titles or descriptions\n",
        "- Remove extra whitespace and standardize the text format\n",
        "\n",
        "### 2. Semantic Understanding using Sentence Transformers\n",
        "- We use the 'all-MiniLM-L6-v2' model from the sentence-transformers library\n",
        "- This model converts our text into high-dimensional vector embeddings\n",
        "- Each video's title and description are combined and transformed into a vector that captures its semantic meaning\n",
        "- These embeddings allow us to measure semantic similarity between videos, even if they don't share exact words\n",
        "\n",
        "### 3. Concept Extraction using spaCy\n",
        "The `extract_key_concepts` function uses spaCy's powerful NLP capabilities to:\n",
        "- Identify noun phrases that represent key concepts in the video content\n",
        "- Extract named entities (like Python, OpenCV, etc.)\n",
        "- Focus on phrases of 3 words or less to capture concise, meaningful concepts\n",
        "- Remove duplicates and clean the extracted concepts\n",
        "\n",
        "This process helps us understand what each video is about at a conceptual level. For example, from a video titled \"Introduction to Python Image Processing with OpenCV\", it might extract concepts like:\n",
        "- python\n",
        "- image processing\n",
        "- opencv\n",
        "- introduction\n",
        "These concepts become crucial for building relationships between videos.\n",
        "\n",
        "### 4. Topic Identification\n",
        "Again, in the previous tutorial we manually defined a few topics but here we will assign our YouTube videos into one of these several topics. The `extract_video_topics` function categorizes videos into predefined topics like:\n",
        "- Python basics\n",
        "- Image processing\n",
        "- Machine learning\n",
        "- Computer vision\n",
        "- Bio applications\n",
        "This categorization is done by looking for specific keywords in the title and description, helping us organize content into meaningful groups.\n",
        "\n",
        "### 5. Difficulty Assessment\n",
        "We automatically infer video difficulty levels (beginner, intermediate, advanced) based on several factors:\n",
        "- Explicit indicators in the title (like \"introduction\", \"advanced\")\n",
        "- Video sequence number (earlier videos tend to be more basic)\n",
        "- Number of technical concepts covered\n",
        "- Complexity of the content\n",
        "\n",
        "This way of automatic processing of the video content creates the foundation for building our knowledge graph. The extracted features, concepts, and relationships will determine how videos are connected and how we can navigate between them effectively.\n",
        "\n",
        "## Building the Knowledge Graph\n",
        "\n",
        "After processing our video content, we construct a knowledge graph that represents the relationships between videos. This is where our system becomes truly powerful, as it captures not just the content of videos, but how they relate to each other in meaningful ways.\n",
        "\n",
        "### Graph Construction Process\n",
        "\n",
        "The knowledge graph is built using NetworkX, with each video becoming a node in the graph. The `build_knowledge_graph` function handles this process:\n",
        "\n",
        "1. **Node Creation**\n",
        "   - Each video becomes a node in our graph\n",
        "   - Node attributes include:\n",
        "     - Title\n",
        "     - Description\n",
        "     - Difficulty level\n",
        "     - Duration\n",
        "     - Key concepts\n",
        "     - Topics covered\n",
        "     - Video number (if part of a series)\n",
        "   These rich attributes allow us to understand each video's content and context thoroughly.\n",
        "\n",
        "2. **Edge Creation**\n",
        "   The most interesting part is how we automatically create edges between videos. We use multiple factors:\n",
        "\n",
        "   a) **Semantic Similarity**\n",
        "   - Use the cosine similarity between video embeddings\n",
        "   - Connect videos if their similarity exceeds a threshold (default for now is 0.5)\n",
        "   - This catches semantic relationships even when videos don't share exact keywords\n",
        "\n",
        "   b) **Relationship Type Determination**\n",
        "   The `determine_relationship` function assigns relationship types between videos:\n",
        "   - **Prerequisite**: Video A should be watched before Video B\n",
        "   - **Advanced**: Video B builds upon concepts in Video A\n",
        "   - **Related**: Videos cover related topics but don't have a strict order\n",
        "\n",
        "   Relationships are determined based on:\n",
        "   - Video sequence numbers (if part of a series)\n",
        "   - Difficulty levels (beginner → intermediate → advanced)\n",
        "   - Topic relationships (e.g., Python basics are prerequisites for advanced topics)\n",
        "\n",
        "### Graph Visualization\n",
        "\n",
        "We create several types of visualizations to understand our knowledge graph:\n",
        "\n",
        "1. **Complete Knowledge Graph** (`visualize_knowledge_graph`)\n",
        "   - Shows all videos and their relationships\n",
        "   - Color-coded by difficulty level:\n",
        "     - Light green for beginner\n",
        "     - Light blue for intermediate\n",
        "     - Light pink for advanced\n",
        "   - Interactive visualization where you can:\n",
        "     - Hover over nodes to see video details\n",
        "     - Drag nodes to explore relationships\n",
        "     - Zoom in/out to focus on specific areas\n",
        "\n",
        "2. **Topic Subgraphs** (`visualize_topic_subgraph`)\n",
        "   - Shows videos related to specific topics (e.g., 'python_basics', 'image_processing')\n",
        "   - Helps understand the structure within each topic area\n",
        "   - Useful for seeing prerequisite chains within a topic\n",
        "\n",
        "3. **Simplified Graph** (`visualize_simplified_graph`)\n",
        "   - Two versions:\n",
        "     - Strong connections only (high similarity weight)\n",
        "     - Python basics structure (foundational content)\n",
        "   - Helps see the core structure of the content\n",
        "   - Reduces visual complexity for better understanding\n",
        "\n",
        "4. **Learning Path Visualization** (`visualize_learning_path`)\n",
        "   - Shows the recommended sequence of videos for a specific learning goal\n",
        "   - Highlights the progression from prerequisites to advanced content\n",
        "   - Color-coded to show difficulty progression\n",
        "\n",
        "All visualizations are saved as interactive HTML files using the pyvis library, which allows for:\n",
        "- Zooming and panning\n",
        "- Node dragging for better arrangement\n",
        "- Hovering for detailed information\n",
        "- Physics-based layout for natural clustering\n",
        "\n",
        "These visualizations not only help us understand the structure of our educational content but also validate the relationships our system has identified. They're particularly useful for:\n",
        "- Identifying gaps in content\n",
        "- Ensuring proper prerequisite chains\n",
        "- Finding isolated content that needs better integration\n",
        "- Understanding the overall structure of the educational material\n",
        "\n",
        "## Learning Path Generation and Recommendations\n",
        "\n",
        "Once our knowledge graph is built, we can use it to generate personalized learning paths and make intelligent recommendations. This is where the system demonstrates its real power in educational content organization.\n",
        "\n",
        "### Learning Path Generation\n",
        "\n",
        "The `find_learning_path` function is central to our recommendation system. Here's how it works:\n",
        "\n",
        "1. **Goal Understanding**\n",
        "   - Takes a learning goal as input (e.g., \"Mastering Python for Bioimage analysis\")\n",
        "   - Converts the goal into an embedding using the same sentence transformer model\n",
        "   - Compares this embedding with all video embeddings to find relevant content\n",
        "\n",
        "2. **Topic Prerequisites**\n",
        "   We define prerequisite relationships between topics. For example:\n",
        "   ```\n",
        "   'image_processing': needs {'python_basics', 'data_structures'}\n",
        "   'machine_learning': needs {'python_basics', 'data_structures', 'data_analysis'}\n",
        "   'computer_vision': needs {'python_basics', 'image_processing'}\n",
        "   'bio_applications': needs {'python_basics', 'image_processing'}\n",
        "   ```\n",
        "\n",
        "3. **Path Construction**\n",
        "   The system builds a path through the content by:\n",
        "   a) Starting with foundational content\n",
        "      - Automatically includes Python basics for technical topics\n",
        "      - Sorts basics by video number to maintain logical progression\n",
        "   \n",
        "   b) Adding topic-specific content\n",
        "      - Uses similarity scores to find most relevant videos\n",
        "      - Checks and includes prerequisites before advanced content\n",
        "      - Ensures proper skill progression\n",
        "\n",
        "### Path Analysis and Validation\n",
        "\n",
        "The `analyze_path_coverage` function examines generated paths to ensure quality:\n",
        "\n",
        "1. **Topic Coverage**\n",
        "   - Tracks which topics are covered in the path\n",
        "   - Ensures all necessary prerequisite topics are included\n",
        "\n",
        "2. **Prerequisite Validation**\n",
        "   - Checks if advanced topics (like machine learning) have necessary basics\n",
        "   - Issues warnings if prerequisites are missing\n",
        "   - Helps maintain logical learning progression\n",
        "\n",
        "3. **Path Statistics**\n",
        "   We collect detailed statistics about each path:\n",
        "   - Total number of videos\n",
        "   - Total duration\n",
        "   - Difficulty breakdown\n",
        "   - Topic coverage\n",
        "   - Concept progression\n",
        "\n",
        "### Querying the System\n",
        "\n",
        "Our system supports various types of queries:\n",
        "\n",
        "1. **Topic-Based Queries** (`query_videos_by_topic`)\n",
        "   - Find videos by topic and difficulty level\n",
        "   - Useful for focused learning in specific areas\n",
        "   - Can filter by beginner/intermediate/advanced content\n",
        "\n",
        "2. **Prerequisite Queries** (`get_video_prerequisites`)\n",
        "   - Find what videos should be watched before a specific video\n",
        "   - Shows relevance scores for each prerequisite\n",
        "   - Helps ensure proper preparation for advanced content\n",
        "\n",
        "3. **Learning Path Queries** (`get_learning_path_from_db`)\n",
        "   - Generate complete learning paths for specific goals\n",
        "   - Includes:\n",
        "     - Step-by-step video sequence\n",
        "     - Estimated completion time\n",
        "     - Difficulty progression\n",
        "     - Topic coverage analysis\n",
        "\n",
        "### System Persistence and Database Structure\n",
        "\n",
        "Instead of using simple serialization (like pickle), we use SQLite because:\n",
        "1. **Structured Storage**\n",
        "   - Maintains relationships between different components\n",
        "   - Allows for complex queries\n",
        "   - Ensures data integrity\n",
        "\n",
        "2. **Efficient Querying**\n",
        "   - Fast retrieval of video information\n",
        "   - Efficient path generation\n",
        "   - Quick prerequisite lookups\n",
        "\n",
        "3. **Data Organization**\n",
        "   Our database has three main tables:\n",
        "   - `nodes`: Stores video information\n",
        "   - `edges`: Stores relationships between videos\n",
        "   - `embeddings`: Stores video embeddings for similarity calculations\n",
        "\n",
        "This structured storage allows us to:\n",
        "- Quickly rebuild the knowledge graph\n",
        "- Run complex queries efficiently\n",
        "- Maintain relationship integrity\n",
        "- Update content without rebuilding everything\n",
        "\n",
        "## System Outputs and Practical Applications\n",
        "\n",
        "Let's look at what we get from this system and how it can be practically used.\n",
        "\n",
        "### Output Organization\n",
        "\n",
        "All system outputs are organized in a structured directory:\n",
        "```\n",
        "/video_recommender/results/\n",
        "├── database/\n",
        "│   └── video_recommender.db\n",
        "├── visualizations/\n",
        "│   ├── knowledge_graph.html\n",
        "│   ├── learning_path_*.html\n",
        "│   ├── topic_subgraph_*.html\n",
        "│   └── simplified_*.html\n",
        "└── queries/\n",
        "    ├── learning_path_*.txt\n",
        "    └── topic_query_*.txt\n",
        "```\n",
        "\n",
        "### Practical Applications\n",
        "\n",
        "1. **For Content Creators (like myself)**\n",
        "   - **Content Gap Analysis**\n",
        "     - Identify missing prerequisite content\n",
        "     - Spot areas needing more advanced material\n",
        "     - See which topics are under-represented\n",
        "\n",
        "   - **Content Organization**\n",
        "     - Verify logical progression of video series\n",
        "     - Ensure proper coverage of prerequisites\n",
        "     - Plan future content based on graph structure\n",
        "\n",
        "   - **Channel Management**\n",
        "     - Optimize playlist organization\n",
        "     - Create better video descriptions\n",
        "     - Link related videos effectively\n",
        "\n",
        "2. **For Learners**\n",
        "   - **Personalized Learning Paths**\n",
        "     - Get customized pathways for specific goals\n",
        "     - Understand prerequisites clearly\n",
        "     - Track learning progress\n",
        "\n",
        "   - **Topic Exploration**\n",
        "     - Find related content easily\n",
        "     - Understand topic relationships\n",
        "     - Choose appropriate difficulty levels\n",
        "\n",
        "3. **For Educational Institutions**\n",
        "   - **Curriculum Planning**\n",
        "     - Design coherent course sequences\n",
        "     - Ensure proper skill progression\n",
        "     - Create balanced learning paths\n",
        "\n",
        "### Real-World Impact\n",
        "\n",
        "In my YouTube channel (DigitalSreeni), this system helps:\n",
        "- New viewers find appropriate starting points\n",
        "- Regular viewers progress logically through topics\n",
        "- Advanced viewers find specific content quickly\n",
        "- Me (as a content creator) maintain content coherence\n",
        "\n",
        "### Future Extensions and Possibilities\n",
        "\n",
        "1. **Technical Enhancements**\n",
        "   - Real-time content updates\n",
        "   - User feedback integration\n",
        "   - More sophisticated difficulty estimation\n",
        "   - Advanced recommendation algorithms\n",
        "\n",
        "2. **Feature Additions**\n",
        "   - User progress tracking\n",
        "   - Interactive learning path modification\n",
        "   - Content engagement metrics\n",
        "   - Multi-language support\n",
        "\n",
        "3. **Integration Possibilities**\n",
        "   - YouTube API integration\n",
        "   - Learning Management Systems (LMS)\n",
        "   - Course creation platforms\n",
        "   - Social learning features\n",
        "\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "The goal is to make this a comprehensive project on building a recommender system for learning content using knowledge graphs and NLP. While I've used it for my YouTube channel, the principles and techniques can be applied to any educational content platform.\n"
      ],
      "metadata": {
        "id": "BiAApEiCjWDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers spacy gensim pandas numpy networkx pyvis\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "51-GXVxMxgBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "import re\n",
        "import networkx as nx\n",
        "from typing import List, Dict, Set, Tuple\n",
        "from gensim import corpora, models\n",
        "import sqlite3\n",
        "import json\n",
        "from pyvis.network import Network\n",
        "import os"
      ],
      "metadata": {
        "id": "2TYRbl-pSVGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Personal note, delete later: Add Doc strings to each function. May be chatGPT can help?\n",
        "\n",
        "def load_video_data(csv_path: str = 'combined_videos.csv') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load and preprocess the video data from CSV. Convert ms to minutes and\n",
        "    date to proper datetime format. Not sure if we will use the date but why not\n",
        "    make it ready for use.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Convert duration from milliseconds to minutes\n",
        "    df['duration_minutes'] = df['Approx Duration (ms)'] / (1000 * 60)\n",
        "\n",
        "    # Convert timestamp to datetime\n",
        "    df['publish_date'] = pd.to_datetime(df['Video Publish Timestamp'])\n",
        "\n",
        "    return df\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean and preprocess text.\n",
        "    Takes a text string, converts it to lowercase, removes special characters,\n",
        "    extra spaces, and returns the cleaned result.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase and remove special characters\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def extract_key_concepts(text: str, nlp) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract key concepts from text using spaCy.\n",
        "    This process helps us understand what each video is about at a conceptual level.\n",
        "    For example, from a video titled \"Introduction to Python Image Processing\n",
        "    with OpenCV\", it might extract concepts like:\n",
        "        python\n",
        "        image processing\n",
        "        opencv\n",
        "        introduction\n",
        "    These concepts become important for building relationships between videos.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    concepts = []\n",
        "\n",
        "    # Extract noun phrases and named entities\n",
        "    for chunk in doc.noun_chunks:     #doc.noun_chunks is a generator provided by Spacy's Doc object and gives noun phrases from the text.\n",
        "        if len(chunk.text.split()) <= 3:  # Limit to phrases of 3 words or less, just so we work with concise phrases\n",
        "            concepts.append(chunk.text)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if len(ent.text.split()) <= 3:\n",
        "            concepts.append(ent.text)\n",
        "\n",
        "    # Remove duplicates and clean\n",
        "    concepts = list(set([clean_text(c) for c in concepts]))\n",
        "    return [c for c in concepts if c]  # Remove empty strings\n",
        "\n",
        "def process_video_data(video_data: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, Dict]:\n",
        "    \"\"\"\n",
        "    Process video data and create embeddings for semantic understanding\n",
        "\n",
        "    We use the 'all-MiniLM-L6-v2' model from the sentence-transformers library\n",
        "    This model converts our text into high-dimensional vector embeddings\n",
        "    Each video's title and description are combined and transformed into a vector that captures its semantic meaning\n",
        "    These embeddings allow us to measure semantic similarity between videos, even if they don't share exact words\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialize models\n",
        "    transformer_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    print(\"Cleaning text...\")\n",
        "    # Clean titles and descriptions\n",
        "    video_data['cleaned_title'] = video_data['Video Title (Original)'].apply(clean_text)\n",
        "    video_data['cleaned_description'] = video_data['Video Description (Original)'].apply(clean_text)\n",
        "\n",
        "    # Combine title and description for embedding\n",
        "    video_data['combined_text'] = video_data['cleaned_title'] + \" \" + video_data['cleaned_description']\n",
        "\n",
        "    print(\"Creating embeddings...\")\n",
        "    # Create embeddings\n",
        "    embeddings = transformer_model.encode(\n",
        "        video_data['combined_text'].tolist(),\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    # Calculate similarity matrix\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    print(\"Extracting concepts...\")\n",
        "    # Extract concepts for each video - by building a dictionary that maps each video (by its index)\n",
        "    # to a list of key concepts extracted from its combined title and description.\n",
        "    #Example structure of the final result....\n",
        "    # {\n",
        "    #     0: ['python', 'image processing', 'opencv'],  # Concepts for video 0\n",
        "    #     1: ['machine learning', 'classification', 'sklearn'],  # Concepts for video 1\n",
        "    #     2: ['deep learning', 'segmentation', 'tensorflow']  # Concepts for video 2\n",
        "    # }\n",
        "\n",
        "    video_concepts = {}\n",
        "    for idx, row in video_data.iterrows():\n",
        "        concepts = extract_key_concepts(row['combined_text'], nlp)\n",
        "        video_concepts[idx] = concepts\n",
        "\n",
        "    return video_data, embeddings, similarity_matrix, video_concepts\n",
        "\n",
        "def extract_video_topics(title: str, description: str) -> Set[str]:\n",
        "    \"\"\"\n",
        "    Extract main topics from video title and description\n",
        "    categorizes videos into predefined topics like:\n",
        "        Python basics\n",
        "        Image processing\n",
        "        Machine learning\n",
        "        Computer vision\n",
        "        Bio applications\n",
        "This categorization is done by looking for specific keywords in the title and\n",
        "description, helping us organize content into meaningful groups.\n",
        "Basically, the output looks like this:\n",
        "Input: text = \"introduction to python image processing learn how to use opencv for microscopy image analysis and visualization\"\n",
        "Output: {'python_basics', 'image_processing', 'computer_vision'}\n",
        "\n",
        "    \"\"\"\n",
        "    # Handle NaN values\n",
        "    title = str(title) if not pd.isna(title) else \"\"\n",
        "    description = str(description) if not pd.isna(description) else \"\"\n",
        "\n",
        "    text = (title + \" \" + description).lower()\n",
        "    topics = set()\n",
        "\n",
        "    # Define topic categories\n",
        "    topic_keywords = {\n",
        "        'python_basics': {'python basics', 'introduction', 'variables', 'functions', 'loops', 'conditionals'},\n",
        "        'data_structures': {'lists', 'dictionaries', 'arrays', 'data structures'},\n",
        "        'image_processing': {'image', 'microscopy', 'bioimage', 'visualization', 'processing'},\n",
        "        'machine_learning': {'machine learning', 'deep learning', 'neural network', 'classification'},\n",
        "        'data_analysis': {'data analysis', 'statistics', 'pandas', 'numpy'},\n",
        "        'computer_vision': {'opencv', 'vision', 'object detection', 'segmentation'},\n",
        "        'bio_applications': {'cell', 'tissue', 'microscopy', 'biology', 'medical'}\n",
        "    }\n",
        "\n",
        "    for topic, keywords in topic_keywords.items():\n",
        "        if any(keyword in text for keyword in keywords):\n",
        "            topics.add(topic)\n",
        "\n",
        "    return topics\n",
        "\n",
        "def infer_video_difficulty(title: str, concepts: List[str], video_number: int = None) -> str:\n",
        "    \"\"\"\n",
        "    Infer difficulty level of a video\n",
        "    by looking for some words that represent beginner or advanced\n",
        "    by video number, as my first videos are basic intro videos\n",
        "    also by number of concepts, like python, opencv, etc. etc.\n",
        "\n",
        "    \"\"\"\n",
        "    title = title.lower()\n",
        "\n",
        "    # Check for explicit difficulty indicators\n",
        "    if any(word in title for word in ['introduction', 'basics', 'beginner', 'what is']):\n",
        "        return 'beginner'\n",
        "    elif any(word in title for word in ['advanced', 'expert', 'complex']):\n",
        "        return 'advanced'\n",
        "\n",
        "    # Consider video number in series\n",
        "    if video_number is not None:\n",
        "        if video_number <= 20:\n",
        "            return 'beginner'\n",
        "        elif video_number > 50:\n",
        "            return 'advanced'\n",
        "\n",
        "    # Check concept complexity\n",
        "    concept_count = len(concepts)\n",
        "    if concept_count < 5:\n",
        "        return 'beginner'\n",
        "    elif concept_count > 10:\n",
        "        return 'advanced'\n",
        "\n",
        "    return 'intermediate'"
      ],
      "metadata": {
        "id": "OPfwMjEoxg2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_knowledge_graph(video_data: pd.DataFrame,\n",
        "                        similarity_matrix: np.ndarray,\n",
        "                        video_concepts: Dict,\n",
        "                        similarity_threshold: float = 0.6) -> nx.DiGraph:\n",
        "    \"\"\"\n",
        "    Build a knowledge graph from video data using NetworkX\n",
        "    Build nodes and edges. Nodes are basically videos and they have various attributes (metadata).\n",
        "\n",
        "    Edges are added based on similarity we calculate (using cosine).\n",
        "    Basically semantic Similarity between video embeddings\n",
        "    Connect videos if their similarity exceeds a threshold (default 0.5 but changed to 0.6 to minimize the number of edges.)\n",
        "    This catches semantic relationships even when videos don't share exact keywords\n",
        "\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes (videos)\n",
        "    for idx, row in video_data.iterrows():\n",
        "        # Extract video number if present\n",
        "        video_num = None\n",
        "        match = re.search(r'(\\d+)', row['Video Title (Original)'])\n",
        "        if match:\n",
        "            video_num = int(match.group(1))\n",
        "\n",
        "        # Get video topics\n",
        "        topics = extract_video_topics(row['Video Title (Original)'],\n",
        "                                   row['Video Description (Original)'])\n",
        "\n",
        "        # Infer difficulty\n",
        "        difficulty = infer_video_difficulty(row['Video Title (Original)'],\n",
        "                                         video_concepts[idx],\n",
        "                                         video_num)\n",
        "\n",
        "        # Add node with metadata\n",
        "        G.add_node(idx,\n",
        "                  title=row['Video Title (Original)'],\n",
        "                  description=row['Video Description (Original)'],\n",
        "                  concepts=video_concepts[idx],\n",
        "                  topics=topics,\n",
        "                  difficulty=difficulty,\n",
        "                  duration=row['Approx Duration (ms)'],\n",
        "                  video_number=video_num)\n",
        "\n",
        "    # Add edges based on similarity and relationships\n",
        "    for i in range(len(video_data)):\n",
        "        for j in range(len(video_data)):\n",
        "            if i != j and similarity_matrix[i][j] >= similarity_threshold:   #similarity_matrix is nothing but our cosine similarity\n",
        "                # Determine edge type and direction\n",
        "                edge_type = determine_relationship(G, i, j)  #determine_relationship is defined next\n",
        "\n",
        "                G.add_edge(i, j,\n",
        "                          weight=similarity_matrix[i][j],\n",
        "                          type=edge_type)\n",
        "\n",
        "    return G\n",
        "\n",
        "def determine_relationship(G: nx.DiGraph, video1_idx: int, video2_idx: int) -> str:\n",
        "    \"\"\"\n",
        "    Determine the relationship type between two videos\n",
        "\n",
        "    Prerequisite: Video A should be watched before Video B\n",
        "    Advanced: Video B builds upon concepts in Video A\n",
        "    Related: Videos cover related topics but don't have a strict order\n",
        "\n",
        "    Relationships are determined based on:\n",
        "      Video sequence numbers (if part of a series)\n",
        "      Difficulty levels (beginner → intermediate → advanced)\n",
        "      Topic relationships (e.g., Python basics are prerequisites for advanced topics)\n",
        "\n",
        "    \"\"\"\n",
        "    v1 = G.nodes[video1_idx]\n",
        "    v2 = G.nodes[video2_idx]\n",
        "\n",
        "    # Check if videos are part of a numbered series\n",
        "    if (v1['video_number'] is not None and\n",
        "        v2['video_number'] is not None):\n",
        "        if v1['video_number'] < v2['video_number']:\n",
        "            return 'prerequisite'\n",
        "        elif v1['video_number'] > v2['video_number']:\n",
        "            return 'advanced'\n",
        "\n",
        "    # Compare difficulty levels\n",
        "    diff_levels = ['beginner', 'intermediate', 'advanced']\n",
        "    v1_diff_idx = diff_levels.index(v1['difficulty'])\n",
        "    v2_diff_idx = diff_levels.index(v2['difficulty'])\n",
        "\n",
        "    if v1_diff_idx < v2_diff_idx:\n",
        "        return 'prerequisite'\n",
        "    elif v1_diff_idx > v2_diff_idx:\n",
        "        return 'advanced'\n",
        "\n",
        "    # Check topic relationships\n",
        "    v1_topics = v1['topics']\n",
        "    v2_topics = v2['topics']\n",
        "\n",
        "    if 'python_basics' in v1_topics and not 'python_basics' in v2_topics:\n",
        "        return 'prerequisite'\n",
        "\n",
        "    return 'related'"
      ],
      "metadata": {
        "id": "LbNzbSe_UlLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "## We are using pyviz for visualization, make sit easy for a complex graph like this one.\n",
        "def visualize_knowledge_graph(G: nx.DiGraph, filename='knowledge_graph.html'):\n",
        "    \"\"\"\n",
        "    Create interactive visualization of the knowledge graph (we are using pyviz)\n",
        "    \"\"\"\n",
        "\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'visualizations', filename)\n",
        "    net = Network(height='750px', width='100%', bgcolor='#ffffff',\n",
        "                 font_color='#000000', directed=True, notebook=True)\n",
        "\n",
        "    # Set physics layout options\n",
        "    net.force_atlas_2based()\n",
        "    net.show_buttons(filter_=['physics'])\n",
        "\n",
        "    # Color mapping for different difficulty levels\n",
        "    color_map = {\n",
        "        'beginner': '#90EE90',      # light green\n",
        "        'intermediate': '#ADD8E6',   # light blue\n",
        "        'advanced': '#FFB6C1'        # light pink\n",
        "    }\n",
        "\n",
        "    # Add nodes\n",
        "    for node_id in G.nodes():\n",
        "        node_data = G.nodes[node_id]\n",
        "        title = node_data['title']\n",
        "        difficulty = node_data['difficulty']\n",
        "        duration = node_data['duration'] / (1000 * 60)  # Convert to minutes\n",
        "        concepts = ', '.join(node_data['concepts'][:5])\n",
        "        topics = ', '.join(node_data['topics'])\n",
        "\n",
        "        hover_text = f\"\"\"\n",
        "        Title: {title}\n",
        "        Topics: {topics}\n",
        "        Difficulty: {difficulty}\n",
        "        Duration: {duration:.1f} min\n",
        "        Key Concepts: {concepts}\n",
        "        \"\"\"\n",
        "\n",
        "        net.add_node(\n",
        "            str(node_id),\n",
        "            label=title[:20] + \"...\",\n",
        "            title=hover_text,\n",
        "            color=color_map[difficulty],\n",
        "            size=20\n",
        "        )\n",
        "\n",
        "    # Add edges\n",
        "    for edge in G.edges(data=True):\n",
        "        source, target, data = edge\n",
        "        net.add_edge(\n",
        "            str(source),\n",
        "            str(target),\n",
        "            value=data['weight'] * 2,\n",
        "            title=f\"Type: {data['type']}\\nWeight: {data['weight']:.2f}\",\n",
        "            arrows='to'\n",
        "        )\n",
        "\n",
        "    # Save the network\n",
        "    net.save_graph(output_path)\n",
        "    print(f\"Knowledge graph visualization saved to {output_path}\")\n",
        "\n",
        "def visualize_learning_path(G: nx.DiGraph, path: List[int], filename='learning_path.html'):\n",
        "    \"\"\"\n",
        "    Visualize a specific learning path\n",
        "    Create an interactive visualization of a learning path in a directed graph,\n",
        "    highlighting nodes and edges with details like title, difficulty, and duration,\n",
        "    and save it as an HTML file.\n",
        "\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'visualizations', filename)\n",
        "    net = Network(height='750px', width='100%', bgcolor='#ffffff',\n",
        "                 font_color='#000000', directed=True, notebook=True)\n",
        "\n",
        "    # Color mapping\n",
        "    color_map = {\n",
        "        'beginner': '#90EE90',\n",
        "        'intermediate': '#ADD8E6',\n",
        "        'advanced': '#FFB6C1'\n",
        "    }\n",
        "\n",
        "    # Add nodes in path order\n",
        "    for i, node_id in enumerate(path):\n",
        "        node_data = G.nodes[node_id]\n",
        "        title = node_data['title']\n",
        "        difficulty = node_data['difficulty']\n",
        "        duration = node_data['duration'] / (1000 * 60)\n",
        "\n",
        "        hover_text = f\"\"\"\n",
        "        Step {i+1}\n",
        "        Title: {title}\n",
        "        Difficulty: {difficulty}\n",
        "        Duration: {duration:.1f} min\n",
        "        Topics: {', '.join(node_data['topics'])}\n",
        "        \"\"\"\n",
        "\n",
        "        net.add_node(\n",
        "            str(node_id),\n",
        "            label=f\"{i+1}. {title[:20]}...\",\n",
        "            title=hover_text,\n",
        "            color=color_map[difficulty],\n",
        "            size=20\n",
        "        )\n",
        "\n",
        "    # Add edges between consecutive nodes in the path\n",
        "    for i in range(len(path)-1):\n",
        "        source = path[i]\n",
        "        target = path[i+1]\n",
        "        if G.has_edge(source, target):\n",
        "            edge_data = G.get_edge_data(source, target)\n",
        "            net.add_edge(\n",
        "                str(source),\n",
        "                str(target),\n",
        "                value=edge_data['weight'] * 2,\n",
        "                title=f\"Type: {edge_data['type']}\",\n",
        "                arrows='to'\n",
        "            )\n",
        "\n",
        "    # Save the network\n",
        "    net.save_graph(output_path)\n",
        "    print(f\"Learning path visualization saved to {output_path}\")\n",
        "\n",
        "def visualize_topic_subgraph(G: nx.DiGraph, topic: str, filename=None):\n",
        "    \"\"\"\n",
        "    Visualize subgraph for a specific topic\n",
        "    Generate an interactive visualization of a subgraph for a specific topic,\n",
        "    highlighting related nodes and edges with difficulty-based colors.\n",
        "    \"\"\"\n",
        "    if filename is None:\n",
        "        filename = f'{topic}_subgraph.html'\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'visualizations', filename)\n",
        "\n",
        "    # Find nodes related to topic\n",
        "    topic_nodes = [n for n, d in G.nodes(data=True) if topic in d['topics']]\n",
        "\n",
        "    # Get subgraph\n",
        "    subgraph = G.subgraph(topic_nodes)\n",
        "\n",
        "    # Create visualization\n",
        "    net = Network(height='750px', width='100%', bgcolor='#ffffff',\n",
        "                 directed=True, notebook=True)\n",
        "\n",
        "    # Add nodes with difficulty-based colors\n",
        "    difficulty_colors = {\n",
        "        'beginner': '#90EE90',\n",
        "        'intermediate': '#ADD8E6',\n",
        "        'advanced': '#FFB6C1'\n",
        "    }\n",
        "\n",
        "    for node_id in subgraph.nodes():\n",
        "        node_data = subgraph.nodes[node_id]\n",
        "        net.add_node(\n",
        "            str(node_id),\n",
        "            label=node_data['title'][:30] + \"...\",\n",
        "            title=f\"Title: {node_data['title']}\\nDifficulty: {node_data['difficulty']}\",\n",
        "            color=difficulty_colors[node_data['difficulty']],\n",
        "            size=20\n",
        "        )\n",
        "\n",
        "    # Add edges\n",
        "    for u, v, data in subgraph.edges(data=True):\n",
        "        net.add_edge(str(u), str(v), value=data['weight'] * 2)\n",
        "\n",
        "    net.save_graph(output_path)\n",
        "    print(f\"Topic subgraph saved to {output_path}\")\n",
        "\n",
        "def visualize_simplified_graph(G: nx.DiGraph, min_weight=0.7, show_basics_only=False, filename=None):\n",
        "    \"\"\"\n",
        "    Creates a simplified visualization of the knowledge graph,\n",
        "    filtering nodes and edges by importance or topic.\n",
        "\n",
        "    \"\"\"\n",
        "    if filename is None:\n",
        "        filename = 'simplified_graph.html'\n",
        "        if show_basics_only:\n",
        "            filename = 'simplified_basics_only_graph.html'\n",
        "\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'visualizations', filename)\n",
        "\n",
        "    net = Network(height='750px', width='100%', bgcolor='#ffffff',\n",
        "                 directed=True, notebook=True)\n",
        "\n",
        "    # Customize physics for better layout\n",
        "    net.set_options(\"\"\"\n",
        "    const options = {\n",
        "        \"physics\": {\n",
        "            \"forceAtlas2Based\": {\n",
        "                \"gravitationalConstant\": -100,\n",
        "                \"springLength\": 100,\n",
        "                \"springConstant\": 0.1\n",
        "            },\n",
        "            \"maxVelocity\": 50,\n",
        "            \"minVelocity\": 0.1,\n",
        "            \"solver\": \"forceAtlas2Based\"\n",
        "        },\n",
        "        \"edges\": {\n",
        "            \"smooth\": {\n",
        "                \"type\": \"continuous\",\n",
        "                \"forceDirection\": \"none\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "    # Filter nodes based on criteria\n",
        "    nodes_to_include = []\n",
        "    for node_id in G.nodes():\n",
        "        node_data = G.nodes[node_id]\n",
        "        if (show_basics_only and 'python_basics' in node_data['topics']) or \\\n",
        "           (not show_basics_only and len(list(G.neighbors(node_id))) > 3):\n",
        "            nodes_to_include.append(node_id)\n",
        "\n",
        "    # Create subgraph\n",
        "    subgraph = G.subgraph(nodes_to_include)\n",
        "\n",
        "    # Color scheme\n",
        "    color_map = {\n",
        "        'python_basics': '#90EE90',\n",
        "        'image_processing': '#ADD8E6',\n",
        "        'machine_learning': '#FFB6C1',\n",
        "        'bio_applications': '#DDA0DD'\n",
        "    }\n",
        "\n",
        "    # Add nodes with simplified information\n",
        "    for node_id in subgraph.nodes():\n",
        "        node_data = subgraph.nodes[node_id]\n",
        "        primary_topic = next(\n",
        "            (topic for topic in ['python_basics', 'image_processing',\n",
        "                               'machine_learning', 'bio_applications']\n",
        "             if topic in node_data['topics']),\n",
        "            'other'\n",
        "        )\n",
        "\n",
        "        net.add_node(\n",
        "            str(node_id),\n",
        "            label=node_data['title'][:30] + \"...\" if len(node_data['title']) > 30 else node_data['title'],\n",
        "            title=f\"Title: {node_data['title']}\\nDifficulty: {node_data['difficulty']}\",\n",
        "            color=color_map.get(primary_topic, '#DCDCDC'),\n",
        "            size=20\n",
        "        )\n",
        "\n",
        "    # Add important edges\n",
        "    for u, v, data in subgraph.edges(data=True):\n",
        "        if data['weight'] >= min_weight:\n",
        "            net.add_edge(\n",
        "                str(u),\n",
        "                str(v),\n",
        "                value=data['weight'] * 2,\n",
        "                title=f\"Relationship: {data['type']}\"\n",
        "            )\n",
        "\n",
        "    net.save_graph(output_path)\n",
        "    print(f\"Simplified graph visualization saved to {output_path}\")"
      ],
      "metadata": {
        "id": "91XEJG4ixqof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "MahFWT3YXiUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_learning_path(G: nx.DiGraph,\n",
        "                      goal: str,\n",
        "                      embeddings: np.ndarray,\n",
        "                      transformer_model) -> List[int]:\n",
        "    \"\"\"\n",
        "    Find optimal learning path for a given goal\n",
        "\n",
        "    This function is key to our recommendation system. Here's how it works:\n",
        "\n",
        "    Goal Understanding:\n",
        "    Takes a learning goal as input (e.g., \"Mastering Python for Bioimage analysis\")\n",
        "    Converts the goal into an embedding using the same sentence transformer model\n",
        "    Compares this embedding with all video embeddings to find relevant content\n",
        "\n",
        "    Topic Prerequisites\n",
        "    We manually define prerequisite relationships between topics. This ensures that\n",
        "    before a learner is shown advanced videos on a topic (e.g., machine learning),\n",
        "    they are first recommended videos covering the necessary foundational knowledge.\n",
        "\n",
        "    For example:\n",
        "    'image_processing': needs {'python_basics', 'data_structures'}\n",
        "    'machine_learning': needs {'python_basics', 'data_structures', 'data_analysis'}\n",
        "    'computer_vision': needs {'python_basics', 'image_processing'}\n",
        "    'bio_applications': needs {'python_basics', 'image_processing'}\n",
        "\n",
        "    Path Construction:\n",
        "    The system builds a path through the content by:\n",
        "    a) Starting with foundational content\n",
        "    Automatically includes Python basics for technical topics\n",
        "    Sorts basics by video number to maintain logical progression\n",
        "    b) Adding topic-specific content\n",
        "    Uses similarity scores to find most relevant videos\n",
        "    Checks and includes prerequisites before advanced content\n",
        "    Ensures proper skill progression\n",
        "\n",
        "    Outputs a list of vidoes\n",
        "\n",
        "    \"\"\"\n",
        "    # Get goal embedding\n",
        "    goal_embedding = transformer_model.encode([goal])\n",
        "    video_similarities = cosine_similarity(goal_embedding, embeddings)[0]\n",
        "\n",
        "    # Define topic prerequisites\n",
        "    topic_prerequisites = {\n",
        "        'image_processing': {'python_basics', 'data_structures'},\n",
        "        'machine_learning': {'python_basics', 'data_structures', 'data_analysis'},\n",
        "        'computer_vision': {'python_basics', 'image_processing'},\n",
        "        'bio_applications': {'python_basics', 'image_processing'}\n",
        "    }\n",
        "\n",
        "    # Initialize path\n",
        "    path = []\n",
        "    visited = set()\n",
        "\n",
        "    # Start with basics if needed\n",
        "    goal_lower = goal.lower()\n",
        "    if any(topic in goal_lower for topic in ['bioimage', 'image', 'machine learning', 'computer vision']):\n",
        "        # Add Python basics videos first\n",
        "        basics_videos = [n for n, d in G.nodes(data=True)\n",
        "                        if 'python_basics' in d['topics'] and\n",
        "                        d['difficulty'] == 'beginner']\n",
        "        basics_videos.sort(key=lambda x: G.nodes[x].get('video_number', float('inf'))) # Sort with a default value if video_number is None\n",
        "        path.extend(basics_videos)\n",
        "        visited.update(basics_videos)\n",
        "\n",
        "    # Get top relevant videos\n",
        "    relevant_indices = np.argsort(video_similarities)[::-1]\n",
        "\n",
        "    # Add videos to path based on relevance and prerequisites\n",
        "    for idx in relevant_indices:\n",
        "        if idx in visited:\n",
        "            continue\n",
        "\n",
        "        node_data = G.nodes[idx]\n",
        "\n",
        "        # Check if prerequisites are met\n",
        "        node_topics = node_data['topics']\n",
        "        prerequisites_needed = set()\n",
        "        for topic in node_topics:\n",
        "            if topic in topic_prerequisites:\n",
        "                prerequisites_needed.update(topic_prerequisites[topic])\n",
        "\n",
        "        # Add missing prerequisites first\n",
        "        if prerequisites_needed:\n",
        "            for prereq_topic in prerequisites_needed:\n",
        "                if not any(prereq_topic in G.nodes[n]['topics'] for n in path):\n",
        "                    prereq_videos = [n for n, d in G.nodes(data=True)\n",
        "                                   if prereq_topic in d['topics'] and\n",
        "                                   n not in visited]\n",
        "                    prereq_videos.sort(key=lambda x: G.nodes[x]['video_number'] or float('inf'))\n",
        "                    path.extend(prereq_videos)\n",
        "                    visited.update(prereq_videos)\n",
        "\n",
        "        # Add the video to path\n",
        "        path.append(idx)\n",
        "        visited.add(idx)\n",
        "\n",
        "    return path\n",
        "\n",
        "def format_learning_path(G: nx.DiGraph, path: List[int]) -> Dict:\n",
        "    \"\"\"\n",
        "    Format a learning path by summarizing total videos, duration,\n",
        "    difficulty breakdown, topics covered, and detailed video info.\n",
        "\n",
        "    Basically, organize and present information about a learning path\n",
        "    in a structured and detailed way.\n",
        "    This function takes a directed graph and creates a summary with:\n",
        "\n",
        "      - The total number of videos.\n",
        "      - The total duration in minutes.\n",
        "      - A breakdown of the difficulty levels.\n",
        "      - A list of unique topics covered.\n",
        "      - Detailed information for each video, like its title, difficulty, duration, associated topics, and key concepts.\n",
        "    \"\"\"\n",
        "    formatted_path = {\n",
        "        'total_videos': len(path),\n",
        "        'total_duration_minutes': sum(G.nodes[n]['duration'] / (1000 * 60) for n in path),\n",
        "        'difficulty_breakdown': {\n",
        "            'beginner': sum(1 for n in path if G.nodes[n]['difficulty'] == 'beginner'),\n",
        "            'intermediate': sum(1 for n in path if G.nodes[n]['difficulty'] == 'intermediate'),\n",
        "            'advanced': sum(1 for n in path if G.nodes[n]['difficulty'] == 'advanced')\n",
        "        },\n",
        "        'topics_covered': set(),\n",
        "        'videos': []\n",
        "    }\n",
        "\n",
        "    for i, node_id in enumerate(path):\n",
        "        node_data = G.nodes[node_id]\n",
        "        formatted_path['topics_covered'].update(node_data['topics'])\n",
        "\n",
        "        video_info = {\n",
        "            'id': node_id,  # Include the video ID\n",
        "            'step': i + 1,\n",
        "            'title': node_data['title'],\n",
        "            'difficulty': node_data['difficulty'],\n",
        "            'duration_minutes': node_data['duration'] / (1000 * 60),\n",
        "            'topics': list(node_data['topics']),\n",
        "            'key_concepts': node_data['concepts'][:5]\n",
        "        }\n",
        "        formatted_path['videos'].append(video_info)\n",
        "\n",
        "    formatted_path['topics_covered'] = list(formatted_path['topics_covered'])\n",
        "    return formatted_path\n",
        "\n",
        "def print_learning_path(formatted_path: Dict):\n",
        "    \"\"\"\n",
        "    Print formatted learning path in a readable way\n",
        "    \"\"\"\n",
        "    print(\"\\nLearning Path Summary\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total Videos: {formatted_path['total_videos']}\")\n",
        "    print(f\"Total Duration: {formatted_path['total_duration_minutes']:.1f} minutes\")\n",
        "    print(f\"                ({formatted_path['total_duration_minutes']/60:.1f} hours)\")\n",
        "\n",
        "    print(\"\\nDifficulty Breakdown:\")\n",
        "    for level, count in formatted_path['difficulty_breakdown'].items():\n",
        "        print(f\"  {level.title()}: {count} videos\")\n",
        "\n",
        "    print(\"\\nTopics Covered:\")\n",
        "    for topic in formatted_path['topics_covered']:\n",
        "        print(f\"  - {topic.replace('_', ' ').title()}\")\n",
        "\n",
        "    print(\"\\nDetailed Video Path:\")\n",
        "    print(\"=\" * 50)\n",
        "    for video in formatted_path['videos']:\n",
        "        print(f\"\\n{video['step']}. {video['title']}\")\n",
        "        print(f\"   Difficulty: {video['difficulty']}\")\n",
        "        print(f\"   Duration: {video['duration_minutes']:.1f} minutes\")\n",
        "        print(f\"   Topics: {', '.join(t.replace('_', ' ').title() for t in video['topics'])}\")\n",
        "        if video['key_concepts']:\n",
        "            print(f\"   Key Concepts: {', '.join(video['key_concepts'])}\")\n",
        "\n",
        "def analyze_path_coverage(G: nx.DiGraph, path: List[int]):\n",
        "    \"\"\"\n",
        "    Analyze topic coverage and prerequisites in the learning path\n",
        "\n",
        "      Tracks which topics are covered in the path\n",
        "      Ensures all necessary prerequisite topics are included\n",
        "      Prerequisite Validation\n",
        "\n",
        "      Checks if advanced topics (like machine learning) have necessary basics\n",
        "      Issues warnings if prerequisites are missing\n",
        "      Helps maintain logical learning progression\n",
        "      Path Statistics We collect detailed statistics about each path:\n",
        "\n",
        "      Total number of videos\n",
        "      Total duration\n",
        "      Difficulty breakdown\n",
        "      Topic coverage\n",
        "      Concept progression\n",
        "\n",
        "Example output\n",
        "{\n",
        "    'covered_topics': {'python_basics', 'data_structures', 'machine_learning'},\n",
        "    'prerequisite_issues': [\"Warning: Advanced topic found without Python basics coverage\"],\n",
        "    'topic_sequence': [['python_basics'], ['data_structures'], ['machine_learning']]\n",
        "}\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    covered_topics = set()\n",
        "    for node_id in path:\n",
        "        covered_topics.update(G.nodes[node_id]['topics'])\n",
        "\n",
        "    # Check if basic prerequisites are included before advanced topics\n",
        "    prerequisite_issues = []\n",
        "    for i, node_id in enumerate(path):\n",
        "        node_topics = G.nodes[node_id]['topics']\n",
        "        if ('machine_learning' in node_topics or 'computer_vision' in node_topics) and \\\n",
        "           'python_basics' not in covered_topics:\n",
        "            prerequisite_issues.append(f\"Warning: Advanced topic found without Python basics coverage\")\n",
        "\n",
        "    return {\n",
        "        'covered_topics': covered_topics,\n",
        "        'prerequisite_issues': prerequisite_issues,\n",
        "        'topic_sequence': [list(G.nodes[n]['topics']) for n in path]\n",
        "    }\n",
        "\n",
        "def create_recommendation_system(csv_path='combined_videos.csv'):\n",
        "    \"\"\"\n",
        "    Create and initialize the complete recommendation system (by running previously defined functions)\n",
        "    Basically return the knowledge graph, the transformer model and embeddings\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialize transformer model\n",
        "    transformer_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Load and process data\n",
        "    print(\"Loading video data...\")\n",
        "    video_data = load_video_data(csv_path)\n",
        "\n",
        "    print(\"Processing video content...\")\n",
        "    processed_data, embeddings, similarity_matrix, video_concepts = process_video_data(video_data)\n",
        "\n",
        "    print(\"Building knowledge graph...\")\n",
        "    G = build_knowledge_graph(processed_data, similarity_matrix, video_concepts)\n",
        "\n",
        "    print(\"System ready!\")\n",
        "    return G, transformer_model, embeddings"
      ],
      "metadata": {
        "id": "lX2TT6s4yHjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialize to a SQL Database\n",
        "\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/data/video_recommender/results'\n",
        "DB_PATH = os.path.join(OUTPUT_DIR, 'database', 'video_recommender.db')\n",
        "\n",
        "def create_output_dirs():\n",
        "    \"\"\"\n",
        "    Create output directories if they don't exist\n",
        "    \"\"\"\n",
        "    subdirs = ['visualizations', 'queries', 'database']\n",
        "    for subdir in subdirs:\n",
        "        path = os.path.join(OUTPUT_DIR, subdir)\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "    print(f\"Created output directories in {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "\n",
        "def create_database(db_name='video_recommender.db'):\n",
        "    \"\"\"\n",
        "    Create SQLite database with necessary tables\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_name, timeout=10)\n",
        "        conn.text_factory = str\n",
        "        c = conn.cursor()\n",
        "\n",
        "        # Drop existing tables to ensure clean slate\n",
        "        c.execute('DROP TABLE IF EXISTS edges')\n",
        "        c.execute('DROP TABLE IF EXISTS nodes')\n",
        "        c.execute('DROP TABLE IF EXISTS embeddings')\n",
        "\n",
        "        # Create tables with explicit types and add video_number\n",
        "        c.execute('''\n",
        "            CREATE TABLE nodes (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                title TEXT NOT NULL,\n",
        "                description TEXT,\n",
        "                difficulty TEXT NOT NULL,\n",
        "                duration INTEGER NOT NULL,\n",
        "                concepts TEXT NOT NULL,\n",
        "                topics TEXT NOT NULL,\n",
        "                video_number INTEGER\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        c.execute('''\n",
        "            CREATE TABLE edges (\n",
        "                source INTEGER NOT NULL,\n",
        "                target INTEGER NOT NULL,\n",
        "                weight REAL NOT NULL,\n",
        "                relationship_type TEXT NOT NULL,\n",
        "                PRIMARY KEY (source, target),\n",
        "                FOREIGN KEY (source) REFERENCES nodes(id),\n",
        "                FOREIGN KEY (target) REFERENCES nodes(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        c.execute('''\n",
        "            CREATE TABLE embeddings (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                embedding BLOB NOT NULL\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        conn.commit()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Database error: {e}\")\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "\n",
        "#Save the recommender system as serialized database\n",
        "def save_recommendation_system(G, transformer_model, embeddings):\n",
        "    \"\"\"\n",
        "    Save the recommendation system to the specified location\n",
        "\n",
        "    \"\"\"\n",
        "    #db_path = os.path.join(OUTPUT_DIR, 'database', 'video_recommender.db')\n",
        "    try:\n",
        "        create_database(DB_PATH)\n",
        "\n",
        "        conn = sqlite3.connect(DB_PATH, timeout=20)\n",
        "        c = conn.cursor()\n",
        "\n",
        "        # Save nodes with video_number\n",
        "        for node_id, data in G.nodes(data=True):\n",
        "            c.execute('''\n",
        "                INSERT OR REPLACE INTO nodes\n",
        "                (id, title, description, difficulty, duration, concepts, topics, video_number)\n",
        "                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            ''', (\n",
        "                node_id,\n",
        "                data['title'],\n",
        "                data.get('description', ''),\n",
        "                data['difficulty'],\n",
        "                data['duration'],\n",
        "                json.dumps(data['concepts']),\n",
        "                json.dumps(list(data['topics'])),\n",
        "                data.get('video_number', None)  # Handle cases where video_number doesn't exist\n",
        "            ))\n",
        "\n",
        "        # Save edges with explicit float conversion\n",
        "        c.execute('DELETE FROM edges')\n",
        "        for source, target, data in G.edges(data=True):\n",
        "            weight = float(data['weight'])\n",
        "            c.execute('''\n",
        "                INSERT INTO edges (source, target, weight, relationship_type)\n",
        "                VALUES (?, ?, ?, ?)\n",
        "            ''', (source, target, weight, data['type']))\n",
        "\n",
        "        # Save embeddings\n",
        "        c.execute('DELETE FROM embeddings')\n",
        "        for i, emb in enumerate(embeddings):\n",
        "            c.execute('INSERT INTO embeddings (id, embedding) VALUES (?, ?)',\n",
        "                     (i, emb.tobytes()))\n",
        "\n",
        "        conn.commit()\n",
        "        print(f\"System saved to {DB_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to database: {e}\")\n",
        "        try:\n",
        "            conn.rollback()\n",
        "        except:\n",
        "            pass\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        try:\n",
        "            conn.close()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "def save_query_results(results, filename):\n",
        "    \"\"\"\n",
        "    Save query results to a text file\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, 'queries', filename)\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(results)\n",
        "    print(f\"Query results saved to {output_path}\")\n",
        "\n",
        "\n",
        "def load_recommendation_system():\n",
        "    \"\"\"\n",
        "    Load the recommendation system from SQLite database, for future use without\n",
        "    building the knowledge graph again.\n",
        "    Note that we are loading the database but then reconstructing the graph\n",
        "    again using NetworkX.\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(DB_PATH):\n",
        "        raise FileNotFoundError(f\"Database not found at {DB_PATH}\")\n",
        "\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.text_factory = str\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Create new graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Load nodes with video_number\n",
        "    c.execute('SELECT * FROM nodes')\n",
        "    for row in c.fetchall():\n",
        "        node_id = row[0]\n",
        "        G.add_node(\n",
        "            node_id,\n",
        "            title=str(row[1]) if isinstance(row[1], bytes) else row[1],\n",
        "            description=str(row[2]) if isinstance(row[2], bytes) else row[2],\n",
        "            difficulty=str(row[3]) if isinstance(row[3], bytes) else row[3],\n",
        "            duration=row[4],\n",
        "            concepts=json.loads(row[5]) if isinstance(row[5], str) else json.loads(row[5].decode()),\n",
        "            topics=set(json.loads(row[6]) if isinstance(row[6], str) else json.loads(row[6].decode())),\n",
        "            video_number=row[7]  # Add video_number to node attributes\n",
        "        )\n",
        "\n",
        "    # Load edges\n",
        "    c.execute('SELECT source, target, CAST(weight AS REAL) as weight, relationship_type FROM edges')\n",
        "    for row in c.fetchall():\n",
        "        G.add_edge(\n",
        "            row[0], row[1],\n",
        "            weight=float(row[2]),\n",
        "            type=str(row[3]) if isinstance(row[3], bytes) else row[3]\n",
        "        )\n",
        "\n",
        "    # Load embeddings\n",
        "    c.execute('SELECT * FROM embeddings ORDER BY id')\n",
        "    embeddings = []\n",
        "    for row in c.fetchall():\n",
        "        embedding = np.frombuffer(row[1], dtype=np.float32)\n",
        "        embeddings.append(embedding)\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    # Recreate transformer model\n",
        "    transformer_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    return G, transformer_model, embeddings\n",
        "\n",
        "def query_videos_by_topic(topic=None, difficulty=None):\n",
        "    \"\"\"\n",
        "    Query videos based on topic and/or difficulty.\n",
        "    Directly querying the databse and not the graph.\n",
        "    Just to show as an example that you can do certain queries on the databse itself.\n",
        "\n",
        "    \"\"\"\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    c = conn.cursor()\n",
        "\n",
        "    query = 'SELECT id, title, difficulty, duration FROM nodes WHERE 1=1'\n",
        "    params = []\n",
        "\n",
        "    if topic:\n",
        "        query += ' AND topics LIKE ?'\n",
        "        params.append(f'%{topic}%')\n",
        "\n",
        "    if difficulty:\n",
        "        query += ' AND difficulty = ?'\n",
        "        params.append(difficulty)\n",
        "\n",
        "    c.execute(query, params)\n",
        "    results = c.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    return [{'id': r[0],\n",
        "             'title': r[1],\n",
        "             'difficulty': r[2],\n",
        "             'duration_minutes': r[3]/1000/60}\n",
        "            for r in results]\n",
        "\n",
        "def get_video_prerequisites(video_id):\n",
        "    \"\"\"\n",
        "    Get prerequisites for a specific video\n",
        "    Directly querying the databse and not the graph.\n",
        "    Just to show as an example that you can do certain queries on the databse itself.\n",
        "\n",
        "    Basically querying source nodes (n.id, n.title, and n.difficulty) that are linked to the\n",
        "    target video (e.target = video_id) through a relationship of type 'prerequisite'.\n",
        "    \"\"\"\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    conn.text_factory = str\n",
        "    c = conn.cursor()\n",
        "\n",
        "    c.execute('''\n",
        "        SELECT n.id, n.title, n.difficulty, CAST(e.weight AS REAL) as weight\n",
        "        FROM edges e\n",
        "        JOIN nodes n ON e.source = n.id\n",
        "        WHERE e.target = ? AND e.relationship_type = 'prerequisite'\n",
        "    ''', (video_id,))\n",
        "\n",
        "    results = c.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    return [{'id': r[0],\n",
        "             'title': str(r[1]) if isinstance(r[1], bytes) else r[1],\n",
        "             'difficulty': str(r[2]) if isinstance(r[2], bytes) else r[2],\n",
        "             'relevance': float(r[3])}\n",
        "            for r in results]\n",
        "\n",
        "def get_learning_path_from_db(goal, db_name='video_recommender.db'):\n",
        "    \"\"\"\n",
        "    Generate learning path for a specific goal from database\n",
        "    Again, we are loading the database but converting back to a Digraph\n",
        "    \"\"\"\n",
        "    # Load the system\n",
        "    G, transformer_model, embeddings = load_recommendation_system()\n",
        "\n",
        "    # Find the path\n",
        "    path = find_learning_path(G, goal, embeddings, transformer_model)\n",
        "\n",
        "    # Format and analyze the path\n",
        "    formatted_path = format_learning_path(G, path)\n",
        "\n",
        "    return formatted_path\n",
        "\n",
        "def print_learning_path_from_db(path_info):\n",
        "    \"\"\"\n",
        "    Print the learning path from database\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"\\nLearning Path Summary\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total Videos: {path_info['total_videos']}\")\n",
        "    print(f\"Total Duration: {path_info['total_duration_minutes']:.1f} minutes\")\n",
        "    print(f\"                ({path_info['total_duration_minutes']/60:.1f} hours)\")\n",
        "\n",
        "    print(\"\\nDifficulty Breakdown:\")\n",
        "    for level, count in path_info['difficulty_breakdown'].items():\n",
        "        print(f\"  {level.title()}: {count} videos\")\n",
        "\n",
        "    print(\"\\nTopics Covered:\")\n",
        "    for topic in path_info['topics_covered']:\n",
        "        print(f\"  - {topic.replace('_', ' ').title()}\")\n",
        "\n",
        "    print(\"\\nDetailed Learning Path:\")\n",
        "    print(\"=\" * 50)\n",
        "    for video in path_info['videos']:\n",
        "        print(f\"\\n{video['step']}. {video['title']}\")\n",
        "        print(f\"   Difficulty: {video['difficulty']}\")\n",
        "        print(f\"   Duration: {video['duration_minutes']:.1f} minutes\")\n",
        "        print(f\"   Topics: {', '.join(t.replace('_', ' ').title() for t in video['topics'])}\")\n",
        "        if video['key_concepts']:\n",
        "            print(f\"   Key Concepts: {', '.join(video['key_concepts'])}\")"
      ],
      "metadata": {
        "id": "l-z_i3CUyIRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def initialize_system(csv_path):\n",
        "    \"\"\"\n",
        "    Initialize and save the recommendation system\n",
        "    \"\"\"\n",
        "    print(\"Initializing recommendation system...\")\n",
        "    create_output_dirs()  # Using the original function instead of setup_environment\n",
        "\n",
        "    # Create and save the system\n",
        "    G, transformer_model, embeddings = create_recommendation_system(csv_path)\n",
        "    save_recommendation_system(G, transformer_model, embeddings)\n",
        "\n",
        "    # Generate and save initial visualizations\n",
        "    visualize_knowledge_graph(G)\n",
        "    visualize_simplified_graph(G, min_weight=0.7, filename='simplified_strong_connections.html')\n",
        "    visualize_simplified_graph(G, show_basics_only=True, filename='simplified_basics_only.html')\n",
        "\n",
        "    return G, transformer_model, embeddings\n",
        "\n",
        "# Getting Prerequisites for a Video\n",
        "def check_prerequisites(video_id):\n",
        "    \"\"\"\n",
        "    Check prerequisites for a specific video and save results\n",
        "    \"\"\"\n",
        "    prereqs = get_video_prerequisites(video_id)\n",
        "\n",
        "    # Prepare output string\n",
        "    output = []\n",
        "    output.append(f\"Prerequisites for Video {video_id}:\")\n",
        "    output.append(\"=\" * 50)\n",
        "\n",
        "    if prereqs:\n",
        "        for prereq in prereqs:\n",
        "            output.append(f\"Title: {prereq['title']}\")\n",
        "            output.append(f\"Difficulty: {prereq['difficulty']}\")\n",
        "            output.append(f\"Relevance Score: {prereq['relevance']:.2f}\")\n",
        "            output.append(\"-\" * 30)\n",
        "    else:\n",
        "        output.append(f\"No prerequisites found for Video {video_id}\")\n",
        "\n",
        "    # Save to file\n",
        "    filename = os.path.join(OUTPUT_DIR, 'queries', f'prerequisites_video_{video_id}.txt')\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(output))\n",
        "\n",
        "    # Also print to console\n",
        "    print('\\n'.join(output))\n",
        "\n",
        "# Getting Learning Path for a Goal\n",
        "def get_path_for_goal(goal):\n",
        "    \"\"\"\n",
        "    Get and display learning path for a specific goal\n",
        "    \"\"\"\n",
        "    print(f\"\\nGenerating learning path for: {goal}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    path_info = get_learning_path_from_db(goal)\n",
        "\n",
        "    # Create output string (all the following code just to dump output to a file)\n",
        "    output = []\n",
        "    output.append(f\"Learning Path for: {goal}\")\n",
        "    output.append(\"=\" * 80)\n",
        "    output.append(f\"\\nTotal Videos: {path_info['total_videos']}\")\n",
        "    output.append(f\"Total Duration: {path_info['total_duration_minutes']:.1f} minutes\")\n",
        "    output.append(f\"                ({path_info['total_duration_minutes']/60:.1f} hours)\")\n",
        "\n",
        "    output.append(\"\\nDifficulty Breakdown:\")\n",
        "    for level, count in path_info['difficulty_breakdown'].items():\n",
        "        output.append(f\"  {level.title()}: {count} videos\")\n",
        "\n",
        "    output.append(\"\\nTopics Covered:\")\n",
        "    for topic in path_info['topics_covered']:\n",
        "        output.append(f\"  - {topic.replace('_', ' ').title()}\")\n",
        "\n",
        "    output.append(\"\\nDetailed Video Path:\")\n",
        "    output.append(\"=\" * 50)\n",
        "    for video in path_info['videos']:\n",
        "        output.append(f\"\\n{video['step']}. {video['title']}\")\n",
        "        output.append(f\"   Difficulty: {video['difficulty']}\")\n",
        "        output.append(f\"   Duration: {video['duration_minutes']:.1f} minutes\")\n",
        "        output.append(f\"   Topics: {', '.join(t.replace('_', ' ').title() for t in video['topics'])}\")\n",
        "        if video['key_concepts']:\n",
        "            output.append(f\"   Key Concepts: {', '.join(video['key_concepts'])}\")\n",
        "\n",
        "    # Save to file\n",
        "    filename = os.path.join(OUTPUT_DIR, 'queries', f'learning_path_{goal.replace(\" \", \"_\")}.txt')\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(output))\n",
        "\n",
        "    # Also print to console\n",
        "    print('\\n'.join(output))\n",
        "\n",
        "    return path_info\n",
        "\n",
        "# Querying Videos by Topic and Difficulty\n",
        "def explore_topics(topic=None, difficulty=None):\n",
        "    \"\"\"\n",
        "    Explore videos by topic and/or difficulty\n",
        "    \"\"\"\n",
        "    videos = query_videos_by_topic(topic=topic, difficulty=difficulty)\n",
        "\n",
        "    # Prepare output string\n",
        "    output = []\n",
        "    output.append(f\"\\nFound {len(videos)} videos\")\n",
        "    if topic:\n",
        "        output.append(f\" for topic '{topic}'\")\n",
        "    if difficulty:\n",
        "        output.append(f\" with {difficulty} difficulty\")\n",
        "    output.append(\":\")\n",
        "    output.append(\"=\" * 50)\n",
        "\n",
        "    for video in videos:\n",
        "        output.append(f\"\\nID: {video['id']}\")\n",
        "        output.append(f\"Title: {video['title']}\")\n",
        "        output.append(f\"Difficulty: {video['difficulty']}\")\n",
        "        output.append(f\"Duration: {video['duration_minutes']:.1f} minutes\")\n",
        "\n",
        "    # Save to file\n",
        "    filename_parts = []\n",
        "    if topic:\n",
        "        filename_parts.append(topic)\n",
        "    if difficulty:\n",
        "        filename_parts.append(difficulty)\n",
        "    filename = os.path.join(OUTPUT_DIR, 'queries', f'topic_query_{\"_\".join(filename_parts)}.txt')\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(output))\n",
        "\n",
        "    # Also print to console\n",
        "    print('\\n'.join(output))\n",
        "\n",
        "\n",
        "def run_example_queries(G, transformer_model, embeddings):\n",
        "    \"\"\"\n",
        "    Run and save example queries\n",
        "    \"\"\"\n",
        "    create_output_dirs()  # Ensure directories exist\n",
        "\n",
        "    # Example goals\n",
        "    example_goals = [\n",
        "        \"Mastering python for Bioimage analysis\",\n",
        "        \"Learning machine learning for microscopy\",\n",
        "       # \"Understanding computer vision and image processing\",\n",
        "       #\"Python programming basics for scientists\"\n",
        "    ]\n",
        "\n",
        "    # Generate learning paths\n",
        "    for goal in example_goals:\n",
        "        print(f\"\\nProcessing goal: {goal}\")\n",
        "        try:\n",
        "            path_info = get_path_for_goal(goal)\n",
        "            if path_info and 'videos' in path_info:\n",
        "                filename = f'learning_path_{goal.replace(\" \", \"_\")}.html'\n",
        "                path = [video['id'] for video in path_info['videos']]\n",
        "                if path:\n",
        "                    visualize_learning_path(G, path, filename)\n",
        "                else:\n",
        "                    print(f\"Warning: No valid path found for goal: {goal}\")\n",
        "            else:\n",
        "                print(f\"Warning: Invalid path_info structure for goal: {goal}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing goal '{goal}': {str(e)}\")\n",
        "\n",
        "    # Explore topics\n",
        "    for topic in ['python_basics', 'machine_learning', 'image_processing']:\n",
        "        for difficulty in ['beginner', 'intermediate', 'advanced']:\n",
        "            try:\n",
        "                explore_topics(topic=topic, difficulty=difficulty)\n",
        "            except Exception as e:\n",
        "                print(f\"Error exploring topic '{topic}' with difficulty '{difficulty}': {str(e)}\")\n",
        "\n",
        "    # Generate topic visualizations\n",
        "    for topic in ['python_basics', 'image_processing', 'bio_applications']:\n",
        "        try:\n",
        "            visualize_topic_subgraph(G, topic)\n",
        "        except Exception as e:\n",
        "            print(f\"Error visualizing topic '{topic}': {str(e)}\")\n",
        "\n",
        "    # Check prerequisites for some example videos\n",
        "    for video_id in [42, 43, 44]:\n",
        "        try:\n",
        "            check_prerequisites(video_id)\n",
        "        except Exception as e:\n",
        "            print(f\"Error checking prerequisites for video {video_id}: {str(e)}\")"
      ],
      "metadata": {
        "id": "UG3i-vtCygzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    csv_path = '/content/drive/MyDrive/data/video_recommender/combined_videos.csv'\n",
        "\n",
        "    # Create directories first\n",
        "    create_output_dirs()\n",
        "\n",
        "    # Initialize or load system\n",
        "    try:\n",
        "        print(\"Checking if system needs initialization...\")\n",
        "        G, transformer_model, embeddings = load_recommendation_system()\n",
        "        print(\"System loaded from database successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"System not found or error loading: {str(e)}\")\n",
        "        print(\"Initializing new system...\")\n",
        "        try:\n",
        "            G, transformer_model, embeddings = initialize_system(csv_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing system: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # Run queries\n",
        "    try:\n",
        "        run_example_queries(G, transformer_model, embeddings)\n",
        "        print(\"\\nAll operations completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during query execution: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "TCKlZnzfyaey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}