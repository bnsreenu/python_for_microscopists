{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP86jGvodGUSWSsoTWRAkDp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnsreenu/python_for_microscopists/blob/master/367_Correlation_Analysis_Part_3_Adv_Correlation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/4iQwHOSwlQo"
      ],
      "metadata": {
        "id": "itT_fpkExKjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Analysis: Understanding Relationships in Data - Part 2\n",
        "**Part 9 of the Statistical Analysis in Python Tutorial Series**\n",
        "\n",
        "## Overview\n",
        "\n",
        "Correlation analysis is one of the most fundamental and widely-used statistical techniques for exploring relationships between variables. This comprehensive tutorial takes you from basic concepts to advanced applications using real-world penguin morphology data.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Dataset: Palmer Penguins\n",
        "\n",
        "We'll use the famous Palmer Penguins dataset, which provides:\n",
        "- **344 penguins** across 3 species (Adelie, Chinstrap, Gentoo)\n",
        "- **Morphological measurements**: bill length/depth, flipper length, body mass\n",
        "- **Real biological relationships** to explore and interpret\n",
        "- **Natural confounding factors** (species effects) to understand\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Tutorial Structure\n",
        "\n",
        "**Part 1: Foundation**\n",
        "- Data exploration and quality assessment\n",
        "- Visual demonstration of correlation concepts\n",
        "- Basic Pearson correlation calculations\n",
        "- Systematic testing of correlation assumptions\n",
        "- Diagnostic plotting and interpretation\n",
        "- Spearman correlation as robust alternative\n",
        "\n",
        "**Part 2: Partial Correlations**  \n",
        "- Partial correlations controlling for confounding\n",
        "- Correlation matrices and multiple relationships\n",
        "- Visualizing correlations\n",
        "\n",
        "**Part 3: Advanced Correlation Analysis**\n",
        "- Confidence Intervals\n",
        "- Statistical Significance\n",
        "\n",
        "**Part 4: Correlation vs. causation**\n",
        "- Correlation vs. causation analysis\n",
        "- Spurious correlation examples\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gE2KpP7GijjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.correlation_tools import corr_nearest, corr_clipped\n",
        "import warnings\n",
        "from itertools import combinations\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from sklearn.utils import resample\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style consistent with previous tutorials\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12"
      ],
      "metadata": {
        "id": "XkIeGgF_A3Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load palmer penguin dataset from online source\n",
        "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
        "\n",
        "penguins = pd.read_csv(url)\n",
        "\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\nDataset Information:\")\n",
        "print(\"=\" * 25)\n",
        "print(f\"Shape: {penguins.shape}\")\n",
        "print(f\"Columns: {list(penguins.columns)}\")"
      ],
      "metadata": {
        "id": "qdl8DK-kA9TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "missing_counts = penguins.isnull().sum()\n",
        "for col, count in missing_counts.items():\n",
        "    print(f\"  {col}: {count}\")\n",
        "\n",
        "# Remove rows with missing values for correlation analysis\n",
        "penguins_clean = penguins.dropna()\n",
        "print(f\"\\nAfter removing missing values: {penguins_clean.shape[0]} complete observations\")"
      ],
      "metadata": {
        "id": "Z9v2sgHYBKNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define continuous variables for correlation analysis\n",
        "continuous_vars = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
        "categorical_vars = ['species', 'island', 'sex']\n"
      ],
      "metadata": {
        "id": "HruziqIJBRF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. Introduction to Correlation Analysis\n",
        "\n",
        "## What is Correlation?\n",
        "\n",
        "Correlation measures the strength and direction of a linear relationship between two continuous variables.\n",
        "It helps us understand how variables move together:\n",
        "\n",
        "- **Positive correlation**: As one variable increases, the other tends to increase\n",
        "- **Negative correlation**: As one variable increases, the other tends to decrease\n",
        "- **Zero correlation**: No linear relationship between variables\n",
        "\n",
        "## Key Properties of Correlation:\n",
        "\n",
        "1. **Range**: Correlation coefficients typically range from -1 to +1\n",
        "2. **Magnitude**: |r| indicates strength of relationship\n",
        "3. **Sign**: + or - indicates direction of relationship\n",
        "4. **No causation**: Correlation does not imply causation\n",
        "\n",
        "## Interpretation Guidelines:\n",
        "- |r| ≥ 0.7: Strong relationship\n",
        "- 0.3 ≤ |r| < 0.7: Moderate relationship\n",
        "- |r| < 0.3: Weak relationship\n",
        "\n",
        "Let's start by exploring our penguin data to understand the relationships between physical measurements.\n"
      ],
      "metadata": {
        "id": "TTkFJznKBhQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis for Correlation Studies\n",
        "\n",
        "## Why EDA Before Correlation Analysis?\n",
        "\n",
        "Understanding data characteristics determines which correlation methods to use and how to interpret results.\n",
        "\n",
        "---\n",
        "\n",
        "## What This Analysis Checks\n",
        "\n",
        "### 1. **Normality Assessment**\n",
        "- **Normal data**: Use Pearson correlation\n",
        "- **Non-normal data**: Use Spearman correlation\n",
        "- **Tools**: Shapiro-Wilk test, Q-Q plots, histograms\n",
        "\n",
        "### 2. **Outlier Detection**\n",
        "- **Impact**: Outliers can distort correlation coefficients\n",
        "- **Methods**: Box plots, IQR method\n",
        "- **Decision**: Remove, transform, or use robust methods\n",
        "\n",
        "### 3. **Species Differences**\n",
        "- **Large differences**: Species likely confounds correlations\n",
        "- **Small differences**: Correlations more likely genuine\n",
        "- **Implication**: Determines need for partial correlation analysis\n",
        "\n",
        "### 4. **Data Quality**\n",
        "- **Coefficient of variation**: Measurement reliability\n",
        "- **Distribution patterns**: Data transformation needs\n",
        "\n",
        "---\n",
        "\n",
        "## Key Decisions This EDA Will Guide\n",
        "\n",
        "**Correlation Method**:\n",
        "- Normal data → Pearson correlation\n",
        "- Non-normal data → Spearman correlation\n",
        "\n",
        "**Analysis Approach**:\n",
        "- Large species effects → Use partial correlations\n",
        "- Few outliers → Standard analysis\n",
        "- Many outliers → Robust methods\n",
        "\n",
        "This foundation ensures our correlation analysis uses appropriate methods and produces reliable results."
      ],
      "metadata": {
        "id": "A-WY5YZ7Ctc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_correlation_data(data, continuous_vars):\n",
        "    \"\"\"\n",
        "    Exploration of the dataset focusing on relationships between continuous variables.\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"EXPLORATORY DATA ANALYSIS FOR CORRELATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Basic descriptive statistics\n",
        "    print(\"\\nDescriptive Statistics for Continuous Variables:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    desc_stats = data[continuous_vars].describe().round(2)\n",
        "    print(desc_stats)\n",
        "\n",
        "    # Calculate coefficient of variation for each variable\n",
        "    # Remember that CV helps to understand the extent of dispersion or variability around the mean of a dataset.\n",
        "    print(f\"\\nCoefficient of Variation (CV = std/mean):\")\n",
        "    print(\"-\" * 40)\n",
        "    for var in continuous_vars:\n",
        "        mean_val = data[var].mean()\n",
        "        std_val = data[var].std()\n",
        "        cv = (std_val / mean_val) * 100\n",
        "        print(f\"{var:<20}: {cv:>6.1f}%\")\n",
        "\n",
        "    # Check distributions for normality (important for Pearson correlation)\n",
        "    print(f\"\\nNormality Assessment (Shapiro-Wilk Test):\")\n",
        "    print(\"-\" * 45)\n",
        "    normality_results = {}\n",
        "\n",
        "    for var in continuous_vars:\n",
        "        # Use sample if data is too large for Shapiro-Wilk\n",
        "        sample_data = data[var].dropna()\n",
        "        if len(sample_data) > 5000:\n",
        "            sample_data = sample_data.sample(5000, random_state=42)\n",
        "\n",
        "        stat, p_value = stats.shapiro(sample_data)\n",
        "        normality_results[var] = {'statistic': stat, 'p_value': p_value}\n",
        "\n",
        "        interpretation = \"Normal\" if p_value > 0.05 else \"Non-normal\"\n",
        "        print(f\"{var:<20}: W = {stat:.4f}, p = {p_value:.4f} ({interpretation})\")\n",
        "\n",
        "    # Create different visualizations\n",
        "    n_vars = len(continuous_vars)\n",
        "    fig, axes = plt.subplots(3, n_vars, figsize=(4*n_vars, 12))\n",
        "    fig.suptitle('Exploratory Data Analysis: Continuous Variables', fontsize=16)\n",
        "\n",
        "    for i, var in enumerate(continuous_vars):\n",
        "        var_data = data[var].dropna()\n",
        "\n",
        "        # Row 1: Histograms with normal overlay\n",
        "        axes[0, i].hist(var_data, bins=30, density=True, alpha=0.7,\n",
        "                       color='skyblue', edgecolor='black')\n",
        "\n",
        "        # Overlay normal distribution\n",
        "        mu, sigma = stats.norm.fit(var_data)\n",
        "        x = np.linspace(var_data.min(), var_data.max(), 100)\n",
        "        axes[0, i].plot(x, stats.norm.pdf(x, mu, sigma), 'r-',\n",
        "                       linewidth=2, label='Normal fit')\n",
        "\n",
        "        axes[0, i].set_title(f'Distribution: {var}')\n",
        "        axes[0, i].set_xlabel(var)\n",
        "        axes[0, i].set_ylabel('Density')\n",
        "        axes[0, i].legend()\n",
        "        axes[0, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Row 2: Q-Q plots for normality\n",
        "        stats.probplot(var_data, dist=\"norm\", plot=axes[1, i])\n",
        "        axes[1, i].set_title(f'Q-Q Plot: {var}')\n",
        "        axes[1, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Row 3: Box plots\n",
        "        axes[2, i].boxplot(var_data, patch_artist=True,\n",
        "                          boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
        "        axes[2, i].set_title(f'Box Plot: {var}')\n",
        "        axes[2, i].set_ylabel(var)\n",
        "        axes[2, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Add outlier information\n",
        "        Q1 = var_data.quantile(0.25)\n",
        "        Q3 = var_data.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outliers = var_data[(var_data < lower_bound) | (var_data > upper_bound)]\n",
        "\n",
        "        axes[2, i].text(0.5, 0.95, f'Outliers: {len(outliers)}',\n",
        "                       transform=axes[2, i].transAxes, ha='center', va='top',\n",
        "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Identify potential outliers\n",
        "    print(f\"\\nOutlier Analysis:\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    outlier_summary = {}\n",
        "    for var in continuous_vars:\n",
        "        var_data = data[var].dropna()\n",
        "        Q1 = var_data.quantile(0.25)\n",
        "        Q3 = var_data.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        outliers = var_data[(var_data < lower_bound) | (var_data > upper_bound)]\n",
        "        outlier_percentage = (len(outliers) / len(var_data)) * 100\n",
        "\n",
        "        outlier_summary[var] = {\n",
        "            'count': len(outliers),\n",
        "            'percentage': outlier_percentage,\n",
        "            'lower_bound': lower_bound,\n",
        "            'upper_bound': upper_bound\n",
        "        }\n",
        "\n",
        "        print(f\"{var:<20}: {len(outliers):>3} outliers ({outlier_percentage:>5.1f}%)\")\n",
        "\n",
        "    # Species-specific analysis\n",
        "    if 'species' in data.columns:\n",
        "        print(f\"\\nSpecies-specific Means:\")\n",
        "        print(\"-\" * 25)\n",
        "        species_means = data.groupby('species')[continuous_vars].mean().round(2)\n",
        "        print(species_means)\n",
        "\n",
        "        # Visualize species differences\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Variable Distributions by Species', fontsize=16)\n",
        "\n",
        "        for i, var in enumerate(continuous_vars):\n",
        "            row = i // 2\n",
        "            col = i % 2\n",
        "\n",
        "            sns.boxplot(data=data, x='species', y=var, ax=axes[row, col])\n",
        "            axes[row, col].set_title(f'{var} by Species')\n",
        "            axes[row, col].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return {\n",
        "        'descriptive_stats': desc_stats,\n",
        "        'normality_results': normality_results,\n",
        "        'outlier_summary': outlier_summary,\n",
        "        'species_means': species_means if 'species' in data.columns else None\n",
        "    }\n",
        "\n",
        "# Step 1: Explore the data\n",
        "exploration_results = explore_correlation_data(penguins_clean, continuous_vars)"
      ],
      "metadata": {
        "id": "YnCAZTgDDonZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Concepts: Visual Demonstration\n",
        "This section builds intuition about correlation by showing what different correlation strengths actually look like in scatter plots. We'll generate simulated data with known correlations (ranging from r = -0.9 to r = +0.9) so you can see how the strength and direction of relationships appear visually. This visual foundation will help you better interpret the correlations we find in the real penguin data that follows.\n",
        "\n",
        "**The correlated data is generated by:**\n",
        "\n",
        "Constructing a covariance matrix where the off-diagonal value is the target correlation r\n",
        "\n",
        "Sampling from a multivariate normal distribution using that covariance structure. The multivariate normal distribution inherently supports specifying correlation (or covariance) between variables.\n",
        "\n",
        "\n",
        "\n",
        "This approach guarantees that the resulting synthetic data pairs (x, y) have the desired correlation structure."
      ],
      "metadata": {
        "id": "KC7CPn7sDnHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_correlation_concepts():\n",
        "    \"\"\"\n",
        "    Demonstrate correlation concepts with simulated examples to build intuition.\n",
        "    We will then look at our penguin dataset\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"CORRELATION CONCEPTS: VISUAL DEMONSTRATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Generate example data with different correlation strengths\n",
        "    np.random.seed(42)\n",
        "    n = 100\n",
        "    x = np.random.normal(0, 1, n)\n",
        "\n",
        "    # Create different correlation scenarios\n",
        "    correlations = [0.9, 0.5, 0.0, -0.5, -0.9]\n",
        "    correlation_data = {}\n",
        "\n",
        "    for r in correlations:\n",
        "        # Generate correlated data using Cholesky decomposition\n",
        "        cov_matrix = np.array([[1, r], [r, 1]])\n",
        "        data_pair = np.random.multivariate_normal([0, 0], cov_matrix, n)\n",
        "        correlation_data[r] = {'x': data_pair[:, 0], 'y': data_pair[:, 1]}\n",
        "\n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(1, len(correlations), figsize=(20, 4))\n",
        "    fig.suptitle('Correlation Strength Examples', fontsize=16)\n",
        "\n",
        "    for i, r in enumerate(correlations):\n",
        "        x_data = correlation_data[r]['x']\n",
        "        y_data = correlation_data[r]['y']\n",
        "\n",
        "        # Calculate actual correlation\n",
        "        actual_r, _ = pearsonr(x_data, y_data)\n",
        "\n",
        "        # Create scatter plot\n",
        "        axes[i].scatter(x_data, y_data, alpha=0.6, s=30)\n",
        "\n",
        "        # Add regression line\n",
        "        slope, intercept, _, _, _ = stats.linregress(x_data, y_data)\n",
        "        line_x = np.linspace(x_data.min(), x_data.max(), 100)\n",
        "        line_y = slope * line_x + intercept\n",
        "        axes[i].plot(line_x, line_y, 'r-', linewidth=2)\n",
        "\n",
        "        # Formatting\n",
        "        axes[i].set_title(f'Target r = {r}\\nActual r = {actual_r:.3f}')\n",
        "        axes[i].set_xlabel('Variable X')\n",
        "        if i == 0:\n",
        "            axes[i].set_ylabel('Variable Y')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "        axes[i].set_xlim(-3, 3)\n",
        "        axes[i].set_ylim(-3, 3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Interpretation guide\n",
        "    print(f\"\\nCorrelation Interpretation Guide:\")\n",
        "    print(\"-\" * 35)\n",
        "    interpretation_guide = {\n",
        "        \"Perfect positive (r = +1.0)\": \"Y increases exactly as X increases\",\n",
        "        \"Strong positive (r = +0.7 to +0.9)\": \"Y tends to increase as X increases\",\n",
        "        \"Moderate positive (r = +0.3 to +0.7)\": \"Y somewhat increases as X increases\",\n",
        "        \"Weak (r = -0.3 to +0.3)\": \"Little to no linear relationship\",\n",
        "        \"Moderate negative (r = -0.7 to -0.3)\": \"Y somewhat decreases as X increases\",\n",
        "        \"Strong negative (r = -0.9 to -0.7)\": \"Y tends to decrease as X increases\",\n",
        "        \"Perfect negative (r = -1.0)\": \"Y decreases exactly as X increases\"\n",
        "    }\n",
        "\n",
        "    for strength, description in interpretation_guide.items():\n",
        "        print(f\"{strength:<30}: {description}\")\n",
        "\n",
        "\n",
        "  # Step 2: Demonstrate correlation concepts\n",
        "demonstrate_correlation_concepts()"
      ],
      "metadata": {
        "id": "8LLZ86boD08l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Pearson Correlations Analysis\n",
        "\n",
        "## What We're Calculating\n",
        "\n",
        "This section computes **Pearson correlations between all pairs** of continuous variables in our penguin dataset and provides a comprehensive interpretation of the relationships.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Outputs\n",
        "\n",
        "### 1. **Correlation Matrix**\n",
        "- Shows all pairwise correlations in a structured table\n",
        "- Values range from -1 to +1\n",
        "- Diagonal is always 1.0 (variables correlate perfectly with themselves)\n",
        "\n",
        "### 2. **Statistical Significance Testing**\n",
        "- **p-values**: Test whether correlations are significantly different from zero\n",
        "- **Sample sizes**: Number of complete observations for each pair\n",
        "- **Significance threshold**: p < 0.05 indicates non-random relationship\n",
        "\n",
        "### 3. **Strength Classification**\n",
        "- **Strong**: |r| ≥ 0.7 (variables move together closely)\n",
        "- **Moderate**: 0.3 ≤ |r| < 0.7 (noticeable relationship)\n",
        "- **Weak**: |r| < 0.3 (little linear relationship)\n",
        "\n",
        "### 4. **Visual Heatmap**\n",
        "- Color-coded correlation matrix\n",
        "- **Red**: Negative correlations\n",
        "- **Blue**: Positive correlations  \n",
        "- **Intensity**: Stronger correlations = darker colors\n",
        "\n",
        "---\n",
        "\n",
        "## What to Look For\n",
        "\n",
        "**Strong Correlations**: Variables that might be measuring similar biological traits or have functional relationships\n",
        "\n",
        "**Unexpected Patterns**: Correlations that don't make biological sense (potential confounding)\n",
        "\n",
        "**Significance vs. Strength**: Large correlations that aren't significant (small sample) or small correlations that are significant (large sample)\n",
        "\n",
        "This foundation analysis will guide our deeper investigation into species effects and causal relationships."
      ],
      "metadata": {
        "id": "oqI9F78YEC3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional explanation, if you are still confused...\n",
        "\n",
        "**What the correlation is doing:**\n",
        "\n",
        "- Takes ALL penguins (regardless of species - Adelie, Chinstrap, Gentoo)\n",
        "- For each individual penguin, looks at both bill_length and bill_depth (or any two variables)\n",
        "- Asks the question: \"Across all these penguins, when bill_length is higher, does bill_depth tend to be higher too?\"\n",
        "- Quantifies this relationship using Pearson's correlation formula\n",
        "- The correlation pools all species together, so it's asking: \"In general, across all penguin types, do longer bills tend to be deeper?\"\n",
        "\n",
        "But this can sometimes be misleading because:\n",
        "\n",
        "Different species might have systematically different bill shapes. The correlation might be driven by between-species differences rather than within-species relationships\n",
        "\n",
        "That's why we will later look at Partial correlations (controlling for species)"
      ],
      "metadata": {
        "id": "wdVXd38ZCosr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_basic_correlations(data, continuous_vars):\n",
        "    \"\"\"\n",
        "    Calculate and interpret basic Pearson correlations between continuous variables.\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"BASIC PEARSON CORRELATIONS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = data[continuous_vars].corr()\n",
        "\n",
        "    print(\"Correlation Matrix:\")\n",
        "    print(\"-\" * 20)\n",
        "    print(corr_matrix.round(3))\n",
        "\n",
        "    # Extract unique pairs and their correlations\n",
        "    print(f\"\\nPairwise Correlations:\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    correlation_results = []\n",
        "\n",
        "    for i in range(len(continuous_vars)):\n",
        "        for j in range(i+1, len(continuous_vars)):\n",
        "            var1 = continuous_vars[i]\n",
        "            var2 = continuous_vars[j]\n",
        "\n",
        "            # FIXED: Calculate correlation with proper alignment\n",
        "            # Get valid pairs first to ensure alignment\n",
        "            valid_pairs = data[[var1, var2]].dropna()\n",
        "\n",
        "            # Calculate correlation and significance test on aligned data\n",
        "            r, p_value = pearsonr(valid_pairs[var1], valid_pairs[var2])\n",
        "\n",
        "            # Calculate sample size\n",
        "            n = len(valid_pairs)\n",
        "\n",
        "            # Interpret strength\n",
        "            if abs(r) >= 0.7:\n",
        "                strength = \"Strong\"\n",
        "            elif abs(r) >= 0.3:\n",
        "                strength = \"Moderate\"\n",
        "            else:\n",
        "                strength = \"Weak\"\n",
        "\n",
        "            # Interpret direction\n",
        "            direction = \"Positive\" if r > 0 else \"Negative\"\n",
        "\n",
        "            # Significance\n",
        "            significance = \"Significant\" if p_value < 0.05 else \"Not significant\"\n",
        "\n",
        "            correlation_results.append({\n",
        "                'var1': var1,\n",
        "                'var2': var2,\n",
        "                'correlation': r,\n",
        "                'p_value': p_value,\n",
        "                'n': n,\n",
        "                'strength': strength,\n",
        "                'direction': direction,\n",
        "                'significance': significance\n",
        "            })\n",
        "\n",
        "            print(f\"{var1} ↔ {var2}:\")\n",
        "            print(f\"  r = {r:>7.3f}, p = {p_value:>7.4f}, n = {n:>3}\")\n",
        "            print(f\"  {strength} {direction.lower()} correlation ({significance.lower()})\")\n",
        "            print()\n",
        "\n",
        "    # Create correlation heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Create mask for upper triangle to show only lower triangle\n",
        "    #mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(corr_matrix,\n",
        "                #mask=mask,\n",
        "                annot=True,\n",
        "                cmap='RdBu_r',\n",
        "                center=0,\n",
        "                square=True,\n",
        "                fmt='.3f',\n",
        "                cbar_kws={'label': 'Pearson Correlation Coefficient'})\n",
        "\n",
        "    plt.title('Correlation Matrix Heatmap', fontsize=14, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"Correlation Summary:\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    correlations_flat = [abs(result['correlation']) for result in correlation_results]\n",
        "    print(f\"Number of correlations: {len(correlations_flat)}\")\n",
        "    print(f\"Mean |correlation|: {np.mean(correlations_flat):.3f}\")\n",
        "    print(f\"Max |correlation|: {np.max(correlations_flat):.3f}\")\n",
        "    print(f\"Min |correlation|: {np.min(correlations_flat):.3f}\")\n",
        "\n",
        "    # Count by strength\n",
        "    strength_counts = {}\n",
        "    for result in correlation_results:\n",
        "        strength = result['strength']\n",
        "        strength_counts[strength] = strength_counts.get(strength, 0) + 1\n",
        "\n",
        "    print(f\"\\nCorrelations by strength:\")\n",
        "    for strength, count in strength_counts.items():\n",
        "        percentage = (count / len(correlation_results)) * 100\n",
        "        print(f\"  {strength}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Significant correlations\n",
        "    significant_corrs = [r for r in correlation_results if r['significance'] == 'Significant']\n",
        "    print(f\"\\nSignificant correlations: {len(significant_corrs)} out of {len(correlation_results)}\")\n",
        "\n",
        "    return {\n",
        "        'correlation_matrix': corr_matrix,\n",
        "        'pairwise_results': correlation_results,\n",
        "        'summary_stats': {\n",
        "            'mean_abs_correlation': np.mean(correlations_flat),\n",
        "            'max_abs_correlation': np.max(correlations_flat),\n",
        "            'min_abs_correlation': np.min(correlations_flat),\n",
        "            'strength_distribution': strength_counts,\n",
        "            'significant_count': len(significant_corrs)\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "# Step 3: Calculate basic correlations\n",
        "correlation_results = calculate_basic_correlations(penguins_clean, continuous_vars)"
      ],
      "metadata": {
        "id": "-YTwDtSTX5lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assumption Checking for Correlation Analysis\n",
        "\n",
        "## Why Check Assumptions?\n",
        "\n",
        "**Pearson correlation makes specific assumptions about the data.** When these assumptions are violated, the correlation coefficient can be misleading or unreliable. This section systematically tests each assumption and provides guidance on when to use alternative methods.\n",
        "\n",
        "---\n",
        "\n",
        "## Pearson Correlation Assumptions\n",
        "\n",
        "### 1. **Linear Relationship**\n",
        "- Variables should be linearly related (straight-line pattern)\n",
        "- **Test**: Compare linear vs. quadratic model fit (R² values)\n",
        "- **Violation**: Curved relationships reduce Pearson correlation strength\n",
        "\n",
        "### 2. **Normal Distribution**\n",
        "- Both variables should be approximately normally distributed\n",
        "- **Test**: Shapiro-Wilk test, Q-Q plots\n",
        "- **Violation**: Skewed data can distort correlation estimates\n",
        "\n",
        "### 3. **Homoscedasticity**\n",
        "- Constant variance across the range (equal spread). Basically, whether the spread of the data points around the regression line is consistent\n",
        "- **Test**: Residuals vs. fitted values plot, Breusch-Pagan test\n",
        "- **Violation**: Unequal variance can affect reliability\n",
        "\n",
        "### 4. **No Extreme Outliers**\n",
        "- Outliers can severely distort correlation coefficients\n",
        "- **Tests**: Z-scores (>3), Mahalanobis distance for bivariate outliers\n",
        "- **Violation**: Single outliers can create or hide relationships\n",
        "\n",
        "### 5. **Independence**\n",
        "- Observations should be independent\n",
        "- **Assessment**: Consider data collection method\n",
        "- **Violation**: Clustered or repeated measures require special handling\n",
        "\n",
        "---\n",
        "\n",
        "## What This Analysis Does\n",
        "\n",
        "### How We Test Each Assumption\n",
        "\n",
        "#### **Linearity Test**\n",
        "- **Method**: Fit both linear (degree 1) and quadratic (degree 2) polynomial models\n",
        "- **Comparison**: Calculate R² for each model\n",
        "- **Decision rule**: If quadratic R² improvement < 0.05, relationship is sufficiently linear. If adding a curved term only explains 5% more variance, the straight-line relationship is adequate for correlation analysis.\n",
        "- **Why this works**: Non-linear relationships will show substantial improvement with quadratic terms. Curved relationships are better captured by quadratic models, so a large R² jump (>0.05) indicates the linear correlation is missing important non-linear patterns.\n",
        "\n",
        "- **Dumbing it down**: We fit a straight line and a curved line to the data and compare how well each explains the relationship. If the curved line only explains 5% more variance than the straight line, we conclude the relationship is linear enough for Pearson correlation..\n",
        "\n",
        "#### **Normality Test**\n",
        "- **Method**: Shapiro-Wilk test for each variable\n",
        "- **Null hypothesis**: Data comes from normal distribution\n",
        "- **Decision rule**: p > 0.05 suggests normality\n",
        "- **Sample size limit**: Uses sample of 5000 if dataset is larger\n",
        "- **Visual check**: Q-Q plots show departures from straight line\n",
        "\n",
        "- **Dumbing it down**: We run a Shapiro-Wilk statistical test on each variable to check if it follows a normal (bell-shaped) distribution. If the p-value is greater than 0.05, we assume the variable is normal enough for Pearson correlation.\n",
        "\n",
        "#### **Homoscedasticity Test**\n",
        "- **Method**: Simplified Breusch-Pagan test\n",
        "- **Process**: Calculate residuals from linear regression, then correlate |residuals| with fitted values\n",
        "- **Logic**: If variance is constant, residuals shouldn't correlate with predicted values\n",
        "- **Decision rule**: p > 0.05 suggests constant variance\n",
        "\n",
        "- **Dumbing it down**: We calculate residuals (errors) from a linear regression, then check if the absolute residuals correlate with the predicted values. If there's no significant correlation (p > 0.05), it means the variance is constant across all values.\n",
        "\n",
        "#### **Outlier Detection**\n",
        "- **Univariate outliers**: Z-scores > 3 (more than 3 standard deviations from mean)\n",
        "- **Bivariate outliers**: Mahalanobis distance with chi-square threshold\n",
        "- **Mahalanobis distance**: Measures how far a point is from the center of data in a multi-dimensional space\n",
        "- **Threshold**: χ² critical value at 99.9% confidence for 2 variables\n",
        "- **Warning level**: >5% outliers suggests attention needed\n",
        "\n",
        "- **Dumbing it down**: For univariate outliers, we identify points more than 3 standard deviations from the mean using z-scores. For bivariate outliers, we calculate Mahalanobis distance (which accounts for the correlation between variables) and flag points that exceed the chi-square threshold at 99.9% confidence. If more than 5% of data points are flagged as outliers by either method, we consider this problematic for correlation analysis.\n",
        "\n",
        "### Visual Diagnostics\n",
        "- **Scatter plots**: Linear vs. curved relationships\n",
        "- **Q-Q plots**: Departure from normality\n",
        "- **Residual plots**: Homoscedasticity violations\n",
        "- **Outlier plots**: Mahalanobis distance detection\n",
        "\n",
        "### Recommendations\n",
        "- **All assumptions met**: Use Pearson correlation confidently\n",
        "- **Some violations**: Compare Pearson vs. Spearman results\n",
        "- **Major violations**: Prefer Spearman rank correlation\n",
        "\n",
        "---\n",
        "\n",
        "## Spearman's Rank Correlation\n",
        "\n",
        "### When to Use Spearman\n",
        "- **Non-normal data**: Robust to distribution shape\n",
        "- **Non-linear monotonic relationships**: Captures consistent increasing/decreasing trends\n",
        "- **Outlier presence**: Less sensitive to extreme values\n",
        "- **Ordinal data**: Works with ranked data\n",
        "\n",
        "### Interpretation\n",
        "- **ρ (rho)**: Spearman correlation coefficient\n",
        "- **Values**: Same range as Pearson (-1 to +1)\n",
        "- **Meaning**: Strength of monotonic (consistently increasing/decreasing) relationship\n",
        "\n",
        "### Comparison Strategy\n",
        "This analysis compares Pearson vs. Spearman correlations to reveal:\n",
        "- **High agreement**: Linear relationships with normal data\n",
        "- **Moderate disagreement**: Some non-linearity or outliers present\n",
        "- **Major disagreement**: Serious assumption violations\n",
        "\n",
        "When assumptions are violated, **Spearman provides more reliable estimates** of the true relationship strength."
      ],
      "metadata": {
        "id": "m2FH_hJ4EqEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_correlation_assumptions(data, var1, var2, create_plots=True):\n",
        "    \"\"\"\n",
        "    Assumption checking for Pearson correlation between two variables.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(f\"ASSUMPTION CHECKING: {var1} vs {var2}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Get clean data for the pair\n",
        "    clean_data = data[[var1, var2]].dropna()\n",
        "    x = clean_data[var1]\n",
        "    y = clean_data[var2]\n",
        "    n = len(clean_data)\n",
        "\n",
        "    print(f\"Sample size: {n} complete pairs\")\n",
        "\n",
        "    # 1. Linear relationship check\n",
        "    print(f\"\\n1. LINEAR RELATIONSHIP\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    # Calculate Pearson correlation\n",
        "    r_pearson, p_pearson = pearsonr(x, y)\n",
        "\n",
        "    # Fit polynomial models to test for non-linearity\n",
        "    #Note: Remember that R-squared indicates how well the model fits the data\n",
        "    # Linear model (degree 1)\n",
        "    linear_coef = np.polyfit(x, y, 1)\n",
        "    linear_pred = np.polyval(linear_coef, x)\n",
        "    linear_r2 = 1 - (np.sum((y - linear_pred) ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
        "\n",
        "    # Quadratic model (degree 2)\n",
        "    quad_coef = np.polyfit(x, y, 2)\n",
        "    quad_pred = np.polyval(quad_coef, x)\n",
        "    quad_r2 = 1 - (np.sum((y - quad_pred) ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
        "\n",
        "    # Test for non-linearity\n",
        "    r2_improvement = quad_r2 - linear_r2\n",
        "    linearity_ok = r2_improvement < 0.05  # Arbitrary threshold\n",
        "\n",
        "    print(f\"Linear R² = {linear_r2:.4f}\")\n",
        "    print(f\"Quadratic R² = {quad_r2:.4f}\")\n",
        "    print(f\"R² improvement = {r2_improvement:.4f}\")\n",
        "    print(f\"Linearity assumption: {'SATISFIED' if linearity_ok else 'QUESTIONABLE'}\")\n",
        "\n",
        "    # 2. Normality check\n",
        "    print(f\"\\n2. NORMALITY\")\n",
        "    print(\"-\" * 15)\n",
        "\n",
        "    # Shapiro-Wilk tests (use sample if too large)\n",
        "    x_sample = x.sample(min(5000, len(x)), random_state=42) if len(x) > 5000 else x\n",
        "    y_sample = y.sample(min(5000, len(y)), random_state=42) if len(y) > 5000 else y\n",
        "\n",
        "    shapiro_x = stats.shapiro(x_sample)\n",
        "    shapiro_y = stats.shapiro(y_sample)\n",
        "\n",
        "    normality_x = shapiro_x.pvalue > 0.05\n",
        "    normality_y = shapiro_y.pvalue > 0.05\n",
        "\n",
        "    print(f\"{var1}: W = {shapiro_x.statistic:.4f}, p = {shapiro_x.pvalue:.4f} ({'Normal' if normality_x else 'Non-normal'})\")\n",
        "    print(f\"{var2}: W = {shapiro_y.statistic:.4f}, p = {shapiro_y.pvalue:.4f} ({'Normal' if normality_y else 'Non-normal'})\")\n",
        "\n",
        "    # 3. Homoscedasticity check\n",
        "    print(f\"\\n3. HOMOSCEDASTICITY\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Calculate residuals from linear regression\n",
        "    residuals = y - linear_pred\n",
        "\n",
        "    # Breusch-Pagan test for heteroscedasticity\n",
        "    # Simple version: correlation between |residuals| and fitted values\n",
        "    abs_residuals = np.abs(residuals)\n",
        "    fitted_values = linear_pred\n",
        "\n",
        "    hetero_corr, hetero_p = pearsonr(fitted_values, abs_residuals)\n",
        "    homoscedasticity_ok = hetero_p > 0.05\n",
        "\n",
        "    print(f\"Breusch-Pagan-like test:\")\n",
        "    print(f\"Correlation |residuals| vs fitted: r = {hetero_corr:.4f}, p = {hetero_p:.4f}\")\n",
        "    print(f\"Homoscedasticity assumption: {'SATISFIED' if homoscedasticity_ok else 'VIOLATED'}\")\n",
        "\n",
        "    # 4. Outlier detection\n",
        "    print(f\"\\n4. OUTLIER DETECTION\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Z-score method for univariate outliers\n",
        "    #Note: Remember that Z-score indicates how many standard deviations a data point is from the mean of a distribution\n",
        "    z_scores_x = np.abs(stats.zscore(x))\n",
        "    z_scores_y = np.abs(stats.zscore(y))\n",
        "\n",
        "    outliers_x = np.sum(z_scores_x > 3)\n",
        "    outliers_y = np.sum(z_scores_y > 3)\n",
        "\n",
        "    # Bivariate outliers using Mahalanobis distance\n",
        "    data_array = np.column_stack([x, y])    # Combine x and y into a 2D array\n",
        "    mean_vec = np.mean(data_array, axis=0)   # Calculate mean of x and mean of y\n",
        "    cov_matrix = np.cov(data_array.T)    # Calculate covariance matrix\n",
        "\n",
        "    try:\n",
        "        inv_cov = np.linalg.inv(cov_matrix)   # Invert the covariance matrix\n",
        "        mahal_dist = []\n",
        "        for i in range(len(data_array)):\n",
        "            diff = data_array[i] - mean_vec   # How far this point is from the center\n",
        "            mahal_dist.append(np.sqrt(diff.T @ inv_cov @ diff))  # Mahalanobis distance formula\n",
        "\n",
        "        mahal_dist = np.array(mahal_dist)\n",
        "        # Chi-square critical value for 2 variables at 99.9% confidence\n",
        "        chi2_critical = stats.chi2.ppf(0.999, df=2)    # Get chi-square threshold for 99.9% confidence\n",
        "        bivariate_outliers = np.sum(mahal_dist**2 > chi2_critical)  # Count outliers\n",
        "\n",
        "    except np.linalg.LinAlgError:\n",
        "        bivariate_outliers = 0\n",
        "        mahal_dist = np.zeros(len(data_array))\n",
        "\n",
        "    total_outliers = max(outliers_x, outliers_y, bivariate_outliers)\n",
        "    outliers_ok = total_outliers < (0.05 * n)  # Less than 5% outliers\n",
        "\n",
        "    print(f\"Univariate outliers (|z| > 3):\")\n",
        "    print(f\"  {var1}: {outliers_x} ({outliers_x/n*100:.1f}%)\")\n",
        "    print(f\"  {var2}: {outliers_y} ({outliers_y/n*100:.1f}%)\")\n",
        "    print(f\"Bivariate outliers (Mahalanobis): {bivariate_outliers} ({bivariate_outliers/n*100:.1f}%)\")\n",
        "    print(f\"Outlier assumption: {'SATISFIED' if outliers_ok else 'ATTENTION NEEDED'}\")\n",
        "\n",
        "    # Create diagnostic plots if requested\n",
        "    if create_plots:\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(f'Correlation Assumptions: {var1} vs {var2}', fontsize=16)\n",
        "\n",
        "        # Plot 1: Scatter plot with linear and quadratic fits\n",
        "        axes[0, 0].scatter(x, y, alpha=0.6, s=30)\n",
        "\n",
        "        # Linear fit\n",
        "        x_line = np.linspace(x.min(), x.max(), 100)\n",
        "        y_linear = np.polyval(linear_coef, x_line)\n",
        "        y_quad = np.polyval(quad_coef, x_line)\n",
        "\n",
        "        axes[0, 0].plot(x_line, y_linear, 'r-', linewidth=2, label=f'Linear (R² = {linear_r2:.3f})')\n",
        "        axes[0, 0].plot(x_line, y_quad, 'g--', linewidth=2, label=f'Quadratic (R² = {quad_r2:.3f})')\n",
        "\n",
        "        axes[0, 0].set_xlabel(var1)\n",
        "        axes[0, 0].set_ylabel(var2)\n",
        "        axes[0, 0].set_title('Linearity Check')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Residuals vs Fitted (homoscedasticity)\n",
        "        axes[0, 1].scatter(fitted_values, residuals, alpha=0.6, s=30)\n",
        "        axes[0, 1].axhline(y=0, color='red', linestyle='--')\n",
        "        axes[0, 1].set_xlabel('Fitted Values')\n",
        "        axes[0, 1].set_ylabel('Residuals')\n",
        "        axes[0, 1].set_title('Residuals vs Fitted (Homoscedasticity)')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 3: Q-Q plot for X variable\n",
        "        stats.probplot(x, dist=\"norm\", plot=axes[0, 2])\n",
        "        axes[0, 2].set_title(f'Q-Q Plot: {var1}')\n",
        "        axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 4: Q-Q plot for Y variable\n",
        "        stats.probplot(y, dist=\"norm\", plot=axes[1, 0])\n",
        "        axes[1, 0].set_title(f'Q-Q Plot: {var2}')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 5: Outlier detection (Mahalanobis distance)\n",
        "        if len(mahal_dist) > 0:\n",
        "            axes[1, 1].scatter(range(len(mahal_dist)), mahal_dist**2, alpha=0.6, s=30)\n",
        "            axes[1, 1].axhline(y=chi2_critical, color='red', linestyle='--',\n",
        "                              label=f'Critical value = {chi2_critical:.1f}')\n",
        "            axes[1, 1].set_xlabel('Observation Index')\n",
        "            axes[1, 1].set_ylabel('Squared Mahalanobis Distance')\n",
        "            axes[1, 1].set_title('Bivariate Outlier Detection')\n",
        "            axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 6: Scale-Location plot\n",
        "        sqrt_abs_residuals = np.sqrt(np.abs(residuals))\n",
        "        axes[1, 2].scatter(fitted_values, sqrt_abs_residuals, alpha=0.6, s=30)\n",
        "        axes[1, 2].set_xlabel('Fitted Values')\n",
        "        axes[1, 2].set_ylabel('√|Residuals|')\n",
        "        axes[1, 2].set_title('Scale-Location Plot')\n",
        "        axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Summary\n",
        "    assumptions_met = sum([linearity_ok, normality_x, normality_y, homoscedasticity_ok, outliers_ok])\n",
        "    total_assumptions = 5\n",
        "\n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"ASSUMPTION CHECK SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Assumptions satisfied: {assumptions_met}/{total_assumptions}\")\n",
        "\n",
        "    if assumptions_met >= 4:\n",
        "        recommendation = \"Pearson correlation is appropriate\"\n",
        "    elif assumptions_met >= 2:\n",
        "        recommendation = \"Consider both Pearson and Spearman correlations\"\n",
        "    else:\n",
        "        recommendation = \"Spearman correlation recommended\"\n",
        "\n",
        "    print(f\"Recommendation: {recommendation}\")\n",
        "\n",
        "    return {\n",
        "        'linearity': linearity_ok,\n",
        "        'normality_x': normality_x,\n",
        "        'normality_y': normality_y,\n",
        "        'homoscedasticity': homoscedasticity_ok,\n",
        "        'outliers_ok': outliers_ok,\n",
        "        'assumptions_met': assumptions_met,\n",
        "        'recommendation': recommendation,\n",
        "        'pearson_r': r_pearson,\n",
        "        'sample_size': n,\n",
        "        'outlier_counts': {\n",
        "            'univariate_x': outliers_x,\n",
        "            'univariate_y': outliers_y,\n",
        "            'bivariate': bivariate_outliers\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def perform_spearman_correlation(data, continuous_vars):\n",
        "    \"\"\"\n",
        "    Perform Spearman's rank correlation analysis as a non-parametric alternative.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"SPEARMAN'S RANK CORRELATION ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"Spearman's rank correlation (ρ) measures monotonic relationships\")\n",
        "    print(\"It's robust to outliers and doesn't assume normality or linearity\")\n",
        "    print()\n",
        "\n",
        "    # Calculate Spearman correlation matrix - using pandas .corr function - all pairs results simultaneously\n",
        "    # Note: The .corr(method='spearman' method automatically converts data to ranks\n",
        "    spearman_matrix = data[continuous_vars].corr(method='spearman')\n",
        "\n",
        "    print(\"Spearman Correlation Matrix:\")\n",
        "    print(\"-\" * 30)\n",
        "    print(spearman_matrix.round(3))\n",
        "\n",
        "    # Compare with Pearson correlations\n",
        "    pearson_matrix = data[continuous_vars].corr(method='pearson')\n",
        "\n",
        "# Just so we can print p-values, let us do this individually, for each pair of variables at a time\n",
        "# using the scipy.stats functions, which returns the p-values\n",
        "    print(f\"\\nSpearman vs Pearson Correlations:\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    spearman_results = []\n",
        "\n",
        "    for i in range(len(continuous_vars)):\n",
        "        for j in range(i+1, len(continuous_vars)):\n",
        "            var1 = continuous_vars[i]\n",
        "            var2 = continuous_vars[j]\n",
        "\n",
        "            # Get aligned data first, then calculate correlations\n",
        "            clean_data = data[[var1, var2]].dropna()\n",
        "            x = clean_data[var1]\n",
        "            y = clean_data[var2]\n",
        "\n",
        "            # Spearman correlation (using function from scipy.stats)\n",
        "            rho, p_spearman = spearmanr(x, y)\n",
        "\n",
        "            # Pearson correlation for comparison (using function from scipy.stats)\n",
        "            r_pearson, p_pearson = pearsonr(x, y)\n",
        "\n",
        "            # Calculate difference\n",
        "            diff = abs(rho - r_pearson)\n",
        "\n",
        "            spearman_results.append({\n",
        "                'var1': var1,\n",
        "                'var2': var2,\n",
        "                'spearman_rho': rho,\n",
        "                'spearman_p': p_spearman,\n",
        "                'pearson_r': r_pearson,\n",
        "                'pearson_p': p_pearson,\n",
        "                'difference': diff,\n",
        "                'sample_size': len(clean_data)\n",
        "            })\n",
        "\n",
        "            # Interpret difference\n",
        "            if diff < 0.1:\n",
        "                agreement = \"High agreement\"\n",
        "            elif diff < 0.2:\n",
        "                agreement = \"Moderate agreement\"\n",
        "            else:\n",
        "                agreement = \"Poor agreement\"\n",
        "\n",
        "            print(f\"{var1} ↔ {var2}:\")\n",
        "            print(f\"  Spearman ρ = {rho:>7.3f} (p = {p_spearman:>6.4f})\")\n",
        "            print(f\"  Pearson  r = {r_pearson:>7.3f} (p = {p_pearson:>6.4f})\")\n",
        "            print(f\"  Difference = {diff:>7.3f} ({agreement})\")\n",
        "            print()\n",
        "\n",
        "    # Create comparison visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Spearman vs Pearson Correlation Comparison', fontsize=16)\n",
        "\n",
        "    # Plot 1: Spearman correlation heatmap\n",
        "    #mask = np.triu(np.ones_like(spearman_matrix, dtype=bool))\n",
        "    sns.heatmap(spearman_matrix,\n",
        "                #mask=mask,\n",
        "                annot=True, cmap='RdBu_r', center=0,\n",
        "                square=True, fmt='.3f', ax=axes[0, 0],\n",
        "                cbar_kws={'label': 'Spearman ρ'})\n",
        "    axes[0, 0].set_title('Spearman Correlation Matrix')\n",
        "\n",
        "    # Plot 2: Pearson correlation heatmap\n",
        "    sns.heatmap(pearson_matrix,\n",
        "                #mask=mask,\n",
        "                annot=True, cmap='RdBu_r', center=0,\n",
        "                square=True, fmt='.3f', ax=axes[0, 1],\n",
        "                cbar_kws={'label': 'Pearson r'})\n",
        "    axes[0, 1].set_title('Pearson Correlation Matrix')\n",
        "\n",
        "    # Plot 3: Spearman vs Pearson scatter plot\n",
        "    spearman_values = []\n",
        "    pearson_values = []\n",
        "\n",
        "    for result in spearman_results:\n",
        "        spearman_values.append(result['spearman_rho'])\n",
        "        pearson_values.append(result['pearson_r'])\n",
        "\n",
        "    axes[1, 0].scatter(pearson_values, spearman_values, alpha=0.7, s=60)\n",
        "\n",
        "    # Add diagonal line (perfect agreement)\n",
        "    min_val = min(min(spearman_values), min(pearson_values))\n",
        "    max_val = max(max(spearman_values), max(pearson_values))\n",
        "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--',\n",
        "                   linewidth=2, label='Perfect agreement')\n",
        "\n",
        "    axes[1, 0].set_xlabel('Pearson r')\n",
        "    axes[1, 0].set_ylabel('Spearman ρ')\n",
        "    axes[1, 0].set_title('Spearman vs Pearson Correlations')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Difference magnitude histogram\n",
        "    differences = [result['difference'] for result in spearman_results]\n",
        "    axes[1, 1].hist(differences, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[1, 1].set_xlabel('|Spearman ρ - Pearson r|')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].set_title('Distribution of Correlation Differences')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add statistics\n",
        "    mean_diff = np.mean(differences)\n",
        "    max_diff = np.max(differences)\n",
        "    axes[1, 1].axvline(mean_diff, color='red', linestyle='--',\n",
        "                      label=f'Mean difference = {mean_diff:.3f}')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"Comparison Summary:\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Mean absolute difference: {mean_diff:.3f}\")\n",
        "    print(f\"Maximum difference: {max_diff:.3f}\")\n",
        "    print(f\"Number of pairs with |diff| > 0.1: {sum(1 for d in differences if d > 0.1)}\")\n",
        "    print(f\"Number of pairs with |diff| > 0.2: {sum(1 for d in differences if d > 0.2)}\")\n",
        "\n",
        "    return {\n",
        "        'spearman_matrix': spearman_matrix,\n",
        "        'pairwise_results': spearman_results,\n",
        "        'comparison_stats': {\n",
        "            'mean_difference': mean_diff,\n",
        "            'max_difference': max_diff,\n",
        "            'differences': differences\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def analyze_penguin_correlations_comprehensive(data, continuous_vars):\n",
        "    \"\"\"\n",
        "    Comprehensive correlation analysis of penguin data with both methods.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"COMPREHENSIVE PENGUIN CORRELATION ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Check assumptions for key variable pairs\n",
        "    key_pairs = [\n",
        "        ('flipper_length_mm', 'body_mass_g'),\n",
        "        ('bill_length_mm', 'bill_depth_mm'),\n",
        "        ('bill_length_mm', 'body_mass_g')\n",
        "    ]\n",
        "\n",
        "    assumption_results = {}\n",
        "\n",
        "    for var1, var2 in key_pairs:\n",
        "        print(f\"\\n Analyzing {var1} vs {var2}\")\n",
        "        assumption_results[f\"{var1}_{var2}\"] = check_correlation_assumptions(\n",
        "            data, var1, var2, create_plots=True\n",
        "        )\n",
        "\n",
        "    # Summary of assumption checks\n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"ASSUMPTION CHECK SUMMARY FOR ALL PAIRS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for pair_name, results in assumption_results.items():\n",
        "        # FIXED: More robust string splitting\n",
        "        parts = pair_name.split('_')\n",
        "        # Reconstruct variable names properly\n",
        "        if len(parts) >= 4:  # e.g., \"bill_length_mm_body_mass_g\"\n",
        "            var1 = '_'.join(parts[:-3])  # Everything except last 3 parts\n",
        "            var2 = '_'.join(parts[-3:])  # Last 3 parts\n",
        "        else:\n",
        "            var1, var2 = pair_name.split('_', 1)\n",
        "\n",
        "        var1_clean = var1.replace('_', ' ')\n",
        "        var2_clean = var2.replace('_', ' ')\n",
        "\n",
        "        print(f\"\\n{var1_clean} ↔ {var2_clean}:\")\n",
        "        print(f\"  Assumptions met: {results['assumptions_met']}/5\")\n",
        "        print(f\"  Recommendation: {results['recommendation']}\")\n",
        "        print(f\"  Pearson r = {results['pearson_r']:.3f}\")\n",
        "\n",
        "    return assumption_results\n",
        "\n",
        "\n",
        "# Execute the analysis\n",
        "print(\"\\n EXECUTING ASSUMPTION CHECKING AND SPEARMAN ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1: Comprehensive assumption checking\n",
        "print(\"\\n Step 1: Checking correlation assumptions for key variable pairs\")\n",
        "penguin_assumptions = analyze_penguin_correlations_comprehensive(penguins_clean, continuous_vars)\n",
        "\n",
        "# Step 2: Spearman correlation analysis\n",
        "print(\"\\n Step 2: Spearman correlation analysis\")\n",
        "spearman_results = perform_spearman_correlation(penguins_clean, continuous_vars)"
      ],
      "metadata": {
        "id": "dhkEpun_Z6Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Partial Correlation Analysis\n",
        "\n",
        "## What is Partial Correlation?\n",
        "\n",
        "Partial correlation measures the relationship between two variables while controlling\n",
        "for the influence of one or more other variables (confounders). It answers the question:\n",
        "\"What would the correlation be if we held the confounding variables constant?\"\n",
        "\n",
        "In summary, Partial correlation statistically removes the effect of species from both variables, then calculates the correlation on what's left over.\n",
        "\n",
        "For our penguin dataset, after accounting for the fact that different species have different baseline measurements, is there still a relationship between bill length and body mass?\n",
        "\n",
        "## Mathematical Formula:\n",
        "For controlling one variable Z:\n",
        "r(xy.z) = [r(xy) - r(xz) × r(yz)] / √[(1 - r²(xz)) × (1 - r²(yz))]\n",
        "\n",
        "## When to Use Partial Correlations:\n",
        "1. **Suspected confounding**: When a third variable might explain the X-Y relationship\n",
        "2. **Spurious correlations**: When apparent relationships might be coincidental\n",
        "3. **Causal inference**: To isolate direct relationships from indirect ones\n",
        "4. **Control variables**: When you want to \"adjust\" for known influences\n",
        "\n",
        "## Example with Penguins:\n",
        "- Simple correlation: Bill length ↔ Body mass might be positive\n",
        "- But: Larger species have both longer bills AND higher mass\n",
        "- Partial correlation: Bill length ↔ Body mass, controlling for species\n",
        "- This reveals the within-species relationship"
      ],
      "metadata": {
        "id": "ByiDw5YFFzLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial Correlation Analysis with Penguin Dataset\n",
        "\n",
        "## What is Partial Correlation?\n",
        "\n",
        "**Partial correlation** measures the relationship between two variables while controlling for the effects of one or more other variables. It answers the question: \"What is the correlation between X and Y when we remove the influence of Z?\"\n",
        "\n",
        "### Why is this important?\n",
        "- **Confounding variables** can create misleading correlations\n",
        "- Species differences might artificially inflate or deflate correlations between morphological traits\n",
        "- Partial correlation helps us understand the \"true\" relationship between variables\n",
        "\n",
        "## Example with Penguins:\n",
        "- Simple correlation: Bill length ↔ Body mass might be positive\n",
        "- But: Larger species have both longer bills AND higher mass\n",
        "- Partial correlation: Bill length ↔ Body mass, controlling for species\n",
        "- This reveals the within-species relationship\n",
        "\n",
        "---\n",
        "\n",
        "## The Mathematical Intuition\n",
        "\n",
        "### Simple Correlation vs Partial Correlation\n",
        "\n",
        "**Simple correlation**: How much do X and Y vary together?\n",
        "```\n",
        "r(X,Y) = correlation between X and Y (ignoring all other factors)\n",
        "```\n",
        "\n",
        "**Partial correlation**: How much do X and Y vary together after removing the influence of Z?\n",
        "```\n",
        "r(X,Y|Z) = correlation between X and Y controlling for Z\n",
        "```\n",
        "\n",
        "### The Formula (for single control variable)\n",
        "```\n",
        "r(X,Y|Z) = [r(X,Y) - r(X,Z) × r(Y,Z)] / √[(1 - r(X,Z)²) × (1 - r(Y,Z)²)]\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `r(X,Y)` = simple correlation between X and Y\n",
        "- `r(X,Z)` = simple correlation between X and Z\n",
        "- `r(Y,Z)` = simple correlation between Y and Z\n",
        "\n",
        "---\n",
        "\n",
        "## Two Methods to Calculate Partial Correlation\n",
        "\n",
        "### Method 1: Regression Residuals Approach (Most Intuitive)\n",
        "\n",
        "This is the method our code uses primarily:\n",
        "\n",
        "1. **Step 1**: Regress X on the control variable(s) Z\n",
        "   - `X = β₀ + β₁Z + ε`\n",
        "   - Extract residuals: `X_residuals = X - X_predicted`\n",
        "\n",
        "2. **Step 2**: Regress Y on the control variable(s) Z\n",
        "   - `Y = γ₀ + γ₁Z + ε`\n",
        "   - Extract residuals: `Y_residuals = Y - Y_predicted`\n",
        "\n",
        "3. **Step 3**: Calculate correlation between residuals\n",
        "   - `r(X,Y|Z) = correlation(X_residuals, Y_residuals)`\n",
        "\n",
        "**Interpretation**: The residuals represent the part of X and Y that cannot be explained by Z. Their correlation is the partial correlation.\n",
        "\n",
        "**In case you don't know: \"Regress X on Z\" means \"predict X using Z\" or \"fit a line to predict X from Z\".** I bet you already know what Residuals are - they are the leftover parts after you remove the predictable pattern.\n",
        "\n",
        "### Method 2: Direct Formula\n",
        "Only works with a single control variable. Uses the mathematical formula shown above.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Species as a Confounding Variable\n",
        "\n",
        "### Why Control for Species?\n",
        "\n",
        "In the penguin dataset, different species have different:\n",
        "- **Body sizes**: Gentoo penguins are larger overall\n",
        "- **Bill shapes**: Adelie penguins have shorter, wider bills\n",
        "- **Ecological niches**: Different feeding strategies\n",
        "\n",
        "**The Problem**: If we don't control for species, we might find correlations that are really just reflecting species differences, not true biological relationships within species.\n",
        "\n",
        "### Example Scenario\n",
        "\n",
        "**Without controlling for species**:\n",
        "- We might find a strong positive correlation between bill length and body mass\n",
        "- But this could be because Gentoo penguins (larger species) have both longer bills AND larger bodies\n",
        "\n",
        "**With controlling for species**:\n",
        "- We remove the between-species differences\n",
        "- Now we see the within-species relationship\n",
        "- This tells us about the biological relationship independent of species identity\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## The Three Scenarios Demonstrated\n",
        "\n",
        "### Scenario 1: True Confounding\n",
        "```\n",
        "Z → X\n",
        "Z → Y\n",
        "X ↔ Y (no direct relationship)\n",
        "```\n",
        "- X and Y are correlated only because both depend on Z\n",
        "- **Partial correlation ≈ 0** (no true relationship)\n",
        "- **Simple correlation > 0** (spurious correlation)\n",
        "\n",
        "### Scenario 2: Partial Relationship\n",
        "```\n",
        "Z → X\n",
        "Z → Y\n",
        "X → Y (direct relationship exists)\n",
        "```\n",
        "- X and Y have both direct and indirect relationships\n",
        "- **Partial correlation > 0** (true relationship exists)\n",
        "- **Simple correlation > Partial correlation** (some confounding)\n",
        "\n",
        "### Scenario 3: Suppressor Variable\n",
        "```\n",
        "X → Z\n",
        "X → Y\n",
        "Z → Y (negative effect)\n",
        "```\n",
        "- Z suppresses the true X-Y relationship\n",
        "- **Partial correlation > Simple correlation** (true relationship stronger)\n",
        "- Controlling for Z reveals the hidden relationship\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FgKf2cEuUaue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_partial_correlation_concepts():\n",
        "    \"\"\"\n",
        "    Demonstrate partial correlation concepts with simulated data.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"PARTIAL CORRELATION CONCEPTS: SIMULATED EXAMPLES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    np.random.seed(42)\n",
        "    n = 200\n",
        "\n",
        "    # Scenario 1: True confounding\n",
        "    print(f\"\\nScenario 1: True Confounding\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Z (confounder) influences both X and Y\n",
        "    z1 = np.random.normal(0, 1, n)\n",
        "    x1 = 0.8 * z1 + np.random.normal(0, 0.6, n)  # X depends on Z\n",
        "    y1 = 0.7 * z1 + np.random.normal(0, 0.7, n)  # Y depends on Z\n",
        "    # X and Y are NOT directly related, only through Z\n",
        "\n",
        "    # Scenario 2: Partial relationship\n",
        "    print(f\"Scenario 2: Partial Relationship\")\n",
        "    print(\"-\" * 32)\n",
        "\n",
        "    # Z influences both X and Y, but X and Y also have direct relationship\n",
        "    z2 = np.random.normal(0, 1, n)\n",
        "    x2 = 0.6 * z2 + np.random.normal(0, 0.8, n)  # X depends on Z\n",
        "    y2 = 0.5 * z2 + 0.4 * x2 + np.random.normal(0, 0.6, n)  # Y depends on both Z and X\n",
        "\n",
        "    # Scenario 3: Suppressor variable\n",
        "    print(f\"Scenario 3: Suppressor Variable\")\n",
        "    print(\"-\" * 32)\n",
        "\n",
        "    # Z suppresses the true X-Y relationship\n",
        "    x3 = np.random.normal(0, 1, n)\n",
        "    z3 = 0.6 * x3 + np.random.normal(0, 0.8, n)  # Z depends on X\n",
        "    y3 = 0.8 * x3 - 0.5 * z3 + np.random.normal(0, 0.5, n)  # Y depends positively on X, negatively on Z\n",
        "\n",
        "    scenarios = [\n",
        "        (\"True Confounding\", x1, y1, z1, \"X and Y related only through Z\"),\n",
        "        (\"Partial Relationship\", x2, y2, z2, \"X and Y have both direct and indirect relationships\"),\n",
        "        (\"Suppressor Variable\", x3, y3, z3, \"Z suppresses the true X-Y relationship\")\n",
        "    ]\n",
        "\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "    fig.suptitle('Partial Correlation Scenarios', fontsize=16)\n",
        "\n",
        "    for i, (title, x, y, z, description) in enumerate(scenarios):\n",
        "        # Calculate correlations\n",
        "        r_xy_simple, _ = pearsonr(x, y)\n",
        "\n",
        "        # Calculate partial correlation manually\n",
        "        r_xz, _ = pearsonr(x, z)\n",
        "        r_yz, _ = pearsonr(y, z)\n",
        "\n",
        "        numerator = r_xy_simple - (r_xz * r_yz)\n",
        "        denominator = np.sqrt((1 - r_xz**2) * (1 - r_yz**2))\n",
        "        r_xy_partial = numerator / denominator if denominator != 0 else np.nan\n",
        "\n",
        "        # Plot 1: X vs Y scatter plot\n",
        "        axes[i, 0].scatter(x, y, alpha=0.6, s=30, c=z, cmap='viridis')\n",
        "        axes[i, 0].set_xlabel('X')\n",
        "        axes[i, 0].set_ylabel('Y')\n",
        "        axes[i, 0].set_title(f'{title}\\nSimple r = {r_xy_simple:.3f}')\n",
        "        axes[i, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Partial correlation (residuals)\n",
        "        # Regress X on Z, get residuals\n",
        "        slope_xz = np.cov(x, z)[0, 1] / np.var(z)\n",
        "        x_residuals = x - (slope_xz * z)\n",
        "\n",
        "        # Regress Y on Z, get residuals\n",
        "        slope_yz = np.cov(y, z)[0, 1] / np.var(z)\n",
        "        y_residuals = y - (slope_yz * z)\n",
        "\n",
        "        axes[i, 1].scatter(x_residuals, y_residuals, alpha=0.6, s=30)\n",
        "        axes[i, 1].set_xlabel('X residuals (controlling for Z)')\n",
        "        axes[i, 1].set_ylabel('Y residuals (controlling for Z)')\n",
        "        axes[i, 1].set_title(f'Partial r = {r_xy_partial:.3f}')\n",
        "        axes[i, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Add regression line to residuals plot\n",
        "        if not np.isnan(r_xy_partial):\n",
        "            z_line = np.polyfit(x_residuals, y_residuals, 1)\n",
        "            p_line = np.poly1d(z_line)\n",
        "            x_line = np.linspace(x_residuals.min(), x_residuals.max(), 100)\n",
        "            axes[i, 1].plot(x_line, p_line(x_line), 'r-', linewidth=2)\n",
        "\n",
        "        # Plot 3: Comparison bar chart\n",
        "        correlations = [r_xy_simple, r_xy_partial]\n",
        "        labels = ['Simple', 'Partial']\n",
        "        colors = ['skyblue', 'lightcoral']\n",
        "\n",
        "        bars = axes[i, 2].bar(labels, correlations, color=colors, alpha=0.7)\n",
        "        axes[i, 2].set_ylabel('Correlation Coefficient')\n",
        "        axes[i, 2].set_title('Simple vs Partial Correlation')\n",
        "        axes[i, 2].grid(True, alpha=0.3)\n",
        "        axes[i, 2].set_ylim(-1, 1)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, corr in zip(bars, correlations):\n",
        "            height = bar.get_height()\n",
        "            axes[i, 2].text(bar.get_x() + bar.get_width()/2., height + 0.02 if height >= 0 else height - 0.05,\n",
        "                           f'{corr:.3f}', ha='center', va='bottom' if height >= 0 else 'top')\n",
        "\n",
        "        # Add scenario description\n",
        "        axes[i, 0].text(0.02, 0.98, description, transform=axes[i, 0].transAxes,\n",
        "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
        "                       verticalalignment='top', fontsize=9)\n",
        "\n",
        "        print(f\"{title}:\")\n",
        "        print(f\"  Simple correlation: {r_xy_simple:.3f}\")\n",
        "        print(f\"  Partial correlation: {r_xy_partial:.3f}\")\n",
        "        print(f\"  Difference: {r_xy_simple - r_xy_partial:.3f}\")\n",
        "        print(f\"  Interpretation: {description}\")\n",
        "        print()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Step 1: Demonstrate partial correlation concepts\n",
        "print(\"\\n Step 1: Demonstrating partial correlation concepts\")\n",
        "demonstrate_partial_correlation_concepts()"
      ],
      "metadata": {
        "id": "jsk6_y3_aI-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Simulated Data Results**\n",
        "\n",
        "The simulated data perfectly demonstrated three key scenarios:\n",
        "\n",
        "**Scenario 1: True Confounding**\n",
        "\n",
        "Simple correlation: r = 0.498 → Partial correlation: r = -0.020\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "X and Y appear moderately correlated. But this correlation is misleading - it's not because X and Y directly influence each other, it's because both X and Y are influenced by Z.\n",
        "\n",
        "After controlling for Z (Partial correlation r = -0.020):\n",
        "\n",
        "We remove Z's influence from both X and Y. Now we see the true direct relationship between X and Y, Which is essentially zero (no meaningful direct relationship)\n",
        "\n",
        "The Key Insight:\n",
        "The huge difference (0.518) reveals that Z was responsible for almost ALL of the apparent X-Y relationship!\n",
        "\n",
        "Lesson: X and Y were only related through their mutual dependence on Z\n",
        "\n",
        "**Scenario 2: Partial Relationship**\n",
        "\n",
        "Simple correlation: r = 0.769 → Partial correlation: r = 0.515\n",
        "\n",
        "Result: Relationship weakens but remains substantial\n",
        "\n",
        "Lesson: X and Y have both direct and indirect relationships through Z\n",
        "\n",
        "**Scenario 3: Suppressor Variable**\n",
        "\n",
        "Simple correlation: r = 0.588 → Partial correlation: r = 0.805\n",
        "\n",
        "Result: The true relationship becomes STRONGER when controlling for Z\n",
        "\n",
        "Lesson: Z was masking/suppressing the true X-Y relationship\n",
        "\n",
        "These simulated examples provide the theoretical foundation for understanding what happens with the real penguin data, where species acts as the confounding variable in different ways for different morphological relationships."
      ],
      "metadata": {
        "id": "X3vWgA5taagu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Partial Correlation\n",
        "\n",
        "## Main Function: `calculate_partial_correlation()`\n",
        "\n",
        "In the following code, we calculate partial correlation between two variables while controlling for confounding variables (like species). This tells us what the correlation would be if we removed the influence of the confounding variables.\n",
        "\n",
        "### Method 1: Regression Residuals Approach\n",
        "- **What it does**: Uses linear regression to predict each variable from the control variables, then calculates correlation between the leftover residuals\n",
        "- **Steps**:\n",
        "  1. Predict X variable using control variables, get residuals\n",
        "  2. Predict Y variable using control variables, get residuals  \n",
        "  3. Calculate correlation between the two sets of residuals\n",
        "- **Why use this**: Most intuitive method that works with any number of control variables\n",
        "\n",
        "### Method 2: Direct Formula (Single Control Only)\n",
        "- **What it does**: Uses a mathematical formula to calculate partial correlation directly from simple correlations\n",
        "- **Formula**: `r_xy.z = (r_xy - r_xz × r_yz) / √[(1-r_xz²)(1-r_yz²)]`\n",
        "- **Why use this**: Faster computation and serves as a verification that Method 1 is working correctly\n",
        "- **Limitation**: Only works when controlling for one variable at a time\n",
        "\n",
        "**Why we use both methods**: Method 1 is the main approach, Method 2 serves as a mathematical check to ensure our calculations are correct. In our code, this method will be skipped as we have more than one control variables (species has 3 categories - Adelie, Chinstrap, Gentoo)\n",
        "\n",
        "## Helper Function: `create_dummy_variables()`\n",
        "\n",
        "Converts categorical variables (like species) into numerical dummy variables for use in regression. Uses one-hot encoding with `drop_first=True` to avoid multicollinearity.\n",
        "\n",
        "## Main Analysis: `analyze_species_effects()`\n",
        "\n",
        "In the following code, we compare simple correlations (ignoring species) with partial correlations (controlling for species) to see how much species affects the relationships between morphological variables.\n",
        "\n",
        "### Process:\n",
        "1. **Create dummy variables** for species (Adelie, Chinstrap, Gentoo)\n",
        "2. **Calculate simple correlations** between morphological variables (ignoring species)\n",
        "3. **Calculate partial correlations** between same variables controlling for species\n",
        "4. **Compare the two** to see how much species was confounding the relationship\n",
        "\n",
        "### Key Relationships Analyzed:\n",
        "- Bill length ↔ Body mass\n",
        "- Flipper length ↔ Body mass  \n",
        "- Bill length ↔ Bill depth\n",
        "- Bill length ↔ Flipper length\n",
        "\n",
        "### Interpretation:\n",
        "- **Large difference (>0.2)**: Species is a major confounder\n",
        "- **Moderate difference (0.1-0.2)**: Some species confounding present\n",
        "- **Small difference (<0.1)**: Relationship exists largely within species\n",
        "\n",
        "## Comparison Analysis: `perform_within_species_correlations()`\n",
        "\n",
        "In the following code, we calculate correlations separately for each species to see if relationships hold within individual species or are driven by between-species differences.\n",
        "\n",
        "### Process:\n",
        "1. **Split data by species** (Adelie, Chinstrap, Gentoo)\n",
        "2. **Calculate correlation matrix** for each species separately\n",
        "3. **Compare correlations across species** to see consistency\n",
        "4. **Calculate variance and range** of correlations across species\n",
        "\n",
        "### Key Insights:\n",
        "- **High variance across species**: Relationship may be driven by species differences\n",
        "- **Low variance across species**: Relationship is consistent within each species\n",
        "- **Visual comparison**: Heatmaps show correlation patterns for each species side-by-side\n",
        "\n",
        "## Overall Goal\n",
        "\n",
        "The code helps distinguish between:\n",
        "1. **True within-species relationships**: Correlations that exist within each species\n",
        "2. **Between-species artifacts**: Correlations that only appear when mixing species together\n",
        "3. **Species confounding**: How much species membership affects the apparent relationships between morphological variables"
      ],
      "metadata": {
        "id": "ETAaf9XHGd4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Notes:\n",
        "\n",
        "**Why only 2 dummy variables for 3 species:**\n",
        "\n",
        "This is about avoiding the multicollinearity problem in regression.\n",
        "\n",
        "When you have 3 categories (Adelie, Chinstrap, Gentoo), you only need 2 dummy variables to represent all the information.\n",
        "\n",
        "With 2 dummy variables:\n",
        "\n",
        "- species_Chinstrap = [0, 1, 0]  --> 1 if Chinstrap, 0 otherwise\n",
        "- species_Gentoo    = [0, 0, 1]  --> 1 if Gentoo, 0 otherwise\n",
        "\n",
        "Now we can identify all 3 species:\n",
        "- Adelie:     Chinstrap=0, Gentoo=0\n",
        "- Chinstrap:  Chinstrap=1, Gentoo=0  \n",
        "- Gentoo:     Chinstrap=0, Gentoo=1\n",
        "\n",
        "**Why not 3 dummy variables?**\n",
        "\n",
        "With 3 dummy variables, we have:\n",
        "\n",
        "species_Adelie + species_Chinstrap + species_Gentoo = 1 (always)\n",
        "\n",
        "This creates perfect multicollinearity - the three variables are perfectly correlated, which breaks regression calculations.\n",
        "\n",
        "**The dropped category (Adelie) becomes the reference group. The regression coefficients tell you how Chinstrap and Gentoo differ from Adelie.**\n",
        "\n",
        "In our code, you see this here:\n",
        "\n",
        "dummies = pd.get_dummies(data['species'], prefix='species', drop_first=True)"
      ],
      "metadata": {
        "id": "eTxnHiwFH807"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_partial_correlation(data, x_var, y_var, control_vars):\n",
        "    \"\"\"\n",
        "    Calculate partial correlation between two variables controlling for others.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"\\nCalculating partial correlation:\")\n",
        "    print(f\"X variable: {x_var}\")\n",
        "    print(f\"Y variable: {y_var}\")\n",
        "    print(f\"Controlling for: {', '.join(control_vars)}\")\n",
        "\n",
        "    # Get clean data\n",
        "    all_vars = [x_var, y_var] + control_vars\n",
        "    clean_data = data[all_vars].dropna()\n",
        "    n = len(clean_data)\n",
        "\n",
        "    print(f\"Sample size: {n}\")\n",
        "\n",
        "    # Method 1: Using regression residuals (most intuitive)\n",
        "    print(f\"\\nMethod 1: Regression Residuals Approach\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    # Step 1: Regress X on control variables, get residuals\n",
        "    X_controls = clean_data[control_vars]\n",
        "    X_target = clean_data[x_var]\n",
        "\n",
        "    model_x = LinearRegression()\n",
        "    model_x.fit(X_controls, X_target)\n",
        "    x_residuals = X_target - model_x.predict(X_controls)\n",
        "\n",
        "    # Step 2: Regress Y on control variables, get residuals\n",
        "    Y_target = clean_data[y_var]\n",
        "\n",
        "    model_y = LinearRegression()\n",
        "    model_y.fit(X_controls, Y_target)\n",
        "    y_residuals = Y_target - model_y.predict(X_controls)\n",
        "\n",
        "    # Step 3: Correlate the residuals\n",
        "    partial_r_residuals, p_value_residuals = pearsonr(x_residuals, y_residuals)\n",
        "\n",
        "    print(f\"Partial correlation (residuals method): {partial_r_residuals:.4f}\")\n",
        "    print(f\"P-value: {p_value_residuals:.6f}\")\n",
        "\n",
        "    # Method 2: Direct formula for single control variable:\n",
        "    # The direct formula method can only handle one control variable at a time.\n",
        "    # But in our code we are Controlling for: species_Chinstrap, species_Gentoo\n",
        "    # So this method will be skipped\n",
        "    if len(control_vars) == 1:\n",
        "        print(f\"\\nMethod 2: Direct Formula (Single Control)\")\n",
        "        print(\"-\" * 45)\n",
        "\n",
        "        z_var = control_vars[0]\n",
        "\n",
        "        # Calculate simple correlations\n",
        "        r_xy, _ = pearsonr(clean_data[x_var], clean_data[y_var])\n",
        "        r_xz, _ = pearsonr(clean_data[x_var], clean_data[z_var])\n",
        "        r_yz, _ = pearsonr(clean_data[y_var], clean_data[z_var])\n",
        "\n",
        "        # Apply partial correlation formula\n",
        "        numerator = r_xy - (r_xz * r_yz)\n",
        "        denominator = np.sqrt((1 - r_xz**2) * (1 - r_yz**2))\n",
        "\n",
        "        if denominator != 0:\n",
        "            partial_r_formula = numerator / denominator\n",
        "        else:\n",
        "            partial_r_formula = np.nan\n",
        "\n",
        "        print(f\"Simple correlations:\")\n",
        "        print(f\"  r({x_var}, {y_var}) = {r_xy:.4f}\")\n",
        "        print(f\"  r({x_var}, {z_var}) = {r_xz:.4f}\")\n",
        "        print(f\"  r({y_var}, {z_var}) = {r_yz:.4f}\")\n",
        "        print(f\"Partial correlation (formula): {partial_r_formula:.4f}\")\n",
        "\n",
        "        # Verify methods agree\n",
        "        diff = abs(partial_r_residuals - partial_r_formula)\n",
        "        print(f\"Difference between methods: {diff:.6f}\")\n",
        "\n",
        "    # Calculate degrees of freedom and significance test\n",
        "    df = n - len(control_vars) - 2\n",
        "\n",
        "    # t-statistic for partial correlation\n",
        "    if abs(partial_r_residuals) < 1:\n",
        "        t_stat = partial_r_residuals * np.sqrt(df / (1 - partial_r_residuals**2))\n",
        "        p_value_t = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n",
        "    else:\n",
        "        t_stat = np.inf\n",
        "        p_value_t = 0.0\n",
        "\n",
        "    print(f\"\\nSignificance test:\")\n",
        "    print(f\"Degrees of freedom: {df}\")\n",
        "    print(f\"t-statistic: {t_stat:.4f}\")\n",
        "    print(f\"p-value: {p_value_t:.6f}\")\n",
        "\n",
        "    # Effect size interpretation\n",
        "    if abs(partial_r_residuals) >= 0.5:\n",
        "        effect_size = \"Large\"\n",
        "    elif abs(partial_r_residuals) >= 0.3:\n",
        "        effect_size = \"Medium\"\n",
        "    elif abs(partial_r_residuals) >= 0.1:\n",
        "        effect_size = \"Small\"\n",
        "    else:\n",
        "        effect_size = \"Negligible\"\n",
        "\n",
        "    print(f\"Effect size: {effect_size}\")\n",
        "\n",
        "    return {\n",
        "        'partial_correlation': partial_r_residuals,\n",
        "        'p_value': p_value_residuals,\n",
        "        't_statistic': t_stat,\n",
        "        'degrees_of_freedom': df,\n",
        "        'effect_size': effect_size,\n",
        "        'sample_size': n,\n",
        "        'x_residuals': x_residuals,\n",
        "        'y_residuals': y_residuals,\n",
        "        'method_comparison': {\n",
        "            'residuals_method': partial_r_residuals,\n",
        "            'formula_method': partial_r_formula if len(control_vars) == 1 else None\n",
        "        }\n",
        "    }\n",
        "\n",
        "# one-hot encoding categorical variables\n",
        "def create_dummy_variables(data, categorical_var):\n",
        "    \"\"\"\n",
        "    Create dummy variables for categorical variable to use in partial correlation.\n",
        "\n",
        "    \"\"\"\n",
        "    # Create dummy variables (drop first to avoid multicollinearity): one-hot encoding categorical variables\n",
        "    dummies = pd.get_dummies(data[categorical_var], prefix=categorical_var, drop_first=True)\n",
        "    return dummies\n",
        "\n",
        "def analyze_species_effects(data, continuous_vars):\n",
        "    \"\"\"\n",
        "    Analyze how species affects correlations between morphological variables.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"SPECIES EFFECTS ON PENGUIN MORPHOLOGY CORRELATIONS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Create species dummy variables\n",
        "    species_dummies = create_dummy_variables(data, 'species')\n",
        "\n",
        "    # Combine with continuous variables\n",
        "    analysis_data = pd.concat([data[continuous_vars], species_dummies], axis=1)\n",
        "    analysis_data = analysis_data.dropna()\n",
        "\n",
        "    print(f\"Species dummy variables created:\")\n",
        "    for col in species_dummies.columns:\n",
        "        print(f\"  • {col}\")\n",
        "\n",
        "    # Analyze key morphological relationships\n",
        "    key_relationships = [\n",
        "        ('bill_length_mm', 'body_mass_g', 'Bill length ↔ Body mass'),\n",
        "        ('flipper_length_mm', 'body_mass_g', 'Flipper length ↔ Body mass'),\n",
        "        ('bill_length_mm', 'bill_depth_mm', 'Bill length ↔ Bill depth'),\n",
        "        ('bill_length_mm', 'flipper_length_mm', 'Bill length ↔ Flipper length')\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(f\"\\nAnalyzing relationships with and without species control:\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    for x_var, y_var, description in key_relationships:\n",
        "        print(f\"\\n{description}\")\n",
        "        print(\"-\" * len(description))\n",
        "\n",
        "\n",
        "        # Simple correlation (no control) - ensure same sample as partial correlation\n",
        "        clean_pair_data = analysis_data[[x_var, y_var]].dropna()\n",
        "        simple_r, simple_p = pearsonr(clean_pair_data[x_var], clean_pair_data[y_var])\n",
        "\n",
        "        # Partial correlation controlling for species\n",
        "        control_vars = list(species_dummies.columns)\n",
        "        partial_result = calculate_partial_correlation(analysis_data, x_var, y_var, control_vars)\n",
        "\n",
        "        # Calculate the difference\n",
        "        difference = simple_r - partial_result['partial_correlation']\n",
        "\n",
        "        print(f\"\\nSummary:\")\n",
        "        print(f\"Simple correlation:    r = {simple_r:.4f} (p = {simple_p:.4f})\")\n",
        "        print(f\"Partial correlation:   r = {partial_result['partial_correlation']:.4f} (p = {partial_result['p_value']:.4f})\")\n",
        "        print(f\"Difference:            Δr = {difference:.4f}\")\n",
        "\n",
        "        # Interpretation\n",
        "        if abs(difference) > 0.2:\n",
        "            interpretation = \"Large species effect - species is a major confounder\"\n",
        "        elif abs(difference) > 0.1:\n",
        "            interpretation = \"Moderate species effect - some confounding present\"\n",
        "        else:\n",
        "            interpretation = \"Small species effect - relationship is largely within-species\"\n",
        "\n",
        "        print(f\"Interpretation: {interpretation}\")\n",
        "\n",
        "        results[f\"{x_var}_{y_var}\"] = {\n",
        "            'description': description,\n",
        "            'simple_correlation': simple_r,\n",
        "            'simple_p_value': simple_p,\n",
        "            'partial_correlation': partial_result['partial_correlation'],\n",
        "            'partial_p_value': partial_result['p_value'],\n",
        "            'difference': difference,\n",
        "            'interpretation': interpretation,\n",
        "            'x_residuals': partial_result['x_residuals'],\n",
        "            'y_residuals': partial_result['y_residuals']\n",
        "        }\n",
        "\n",
        "    # Create comprehensive visualization\n",
        "    fig, axes = plt.subplots(2, len(key_relationships), figsize=(5*len(key_relationships), 10))\n",
        "    fig.suptitle('Simple vs Partial Correlations: Species Effects', fontsize=16)\n",
        "\n",
        "    for i, (x_var, y_var, description) in enumerate(key_relationships):\n",
        "        result = results[f\"{x_var}_{y_var}\"]\n",
        "\n",
        "        # Top row: Simple correlations with species colored\n",
        "        sns.scatterplot(data=data, x=x_var, y=y_var, hue='species', ax=axes[0, i], alpha=0.7)\n",
        "\n",
        "        # Add overall regression line\n",
        "        x_vals = data[x_var].dropna()\n",
        "        y_vals = data[y_var].dropna()\n",
        "        z = np.polyfit(x_vals, y_vals, 1)\n",
        "        p = np.poly1d(z)\n",
        "        axes[0, i].plot(x_vals, p(x_vals), \"r--\", alpha=0.8, linewidth=2, label='Overall trend')\n",
        "\n",
        "        axes[0, i].set_title(f'Simple Correlation\\nr = {result[\"simple_correlation\"]:.3f}')\n",
        "        axes[0, i].legend()\n",
        "        axes[0, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Bottom row: Partial correlation (residuals plot)\n",
        "        axes[1, i].scatter(result['x_residuals'], result['y_residuals'], alpha=0.6, s=30)\n",
        "\n",
        "        # Add regression line for residuals\n",
        "        z_res = np.polyfit(result['x_residuals'], result['y_residuals'], 1)\n",
        "        p_res = np.poly1d(z_res)\n",
        "        x_res_line = np.linspace(result['x_residuals'].min(), result['x_residuals'].max(), 100)\n",
        "        axes[1, i].plot(x_res_line, p_res(x_res_line), \"r-\", linewidth=2)\n",
        "\n",
        "        axes[1, i].set_title(f'Partial Correlation\\nr = {result[\"partial_correlation\"]:.3f}')\n",
        "        axes[1, i].set_xlabel(f'{x_var} (residuals)')\n",
        "        axes[1, i].set_ylabel(f'{y_var} (residuals)')\n",
        "        axes[1, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Add difference annotation\n",
        "        diff_text = f'Δr = {result[\"difference\"]:.3f}'\n",
        "        axes[1, i].text(0.05, 0.95, diff_text, transform=axes[1, i].transAxes,\n",
        "                       bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
        "                       verticalalignment='top')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "def perform_within_species_correlations(data, continuous_vars):\n",
        "    \"\"\"\n",
        "    Calculate correlations within each species separately.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"WITHIN-SPECIES CORRELATION ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    species_list = data['species'].unique()\n",
        "    within_species_results = {}\n",
        "\n",
        "    print(f\"Analyzing correlations within each species:\")\n",
        "\n",
        "    for species in species_list:\n",
        "        print(f\"\\n{species.upper()} PENGUINS\")\n",
        "        print(\"=\" * (len(species) + 9))\n",
        "\n",
        "        species_data = data[data['species'] == species][continuous_vars].dropna()\n",
        "        n_species = len(species_data)\n",
        "\n",
        "        print(f\"Sample size: {n_species}\")\n",
        "\n",
        "        # Calculate correlation matrix for this species\n",
        "        species_corr = species_data.corr()\n",
        "\n",
        "        print(f\"\\nCorrelation matrix:\")\n",
        "        print(species_corr.round(3))\n",
        "\n",
        "        # Store results\n",
        "        within_species_results[species] = {\n",
        "            'correlation_matrix': species_corr,\n",
        "            'sample_size': n_species,\n",
        "            'data': species_data\n",
        "        }\n",
        "\n",
        "    # Create comparison visualization\n",
        "    fig, axes = plt.subplots(1, len(species_list), figsize=(6*len(species_list), 5))\n",
        "    fig.suptitle('Within-Species Correlation Matrices', fontsize=16)\n",
        "\n",
        "    # Ensure axes is a list for consistent indexing\n",
        "    if len(species_list) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, species in enumerate(species_list):\n",
        "        corr_matrix = within_species_results[species]['correlation_matrix']\n",
        "\n",
        "        # Create mask for upper triangle\n",
        "        #mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "        sns.heatmap(corr_matrix,\n",
        "                    #mask=mask,\n",
        "                    annot=True, cmap='RdBu_r', center=0,\n",
        "                   square=True, fmt='.3f', ax=axes[i],\n",
        "                   cbar_kws={'label': 'Pearson r'})\n",
        "\n",
        "        axes[i].set_title(f'{species}\\n(n = {within_species_results[species][\"sample_size\"]})')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Compare correlations across species\n",
        "    print(f\"\\nComparison across species:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Get all unique variable pairs\n",
        "    var_pairs = []\n",
        "    for i in range(len(continuous_vars)):\n",
        "        for j in range(i+1, len(continuous_vars)):\n",
        "            var_pairs.append((continuous_vars[i], continuous_vars[j]))\n",
        "\n",
        "    comparison_df = pd.DataFrame(index=[f\"{var1} ↔ {var2}\" for var1, var2 in var_pairs],\n",
        "                                columns=species_list)\n",
        "\n",
        "    for var1, var2 in var_pairs:\n",
        "        pair_name = f\"{var1} ↔ {var2}\"\n",
        "        for species in species_list:\n",
        "            corr_val = within_species_results[species]['correlation_matrix'].loc[var1, var2]\n",
        "            comparison_df.loc[pair_name, species] = corr_val\n",
        "\n",
        "    print(comparison_df.round(3))\n",
        "\n",
        "    # Calculate variance in correlations across species\n",
        "    print(f\"\\nVariability in correlations across species:\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    for pair_name in comparison_df.index:\n",
        "        correlations = comparison_df.loc[pair_name].values\n",
        "        var_corr = np.var(correlations)\n",
        "        range_corr = np.max(correlations) - np.min(correlations)\n",
        "\n",
        "        print(f\"{pair_name:<35}: variance = {var_corr:.4f}, range = {range_corr:.3f}\")\n",
        "\n",
        "    return within_species_results, comparison_df\n",
        "\n",
        "\n",
        "# Execute Part 3 Analysis\n",
        "print(\"\\n EXECUTING PARTIAL CORRELATION ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# For demonstration, let's use the penguin data\n",
        "test_data = penguins_clean.head()\n",
        "\n",
        "\n",
        "# Step 2: Analyze species effects on correlations\n",
        "print(\"\\n Step 2: Analyzing species effects on penguin morphology\")\n",
        "species_effects = analyze_species_effects(penguins_clean, continuous_vars)\n",
        "\n",
        "# Step 3: Within-species correlation analysis\n",
        "print(\"\\n Step 3: Within-species correlation analysis\")\n",
        "within_species_results, species_comparison = perform_within_species_correlations(penguins_clean, continuous_vars)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qf17_V1jicB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial Correlation Analysis Results Summary\n",
        "\n",
        "## Key Findings: Species Effects on Penguin Morphology\n",
        "\n",
        "### Three Distinct Patterns Emerged\n",
        "\n",
        "**1. Minimal Species Effect: Bill Length ↔ Body Mass**\n",
        "- Simple correlation: r = 0.589\n",
        "- Partial correlation: r = 0.582 (controlling for species)\n",
        "- **Difference: Δr = 0.007** - Almost no change!\n",
        "- **Interpretation**: This relationship is genuinely within-species. The correlation we see is not just due to species differences.\n",
        "\n",
        "**2. Moderate Species Effect: Bill Length ↔ Flipper Length**\n",
        "- Simple correlation: r = 0.653\n",
        "- Partial correlation: r = 0.488\n",
        "- **Difference: Δr = 0.165** - Noticeable reduction\n",
        "- **Interpretation**: Some of the apparent relationship is due to species differences, but a meaningful within-species relationship remains.\n",
        "\n",
        "**3. Major Species Effect: Two Dramatic Examples**\n",
        "\n",
        "**Flipper Length ↔ Body Mass**:\n",
        "- Simple correlation: r = 0.873 (very strong!)\n",
        "- Partial correlation: r = 0.588 (moderate)\n",
        "- **Difference: Δr = 0.285** - Huge reduction!\n",
        "- **Interpretation**: Much of this \"relationship\" was actually just reflecting that larger species (Gentoo) have both longer flippers AND heavier bodies.\n",
        "\n",
        "**Bill Length ↔ Bill Depth** (Most dramatic):\n",
        "- Simple correlation: r = -0.229 (negative relationship!)\n",
        "- Partial correlation: r = 0.530 (positive relationship!)\n",
        "- **Difference: Δr = -0.759** - Complete reversal!\n",
        "- **Interpretation**: This is a classic \"suppressor variable\" effect. Between species, longer bills tend to be shallower, but within species, longer bills are actually deeper.\n",
        "\n",
        "---\n",
        "\n",
        "## What the Visualizations Show\n",
        "\n",
        "### Figure 1: Theoretical Examples\n",
        "The simulated data demonstrates three scenarios:\n",
        "- **True Confounding**: Simple r = 0.498 → Partial r = -0.020 (relationship disappears)\n",
        "- **Partial Relationship**: Simple r = 0.769 → Partial r = 0.515 (relationship weakens but persists)\n",
        "- **Suppressor Variable**: Simple r = 0.588 → Partial r = 0.805 (relationship strengthens)\n",
        "\n",
        "### Figure 2: Real Penguin Data\n",
        "The upper panels show clear species clustering - you can see three distinct groups (Adelie, Chinstrap, Gentoo) in each scatter plot. The lower panels (residuals) show the within-species patterns after removing species effects. Notice how the yellow Δr values reveal the magnitude of species confounding.\n",
        "\n",
        "### Figure 3: Within-Species Patterns\n",
        "The correlation matrices reveal interesting species differences:\n",
        "- **Gentoo penguins** show the strongest correlations overall (darker red colors)\n",
        "- **Adelie penguins** show weaker correlations (lighter colors)\n",
        "- **Bill length ↔ bill depth** varies dramatically: Adelie (r = 0.386) vs. Chinstrap/Gentoo (r ≈ 0.654)\n",
        "\n",
        "---\n",
        "\n",
        "## Biological Insights\n",
        "\n",
        "### What We Learned About Penguin Morphology\n",
        "\n",
        "**1. Body Size Relationships Are Real**\n",
        "- Bill length and body mass correlate strongly within species (r ≈ 0.58)\n",
        "- This suggests genuine biological scaling - bigger individuals have proportionally longer bills\n",
        "\n",
        "**2. Species Differences Mask True Relationships**\n",
        "- The bill length ↔ bill depth relationship was completely obscured by species effects\n",
        "- Between species: longer bills are shallower (different ecological niches)\n",
        "- Within species: longer bills are deeper (individual variation in bill robustness)\n",
        "\n",
        "**3. Some Relationships Are Largely Between-Species**\n",
        "- Flipper length ↔ body mass drops from r = 0.87 to r = 0.59 when controlling for species\n",
        "- This suggests much of this relationship reflects species differences rather than individual scaling\n",
        "\n",
        "### Statistical Significance\n",
        "All partial correlations were highly significant (p < 0.001), indicating these within-species relationships are robust with our sample size of 333 penguins.\n",
        "\n"
      ],
      "metadata": {
        "id": "8M9njXP5Ykn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Advanced Correlation Analysis\n",
        "\n",
        "\n",
        "Up to this point, we have explored standard correlation analysis using Pearson and Spearman's correlation coefficients to measure relationships between variables. We also examined the effect of third variables on correlations using partial correlation analysis to control for confounding factors like species.\n",
        "\n",
        "Now, let us dive into **advanced correlation techniques** that provide deeper statistical insights. While we're still using Pearson correlations as the core calculation, this section transforms correlation from **exploratory data analysis** into **rigorous statistical inference**.\n",
        "\n",
        "## What's NEW and Advanced:\n",
        "\n",
        "### 1. **Statistical Rigor** (Building on basic correlations)\n",
        "- **Before**: Just got correlation coefficient (r = 0.65)\n",
        "- **Now**: Get r = 0.65 with confidence interval [0.58, 0.71] and proper significance testing\n",
        "\n",
        "### 2. **Multiple Testing Problem** (New concern)\n",
        "- **Before**: Tested correlations one by one\n",
        "- **Now**: When testing 6 correlations simultaneously, need to correct p-values to avoid false discoveries\n",
        "\n",
        "### 3. **Method Validation** (New reliability check)\n",
        "- **Before**: Assumed our correlation estimate was accurate\n",
        "- **Now**: Bootstrap resampling shows \"How stable is this correlation if we repeated the study?\"\n",
        "\n",
        "### 4. **Advanced Exploration** (New pattern discovery)\n",
        "- **Before**: Looked at correlations individually  \n",
        "- **Now**: Cluster analysis shows \"Which variables behave similarly?\" and reveals hidden structure\n",
        "\n",
        "### 5. **Study Design Insights** (New planning tool)\n",
        "- **Before**: Used whatever sample size we had\n",
        "- **Now**: Power analysis tells us \"Was our sample size adequate?\" and \"What effect sizes could we actually detect?\"\n",
        "\n",
        "### 6. **Professional Standards** (New reporting quality)\n",
        "- **Before**: Informal reporting of results\n",
        "- **Now**: Academic-quality reporting with proper formatting and statistical disclosure\n",
        "\n",
        "The key difference: This section provides **uncertainty quantification, validation, and professional reporting standards** - like the difference between \"eyeballing\" a trend versus providing statistical evidence with confidence bounds!\n",
        "\n",
        "---\n",
        "\n",
        "## Helper Functions: Getting Reliable Correlation Estimates\n",
        "\n",
        "In the following code, we add statistical reliability to our correlation calculations using some behind-the-scenes mathematical transformations.\n",
        "\n",
        "### What These Functions Do:\n",
        "- **Before**: We got a correlation like r = 0.65 but didn't know how reliable it was\n",
        "- **Now**: We can say \"we're 95% confident the true correlation is between 0.58 and 0.71\"\n",
        "- **How**: Uses Fisher's Z transformation to calculate proper confidence intervals\n",
        "- **Why needed**: Raw correlations don't behave well statistically, but transformed values do\n",
        "\n",
        "## Main Analysis: `comprehensive_correlation_matrix_analysis()`\n",
        "\n",
        "In the following code, we upgrade our basic correlation matrix to include confidence intervals and handle the \"multiple testing problem.\"\n",
        "\n",
        "### What's the Multiple Testing Problem?\n",
        "- **Before**: Tested 6 correlations separately, each with 5% chance of being wrong\n",
        "- **Problem**: With 6 tests, we have ~26% chance of at least one false positive!\n",
        "- **Now**: Apply corrections to keep our overall error rate at 5%\n",
        "- **Note**: How in world did we get ~26%? Well, given α and n, the probability of at least one false positive is: 1−(1−α)^n. So for α=0.05 and n=6, we get: 1-(1-0.05)^6 = 26.5%\n",
        "\n",
        "\n",
        "### Four Different Correction Methods:\n",
        "- **Bonferroni**: Very strict, rarely makes mistakes but might miss real relationships\n",
        "- **Holm**: Slightly less strict than Bonferroni\n",
        "- **FDR (Benjamini-Hochberg)**: Balanced approach - good middle ground\n",
        "- **FDR (Benjamini-Yekutieli)**: More conservative version of FDR\n",
        "\n",
        "### What We Get:\n",
        "- **Before**: \"Bill length and body mass are correlated, r = 0.59\"\n",
        "- **Now**: \"Bill length and body mass are correlated, r = 0.59 [0.51, 0.66], and this remains significant even after correcting for multiple testing\"\n",
        "\n",
        "## Visualization: `create_advanced_correlation_visualizations()`\n",
        "\n",
        "In the following code, we create six plots that tell different stories about our correlations.\n",
        "\n",
        "### Six Ways to View Our Results:\n",
        "1. **Enhanced Heatmap**: Same as before, but now with stars showing significance levels\n",
        "2. **Confidence Intervals**: Shows the \"uncertainty bars\" around each correlation estimate\n",
        "3. **P-value Distribution**: Reveals if we have a multiple testing problem (too many small p-values)\n",
        "4. **CI Width vs Sample Size**: Demonstrates \"bigger samples give more precise estimates\"\n",
        "5. **Multiple Comparisons Impact**: Shows how many correlations survive each correction method\n",
        "6. **Correlation Strength Distribution**: Categorizes our correlations from weak to strong\n",
        "\n",
        "### Why Six Different Views?\n",
        "Each plot answers a different question: \"How precise are our estimates?\" \"Are we making too many comparisons?\" \"How strong are these relationships really?\"\n",
        "\n",
        "## Clustering Analysis: `hierarchical_clustering_correlations()`\n",
        "\n",
        "In the following code, we discover which variables \"hang out together\" by clustering based on correlation patterns.\n",
        "\n",
        "### What This Reveals:\n",
        "- **Before**: Looked at correlations one pair at a time\n",
        "- **Now**: \"Bill length and bill depth cluster together, suggesting they measure similar aspects of penguin size\"\n",
        "- **Practical insight**: Variables that cluster together might be measuring the same underlying trait\n",
        "\n",
        "### How It Works:\n",
        "1. **Convert similarities to distances**: Highly correlated variables are \"close,\" uncorrelated variables are \"far\"\n",
        "2. **Group similar variables**: Creates a family tree showing which variables are most related\n",
        "3. **Reorder our correlation matrix**: Puts similar variables next to each other for easier interpretation\n",
        "\n",
        "## Bootstrap Analysis: `bootstrap_correlation_confidence_intervals()`\n",
        "\n",
        "In the following code, we ask \"What if we repeated this study 1000 times?\" to test the stability of our correlations.\n",
        "\n",
        "### The Bootstrap Question:\n",
        "- **Before**: \"We found r = 0.65, but is this reliable?\"\n",
        "- **Now**: \"If we collected 1000 different samples of penguins, 95% of the time we'd get correlations between 0.58 and 0.71\"\n",
        "\n",
        "### How Bootstrap Works:\n",
        "1. **Simulate new studies**: Randomly resample our data 1000 times (with replacement)\n",
        "2. **Calculate correlations**: Get 1000 different correlation estimates\n",
        "3. **Build confidence intervals**: See the range of results we'd expect from repeated studies\n",
        "\n",
        "### Two Methods Compared:\n",
        "- **Fisher's Z method**: Based on mathematical theory\n",
        "- **Bootstrap method**: Based on our actual data\n",
        "- **When they agree**: Our correlation estimate is very reliable\n",
        "- **When they disagree**: Might indicate data issues or assumption violations\n",
        "\n",
        "## Power Analysis: `power_analysis_correlation()`\n",
        "\n",
        "In the following code, we answer study design questions like \"Was our sample big enough?\" and \"What relationships could we actually detect?\"\n",
        "\n",
        "### Four Key Questions Answered:\n",
        "1. **\"How does sample size affect our ability to detect relationships?\"**\n",
        "   - Shows that small studies can only find very strong correlations\n",
        "   \n",
        "2. **\"How many penguins do we need to find a medium-sized correlation?\"**\n",
        "   - Provides concrete sample size recommendations for future studies\n",
        "   \n",
        "3. **\"Given our sample size, what's the weakest correlation we could reliably detect?\"**\n",
        "   - Reveals the limitations of our current study\n",
        "   \n",
        "4. **\"Are we underpowered or overpowered for our research questions?\"**\n",
        "   - Helps interpret negative results (absence of correlation)\n",
        "\n",
        "### Practical Applications:\n",
        "- **Study planning**: \"We need 85 penguins to detect a medium correlation with 80% confidence\"\n",
        "- **Result interpretation**: \"We had 333 penguins, so we could detect even small correlations reliably\"\n",
        "\n",
        "## Reporting Guide: `practical_correlation_reporting_guide()`\n",
        "\n",
        "In the following code, we learn how to communicate our results professionally and clearly.\n",
        "\n",
        "### What Good Reporting Includes:\n",
        "- **Before**: \"Bill length and body mass are correlated\"\n",
        "- **Now**: \"There was a large positive correlation between bill length and body mass, r(331) = 0.59, 95% CI [0.51, 0.66], p < .001\"\n",
        "\n",
        "### Why This Matters:\n",
        "- **Reproducibility**: Other researchers can understand exactly what we did\n",
        "- **Transparency**: Readers can judge the strength and reliability of our evidence\n",
        "- **Professional standards**: Meets journal and academic requirements\n",
        "\n",
        "### Different Formats Provided:\n",
        "- **Standard Format**: Complete statistical reporting with all details\n",
        "- **APA Format**: Follows psychology/social science journal standards\n",
        "- **Summary Table**: Organized overview of all results\n",
        "- **Multiple Comparisons Disclosure**: Shows impact of statistical corrections\n",
        "\n",
        "## Overall Workflow: From Basic to Bulletproof\n",
        "\n",
        "This advanced section transforms basic correlations into publication-ready statistical analysis:\n",
        "1. **Calculate** reliable estimates with confidence intervals\n",
        "2. **Visualize** results to reveal patterns and check assumptions\n",
        "3. **Cluster** variables to understand underlying structure\n",
        "4. **Bootstrap** to validate our findings\n",
        "5. **Analyze power** to understand study limitations and strengths\n",
        "6. **Report** results with professional statistical standards\n",
        "\n",
        "The goal: Move from \"we found some correlations\" to \"we have statistically robust evidence for these relationships, with known precision and reliability.\""
      ],
      "metadata": {
        "id": "tpG4d0ZFGwMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating confidence intervals to pearson r using Fisher's Z-transform\n",
        "# 1. Convert the correlation r to Fisher’s Z-space where it’s approximately normally distributed.\n",
        "# once normally distributed, we can use standard techniques for confidence intervals.\n",
        "# 2. Calculate the standard error (SE) in Z-space using the sample size n.\n",
        "# 3. Build a confidence interval in Z-space using a critical value from the normal distribution.\n",
        "# 4. Convert the interval back to the correlation scale using the inverse Fisher transformation.\n",
        "\n",
        "def fishers_z_transform(r):\n",
        "    \"\"\"\n",
        "    Apply Fisher's Z transformation to correlation coefficient.\n",
        "    Formula: Z = 0.5 × ln((1 + r) / (1 - r))\n",
        "\n",
        "    \"\"\"\n",
        "    # Handle edge cases where r is exactly ±1\n",
        "    r = np.clip(r, -0.9999, 0.9999)\n",
        "    return 0.5 * np.log((1 + r) / (1 - r))\n",
        "\n",
        "def inverse_fishers_z(z):\n",
        "    \"\"\"\n",
        "    Convert Fisher's Z back to correlation coefficient.\n",
        "\n",
        "    \"\"\"\n",
        "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
        "\n",
        "def correlation_confidence_interval(r, n, confidence_level=0.95):\n",
        "    \"\"\"\n",
        "    Calculate confidence interval for a correlation coefficient using Fisher's Z transformation.\n",
        "\n",
        "    \"\"\"\n",
        "    # Fisher's Z transformation\n",
        "    z = fishers_z_transform(r)\n",
        "\n",
        "    # Standard error of Z (1 over sq. root n-3)\n",
        "    se_z = 1 / np.sqrt(n - 3)\n",
        "\n",
        "    # Critical value for confidence interval\n",
        "    alpha = 1 - confidence_level\n",
        "    z_critical = stats.norm.ppf(1 - alpha/2)\n",
        "\n",
        "    # Confidence interval for Z\n",
        "    z_lower = z - z_critical * se_z\n",
        "    z_upper = z + z_critical * se_z\n",
        "\n",
        "    # Transform back to correlation scale\n",
        "    r_lower = inverse_fishers_z(z_lower)\n",
        "    r_upper = inverse_fishers_z(z_upper)\n",
        "\n",
        "    return r_lower, r_upper, z, se_z\n",
        "\n",
        "\n",
        "#Correlation matrix analysis\n",
        "# 1. Calculate Pairwise Pearson Correlations: For each pair of continuous variables,\n",
        "# compute the correlation coefficient and its p-value, along with the sample size used.\n",
        "\n",
        "# 2. Build Confidence Intervals for Each Correlation: Using Fisher's Z transformation,\n",
        "# estimate confidence intervals (default 95%) for the correlation values, helping you gauge the uncertainty.\n",
        "# It's like calculating 3 correlation matrices, one we had before and two more - lower and upper uncertainties\n",
        "\n",
        "# 3. Store and Display Results: Organize the raw correlations, p-values, CI bounds,\n",
        "# and sample sizes into matrices and also prints detailed stats for each unique pair.\n",
        "\n",
        "# 4. Apply Multiple Comparisons Correction: To control the false discovery rate across\n",
        "# all pairwise tests, adjust p-values using methods like Bonferroni, Holm, and\n",
        "# FDR (Benjamini-Hochberg/Yekutieli), and report how many remain statistically significant.\n",
        "\n",
        "def comprehensive_correlation_matrix_analysis(data, continuous_vars, confidence_level=0.95):\n",
        "    \"\"\"\n",
        "    Perform comprehensive correlation matrix analysis with confidence intervals and significance testing.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"COMPREHENSIVE CORRELATION MATRIX ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Calculate correlation matrix with all pairwise statistics\n",
        "    n_vars = len(continuous_vars)\n",
        "    results_matrix = {}\n",
        "\n",
        "    # Initialize result storage\n",
        "    correlations = np.zeros((n_vars, n_vars))\n",
        "    p_values = np.zeros((n_vars, n_vars))\n",
        "    lower_bounds = np.zeros((n_vars, n_vars))\n",
        "    upper_bounds = np.zeros((n_vars, n_vars))\n",
        "    sample_sizes = np.zeros((n_vars, n_vars))\n",
        "\n",
        "    print(f\"Calculating correlations with {confidence_level*100:.0f}% confidence intervals:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    detailed_results = []\n",
        "\n",
        "    for i, var1 in enumerate(continuous_vars):\n",
        "        for j, var2 in enumerate(continuous_vars):\n",
        "            if i == j:\n",
        "                # Diagonal elements\n",
        "                correlations[i, j] = 1.0\n",
        "                p_values[i, j] = 0.0\n",
        "                lower_bounds[i, j] = 1.0\n",
        "                upper_bounds[i, j] = 1.0\n",
        "                sample_sizes[i, j] = len(data[var1].dropna())\n",
        "            else:\n",
        "                # Off-diagonal elements\n",
        "                clean_data = data[[var1, var2]].dropna()\n",
        "                x = clean_data[var1]\n",
        "                y = clean_data[var2]\n",
        "                n = len(clean_data)\n",
        "\n",
        "                # Calculate correlation and significance\n",
        "                r, p = pearsonr(x, y)\n",
        "\n",
        "                # Calculate confidence interval\n",
        "                r_lower, r_upper, fisher_z, se_z = correlation_confidence_interval(r, n, confidence_level)\n",
        "\n",
        "                # Store results\n",
        "                correlations[i, j] = r\n",
        "                p_values[i, j] = p\n",
        "                lower_bounds[i, j] = r_lower\n",
        "                upper_bounds[i, j] = r_upper\n",
        "                sample_sizes[i, j] = n\n",
        "\n",
        "                # Store detailed results for unique pairs\n",
        "                if i < j:\n",
        "                    detailed_results.append({\n",
        "                        'var1': var1,\n",
        "                        'var2': var2,\n",
        "                        'correlation': r,\n",
        "                        'p_value': p,\n",
        "                        'ci_lower': r_lower,\n",
        "                        'ci_upper': r_upper,\n",
        "                        'fisher_z': fisher_z,\n",
        "                        'se_z': se_z,\n",
        "                        'sample_size': n,\n",
        "                        'ci_width': r_upper - r_lower\n",
        "                    })\n",
        "\n",
        "                    print(f\"{var1} ↔ {var2}:\")\n",
        "                    print(f\"  r = {r:>7.3f} [{r_lower:>6.3f}, {r_upper:>6.3f}], p = {p:>7.4f}, n = {n:>3}\")\n",
        "\n",
        "    # Create DataFrames for easy manipulation\n",
        "    corr_df = pd.DataFrame(correlations, index=continuous_vars, columns=continuous_vars)\n",
        "    p_values_df = pd.DataFrame(p_values, index=continuous_vars, columns=continuous_vars)\n",
        "\n",
        "    print(f\"\\nCorrelation Matrix:\")\n",
        "    print(\"-\" * 20)\n",
        "    print(corr_df.round(3))\n",
        "\n",
        "    # Multiple comparisons correction\n",
        "    print(f\"\\nMultiple Comparisons Correction:\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    # Extract p-values for unique pairs (upper triangle)\n",
        "    unique_p_values = []\n",
        "    for result in detailed_results:\n",
        "        unique_p_values.append(result['p_value'])\n",
        "\n",
        "    # Apply different correction methods\n",
        "    # We use: statsmodels.stats.multitest.multipletests where these can be inputs\n",
        "    # Bonferroni correction works by dividing your desired significance level (like 0.05)\n",
        "    # by the number of comparisons you're making. So if you're testing 6 pairs, the new threshold\n",
        "    # becomes 0.05 ÷ 6 = 0.0083 — meaning each individual p-value must be less than 0.0083 to be considered significant.\n",
        "    corrections = {\n",
        "        'Bonferroni': 'bonferroni',\n",
        "        'Holm': 'holm',\n",
        "        'FDR (Benjamini-Hochberg)': 'fdr_bh',\n",
        "        'FDR (Benjamini-Yekutieli)': 'fdr_by'\n",
        "    }\n",
        "\n",
        "    for method_name, method_code in corrections.items():\n",
        "        rejected, p_adjusted, alpha_sidak, alpha_bonf = multipletests(unique_p_values, method=method_code)\n",
        "\n",
        "        # Update detailed results\n",
        "        for i, result in enumerate(detailed_results):\n",
        "            result[f'p_adjusted_{method_code}'] = p_adjusted[i]\n",
        "            result[f'significant_{method_code}'] = rejected[i]\n",
        "\n",
        "        significant_count = sum(rejected)\n",
        "        print(f\"{method_name:<25}: {significant_count:>2} significant out of {len(unique_p_values)}\")\n",
        "\n",
        "    return {\n",
        "        'correlation_matrix': corr_df,\n",
        "        'p_values_matrix': p_values_df,\n",
        "        'lower_bounds': lower_bounds,\n",
        "        'upper_bounds': upper_bounds,\n",
        "        'detailed_results': detailed_results,\n",
        "        'sample_sizes': sample_sizes\n",
        "    }\n",
        "\n",
        "def create_advanced_correlation_visualizations(correlation_analysis, continuous_vars):\n",
        "    \"\"\"\n",
        "    Create advanced visualizations for correlation analysis.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"ADVANCED CORRELATION VISUALIZATIONS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    corr_matrix = correlation_analysis['correlation_matrix']\n",
        "    p_values_matrix = correlation_analysis['p_values_matrix']\n",
        "    detailed_results = correlation_analysis['detailed_results']\n",
        "\n",
        "    # Create comprehensive visualization\n",
        "    fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "    # 1. Enhanced correlation heatmap with significance stars\n",
        "    ax1 = plt.subplot(2, 3, 1)\n",
        "\n",
        "    # Create annotation matrix with significance stars\n",
        "    annotations = corr_matrix.round(3).astype(str)\n",
        "\n",
        "    for i in range(len(continuous_vars)):\n",
        "        for j in range(len(continuous_vars)):\n",
        "            if i != j:\n",
        "                p_val = p_values_matrix.iloc[i, j]\n",
        "                corr_val = corr_matrix.iloc[i, j]\n",
        "\n",
        "                # Add significance stars\n",
        "                if p_val < 0.001:\n",
        "                    star = \"***\"\n",
        "                elif p_val < 0.01:\n",
        "                    star = \"**\"\n",
        "                elif p_val < 0.05:\n",
        "                    star = \"*\"\n",
        "                else:\n",
        "                    star = \"\"\n",
        "\n",
        "                annotations.iloc[i, j] = f\"{corr_val:.3f}{star}\"\n",
        "\n",
        "    # Create mask for upper triangle\n",
        "    # mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "    sns.heatmap(corr_matrix,\n",
        "                #mask=mask,\n",
        "                annot=annotations, fmt='', cmap='RdBu_r', center=0,\n",
        "                square=True, ax=ax1, cbar_kws={'label': 'Pearson r'})\n",
        "    ax1.set_title('Correlation Matrix with Significance\\n(* p<0.05, ** p<0.01, *** p<0.001)')\n",
        "\n",
        "    # 2. Confidence interval visualization\n",
        "    ax2 = plt.subplot(2, 3, 2)\n",
        "\n",
        "    # Extract data for plotting\n",
        "    correlations = [result['correlation'] for result in detailed_results]\n",
        "    ci_lowers = [result['ci_lower'] for result in detailed_results]\n",
        "    ci_uppers = [result['ci_upper'] for result in detailed_results]\n",
        "    pair_names = [f\"{result['var1'][:8]}↔{result['var2'][:8]}\" for result in detailed_results]\n",
        "\n",
        "    y_pos = np.arange(len(pair_names))\n",
        "\n",
        "    # Create horizontal error bars\n",
        "    ax2.errorbar(correlations, y_pos, xerr=[np.array(correlations) - np.array(ci_lowers),\n",
        "                                           np.array(ci_uppers) - np.array(correlations)],\n",
        "                fmt='o', capsize=5, capthick=2, markersize=8)\n",
        "\n",
        "    ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "    ax2.set_yticks(y_pos)\n",
        "    ax2.set_yticklabels(pair_names)\n",
        "    ax2.set_xlabel('Correlation Coefficient')\n",
        "    ax2.set_title('Correlations with 95% Confidence Intervals')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. P-value distribution\n",
        "    ax3 = plt.subplot(2, 3, 3)\n",
        "\n",
        "    p_vals = [result['p_value'] for result in detailed_results]\n",
        "    ax3.hist(p_vals, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    ax3.axvline(x=0.05, color='red', linestyle='--', label='α = 0.05')\n",
        "    ax3.set_xlabel('P-values')\n",
        "    ax3.set_ylabel('Frequency')\n",
        "    ax3.set_title('Distribution of P-values')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Confidence interval widths\n",
        "    ax4 = plt.subplot(2, 3, 4)\n",
        "\n",
        "    ci_widths = [result['ci_upper'] - result['ci_lower'] for result in detailed_results]\n",
        "    sample_sizes = [result['sample_size'] for result in detailed_results]\n",
        "\n",
        "    ax4.scatter(sample_sizes, ci_widths, alpha=0.7, s=60)\n",
        "    ax4.set_xlabel('Sample Size')\n",
        "    ax4.set_ylabel('95% CI Width')\n",
        "    ax4.set_title('CI Width vs Sample Size')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add trendline\n",
        "    z = np.polyfit(sample_sizes, ci_widths, 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax4.plot(sample_sizes, p(sample_sizes), \"r--\", alpha=0.8)\n",
        "\n",
        "    # 5. Multiple comparisons impact\n",
        "    ax5 = plt.subplot(2, 3, 5)\n",
        "\n",
        "    methods = ['Uncorrected', 'Bonferroni', 'Holm', 'FDR (BH)']\n",
        "    significant_counts = []\n",
        "\n",
        "    # Uncorrected\n",
        "    uncorrected_sig = sum(1 for result in detailed_results if result['p_value'] < 0.05)\n",
        "    significant_counts.append(uncorrected_sig)\n",
        "\n",
        "    # Corrected methods\n",
        "    for method in ['bonferroni', 'holm', 'fdr_bh']:\n",
        "        method_sig = sum(1 for result in detailed_results if result[f'significant_{method}'])\n",
        "        significant_counts.append(method_sig)\n",
        "\n",
        "    bars = ax5.bar(methods, significant_counts, alpha=0.7,\n",
        "                   color=['red', 'orange', 'yellow', 'lightgreen'])\n",
        "    ax5.set_ylabel('Number of Significant Correlations')\n",
        "    ax5.set_title('Impact of Multiple Comparisons Correction')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, significant_counts):\n",
        "        height = bar.get_height()\n",
        "        ax5.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "                f'{count}', ha='center', va='bottom')\n",
        "\n",
        "    # 6. Correlation strength distribution\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "\n",
        "    abs_correlations = [abs(result['correlation']) for result in detailed_results]\n",
        "\n",
        "    # Create bins for correlation strength\n",
        "    bins = [0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
        "    labels = ['Negligible\\n(0-0.1)', 'Small\\n(0.1-0.3)', 'Medium\\n(0.3-0.5)',\n",
        "              'Large\\n(0.5-0.7)', 'Very Large\\n(0.7-0.9)', 'Nearly Perfect\\n(0.9-1.0)']\n",
        "\n",
        "    counts, _ = np.histogram(abs_correlations, bins=bins)\n",
        "\n",
        "    bars = ax6.bar(range(len(counts)), counts, alpha=0.7,\n",
        "                   color=['lightblue', 'lightgreen', 'yellow', 'orange', 'red', 'darkred'])\n",
        "    ax6.set_xticks(range(len(counts)))\n",
        "    ax6.set_xticklabels(labels, rotation=45, ha='right')\n",
        "    ax6.set_ylabel('Number of Correlations')\n",
        "    ax6.set_title('Distribution of Correlation Strengths')\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add percentage labels\n",
        "    total_corrs = len(abs_correlations)\n",
        "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
        "        if count > 0:\n",
        "            percentage = (count / total_corrs) * 100\n",
        "            ax6.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
        "                    f'{percentage:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Hierarchical Clustering of Correlation Matrix\n",
        "# 1. Convert Correlations to Distances: Transform the correlation matrix into a distance matrix using the formula:\n",
        "# Distance = 1 −∣correlation∣ - This way, variables that are highly correlated (positive or negative) are considered “close.”\n",
        "\n",
        "# 2. Perform Hierarchical Clustering: Apply hierarchical clustering (Ward's method) to group similar variables based on their\n",
        "# pairwise distances — identifying natural clusters of correlated variables.\n",
        "\n",
        "# 3. Visualize the Results: Plot a dendrogram showing how variables cluster hierarchically.\n",
        "# Reorder and plot the correlation matrix based on clustering so related variables are grouped visually in blocks.\n",
        "\n",
        "# 4. Print Cluster Groupings at Multiple Levels: Show which variables fall into 2, 3, or 4 clusters,\n",
        "# helping us interpret the structure at different granularity levels.\n",
        "\n",
        "def hierarchical_clustering_correlations(correlation_matrix, continuous_vars):\n",
        "    \"\"\"\n",
        "    Perform hierarchical clustering on correlation matrix to identify variable groups.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"HIERARCHICAL CLUSTERING OF CORRELATION MATRIX\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Convert correlation matrix to distance matrix\n",
        "    # Distance = 1 - |correlation|\n",
        "    distance_matrix = 1 - np.abs(correlation_matrix.values)\n",
        "\n",
        "    # Ensure distance matrix is symmetric and has zero diagonal (distance to itself is obviously zero)\n",
        "    np.fill_diagonal(distance_matrix, 0)\n",
        "    distance_matrix = (distance_matrix + distance_matrix.T) / 2\n",
        "\n",
        "    # Convert to condensed form for scipy\n",
        "    condensed_distances = squareform(distance_matrix) # 2D to 1D. Linkage function expects distances in 1D format—not a full 2D matrix\n",
        "\n",
        "    # Perform hierarchical clustering\n",
        "    linkage_matrix = linkage(condensed_distances, method='ward') # Performs hierarchical agglomerative clustering using the Ward method.\n",
        "\n",
        "    # Create dendrogram\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Plot dendrogram\n",
        "    dendrogram_result = dendrogram(linkage_matrix, labels=continuous_vars,\n",
        "                                  orientation='top', ax=ax1)\n",
        "    ax1.set_title('Hierarchical Clustering of Variables\\n(Based on Correlation Distance)')\n",
        "    ax1.set_ylabel('Distance')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Reorder correlation matrix based on clustering\n",
        "    cluster_order = dendrogram_result['leaves']\n",
        "    reordered_vars = [continuous_vars[i] for i in cluster_order]\n",
        "    reordered_corr = correlation_matrix.loc[reordered_vars, reordered_vars]\n",
        "\n",
        "    # Plot reordered correlation matrix\n",
        "    # mask = np.triu(np.ones_like(reordered_corr, dtype=bool))\n",
        "    sns.heatmap(reordered_corr,\n",
        "                #mask=mask,\n",
        "                annot=True, cmap='RdBu_r', center=0,\n",
        "                square=True, fmt='.3f', ax=ax2,\n",
        "                cbar_kws={'label': 'Pearson r'})\n",
        "    ax2.set_title('Reordered Correlation Matrix\\n(Clustered Variables)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Extract clusters at different levels\n",
        "    print(f\"Variable clustering at different cut levels:\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    for n_clusters in [2, 3, 4]:\n",
        "        cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
        "\n",
        "        print(f\"\\n{n_clusters} clusters:\")\n",
        "        for cluster_id in range(1, n_clusters + 1):\n",
        "            cluster_vars = [continuous_vars[i] for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
        "            print(f\"  Cluster {cluster_id}: {cluster_vars}\")\n",
        "\n",
        "    return {\n",
        "        'linkage_matrix': linkage_matrix,\n",
        "        'distance_matrix': distance_matrix,\n",
        "        'cluster_order': cluster_order,\n",
        "        'reordered_correlation_matrix': reordered_corr\n",
        "    }\n",
        "\n",
        "\n",
        "# Bootstrap Confidence Intervals for Correlation\n",
        "# 1. Calculate the Original Correlation: Compute the Pearson correlation and p-value\n",
        "# between two variables using the original dataset (after dropping missing values).\n",
        "\n",
        "# 2. Generate a Bootstrap Distribution: Repeatedly resample the data with replacement\n",
        "# to create many new datasets, compute correlation for each, and build a distribution of correlation values.\n",
        "\n",
        "# 3. Compute Bootstrap Confidence Intervals: Use the percentile method (e.g., 2.5th and 97.5th percentiles for 95% CI)\n",
        "# to estimate a confidence interval from the bootstrap distribution.\n",
        "# Also compare this to the Fisher’s Z-based confidence interval.\n",
        "\n",
        "# 4. Visualize and Report Key Statistics: Plot the bootstrap distribution, highlight key values like the original correlation,\n",
        "# bootstrap mean, and confidence bounds, and shows a Q-Q plot to assess normality of the bootstrap distribution.\n",
        "# Also report bias and standard deviation of the bootstrap estimate.\n",
        "\n",
        "def bootstrap_correlation_confidence_intervals(data, var1, var2, n_bootstrap=1000, confidence_level=0.95):\n",
        "    \"\"\"\n",
        "    Calculate bootstrap confidence intervals for correlation coefficient.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 60)\n",
        "    print(f\"BOOTSTRAP CONFIDENCE INTERVALS: {var1} vs {var2}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Get clean data\n",
        "    clean_data = data[[var1, var2]].dropna()\n",
        "    x = clean_data[var1].values\n",
        "    y = clean_data[var2].values\n",
        "    n = len(clean_data)\n",
        "\n",
        "    print(f\"Original sample size: {n}\")\n",
        "    print(f\"Bootstrap samples: {n_bootstrap}\")\n",
        "    print(f\"Confidence level: {confidence_level*100:.0f}%\")\n",
        "\n",
        "    # Calculate original correlation\n",
        "    original_r, original_p = pearsonr(x, y)\n",
        "\n",
        "    # Bootstrap sampling\n",
        "    bootstrap_correlations = []\n",
        "\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "\n",
        "    for i in range(n_bootstrap):\n",
        "        # Resample with replacement\n",
        "        indices = np.random.choice(n, size=n, replace=True)\n",
        "        x_boot = x[indices]\n",
        "        y_boot = y[indices]\n",
        "\n",
        "        # Calculate correlation for bootstrap sample\n",
        "        r_boot, _ = pearsonr(x_boot, y_boot)\n",
        "        bootstrap_correlations.append(r_boot)\n",
        "\n",
        "    bootstrap_correlations = np.array(bootstrap_correlations)\n",
        "\n",
        "    # Calculate confidence intervals using percentile method\n",
        "    alpha = 1 - confidence_level\n",
        "    lower_percentile = (alpha / 2) * 100\n",
        "    upper_percentile = (1 - alpha / 2) * 100\n",
        "\n",
        "    ci_lower_boot = np.percentile(bootstrap_correlations, lower_percentile)\n",
        "    ci_upper_boot = np.percentile(bootstrap_correlations, upper_percentile)\n",
        "\n",
        "    # Calculate Fisher's Z confidence interval for comparison\n",
        "    ci_lower_fisher, ci_upper_fisher, _, _ = correlation_confidence_interval(original_r, n, confidence_level)\n",
        "\n",
        "    # Statistics\n",
        "    boot_mean = np.mean(bootstrap_correlations)\n",
        "    boot_std = np.std(bootstrap_correlations)\n",
        "    boot_bias = boot_mean - original_r\n",
        "\n",
        "    print(f\"\\nResults:\")\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"Original correlation: {original_r:.4f}\")\n",
        "    print(f\"Bootstrap mean: {boot_mean:.4f}\")\n",
        "    print(f\"Bootstrap bias: {boot_bias:.4f}\")\n",
        "    print(f\"Bootstrap std: {boot_std:.4f}\")\n",
        "\n",
        "    print(f\"\\nConfidence Intervals:\")\n",
        "    print(\"-\" * 22)\n",
        "    print(f\"Bootstrap CI:   [{ci_lower_boot:.4f}, {ci_upper_boot:.4f}]\")\n",
        "    print(f\"Fisher's Z CI:  [{ci_lower_fisher:.4f}, {ci_upper_fisher:.4f}]\")\n",
        "\n",
        "    # Visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Plot 1: Bootstrap distribution\n",
        "    ax1.hist(bootstrap_correlations, bins=50, alpha=0.7, density=True,\n",
        "            color='skyblue', edgecolor='black')\n",
        "\n",
        "    # Add lines for original correlation and confidence interval\n",
        "    ax1.axvline(original_r, color='red', linestyle='-', linewidth=2, label=f'Original r = {original_r:.3f}')\n",
        "    ax1.axvline(ci_lower_boot, color='orange', linestyle='--', linewidth=2)\n",
        "    ax1.axvline(ci_upper_boot, color='orange', linestyle='--', linewidth=2,\n",
        "               label=f'Bootstrap 95% CI')\n",
        "    ax1.axvline(boot_mean, color='green', linestyle=':', linewidth=2, label=f'Bootstrap mean = {boot_mean:.3f}')\n",
        "\n",
        "    ax1.set_xlabel('Correlation Coefficient')\n",
        "    ax1.set_ylabel('Density')\n",
        "    ax1.set_title(f'Bootstrap Distribution of Correlations\\n{var1} vs {var2}')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Q-Q plot to check normality of bootstrap distribution\n",
        "    stats.probplot(bootstrap_correlations, dist=\"norm\", plot=ax2)\n",
        "    ax2.set_title('Q-Q Plot: Bootstrap Distribution Normality')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'original_correlation': original_r,\n",
        "        'original_p_value': original_p,\n",
        "        'bootstrap_correlations': bootstrap_correlations,\n",
        "        'bootstrap_mean': boot_mean,\n",
        "        'bootstrap_std': boot_std,\n",
        "        'bootstrap_bias': boot_bias,\n",
        "        'ci_lower_bootstrap': ci_lower_boot,\n",
        "        'ci_upper_bootstrap': ci_upper_boot,\n",
        "        'ci_lower_fisher': ci_lower_fisher,\n",
        "        'ci_upper_fisher': ci_upper_fisher\n",
        "    }\n",
        "\n",
        "# Power Analysis for Correlation Studies\n",
        "\n",
        "# 1. Estimate Required Sample Sizes for Desired Power: For a given target power (e.g. 80%), calculate how many samples\n",
        "# we need to detect various correlation strengths (e.g., small = 0.1, medium = 0.3, large = 0.5) using Fisher’s Z transformation.\n",
        "\n",
        "# 2. Compute Statistical Power Across Sample Sizes and Effect Sizes: Calculate and plot how power increases as\n",
        "# sample size increases for fixed correlation levels — helping us see when our study becomes adequately powered.\n",
        "\n",
        "# 3. Visualize Critical Trade-offs\n",
        "# Generate four key plots:\n",
        "#   i. Power vs Sample Size for various effect sizes\n",
        "#   ii. Sample Size vs Effect Size for achieving target power\n",
        "#   iii. Power vs Effect Size for different sample sizes\n",
        "#   iv. Minimum Detectable Correlation for a range of sample sizes at fixed power\n",
        "\n",
        "# 4. Highlight Cohen’s Guidelines: Overlay visual benchmarks (small, medium, large effects) to help us interpret results\n",
        "# using widely accepted effect size categories.\n",
        "\n",
        "\n",
        "def power_analysis_correlation(effect_sizes=None, sample_sizes=None, alpha=0.05, power=0.8):\n",
        "    \"\"\"\n",
        "    Perform power analysis for correlation studies.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"POWER ANALYSIS FOR CORRELATION STUDIES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if effect_sizes is None:\n",
        "        effect_sizes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "    if sample_sizes is None:\n",
        "        sample_sizes = list(range(10, 201, 10))\n",
        "\n",
        "    # Cohen's guidelines for effect sizes\n",
        "    print(f\"Cohen's guidelines for correlation effect sizes:\")\n",
        "    print(f\"Small effect:    |r| = 0.10\")\n",
        "    print(f\"Medium effect:   |r| = 0.30\")\n",
        "    print(f\"Large effect:    |r| = 0.50\")\n",
        "\n",
        "    print(f\"\\nPower analysis parameters:\")\n",
        "    print(f\"Alpha level: {alpha}\")\n",
        "    print(f\"Desired power: {power}\")\n",
        "\n",
        "    # Function to calculate power for given r and n\n",
        "    def calculate_power(r, n, alpha=0.05):\n",
        "        if n <= 3:\n",
        "            return 0.0\n",
        "\n",
        "        # Calculate non-centrality parameter\n",
        "        z_r = fishers_z_transform(r)\n",
        "        se_z = 1 / np.sqrt(n - 3)\n",
        "\n",
        "        # Critical value\n",
        "        z_critical = stats.norm.ppf(1 - alpha/2)\n",
        "\n",
        "        # Power calculation using non-central distribution\n",
        "        # This is an approximation\n",
        "        delta = abs(z_r) / se_z\n",
        "        power = 1 - stats.norm.cdf(z_critical - delta) + stats.norm.cdf(-z_critical - delta)\n",
        "\n",
        "        return min(power, 1.0)\n",
        "\n",
        "    # Function to calculate required sample size\n",
        "    def calculate_required_n(r, power=0.8, alpha=0.05):\n",
        "        if abs(r) < 0.01:\n",
        "            return float('inf')\n",
        "\n",
        "        z_alpha = stats.norm.ppf(1 - alpha/2)\n",
        "        z_beta = stats.norm.ppf(power)\n",
        "\n",
        "        # Fisher's Z transformation\n",
        "        z_r = fishers_z_transform(r)\n",
        "\n",
        "        # Required sample size formula\n",
        "        n_required = ((z_alpha + z_beta) / z_r)**2 + 3\n",
        "\n",
        "        return max(4, int(np.ceil(n_required)))\n",
        "\n",
        "    # Calculate power curves\n",
        "    print(f\"\\nSample size requirements for {power*100:.0f}% power:\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    required_ns = []\n",
        "    for r in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "        req_n = calculate_required_n(r, power, alpha)\n",
        "        required_ns.append(req_n)\n",
        "        print(f\"r = {r:.1f}: n = {req_n:>4}\")\n",
        "\n",
        "    # Create power analysis visualizations\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Power Analysis for Correlation Studies', fontsize=16)\n",
        "\n",
        "    # Plot 1: Power vs Sample Size for different effect sizes\n",
        "    for r in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "        powers = [calculate_power(r, n, alpha) for n in sample_sizes]\n",
        "        ax1.plot(sample_sizes, powers, label=f'r = {r:.1f}', linewidth=2)\n",
        "\n",
        "    ax1.axhline(y=power, color='red', linestyle='--', label=f'{power*100:.0f}% Power')\n",
        "    ax1.set_xlabel('Sample Size')\n",
        "    ax1.set_ylabel('Statistical Power')\n",
        "    ax1.set_title('Power vs Sample Size')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim(0, 1)\n",
        "\n",
        "    # Plot 2: Required Sample Size vs Effect Size\n",
        "    effect_range = np.arange(0.1, 0.8, 0.05)\n",
        "    required_sample_sizes = [calculate_required_n(r, power, alpha) for r in effect_range]\n",
        "\n",
        "    # Cap extremely large sample sizes for visualization\n",
        "    required_sample_sizes = [min(n, 1000) for n in required_sample_sizes]\n",
        "\n",
        "    ax2.plot(effect_range, required_sample_sizes, 'b-', linewidth=2, marker='o')\n",
        "    ax2.set_xlabel('Effect Size (|r|)')\n",
        "    ax2.set_ylabel('Required Sample Size')\n",
        "    ax2.set_title(f'Sample Size for {power*100:.0f}% Power')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add Cohen's guidelines\n",
        "    ax2.axvline(x=0.1, color='green', linestyle=':', alpha=0.7, label='Small')\n",
        "    ax2.axvline(x=0.3, color='orange', linestyle=':', alpha=0.7, label='Medium')\n",
        "    ax2.axvline(x=0.5, color='red', linestyle=':', alpha=0.7, label='Large')\n",
        "    ax2.legend(title='Effect Size')\n",
        "\n",
        "    # Plot 3: Power vs Effect Size for different sample sizes\n",
        "    sample_size_examples = [30, 50, 100, 200]\n",
        "    effect_range_fine = np.arange(0.05, 0.8, 0.02)\n",
        "\n",
        "    for n in sample_size_examples:\n",
        "        powers = [calculate_power(r, n, alpha) for r in effect_range_fine]\n",
        "        ax3.plot(effect_range_fine, powers, label=f'n = {n}', linewidth=2)\n",
        "\n",
        "    ax3.axhline(y=power, color='red', linestyle='--', label=f'{power*100:.0f}% Power')\n",
        "    ax3.set_xlabel('Effect Size (|r|)')\n",
        "    ax3.set_ylabel('Statistical Power')\n",
        "    ax3.set_title('Power vs Effect Size')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.set_ylim(0, 1)\n",
        "\n",
        "    # Plot 4: Minimum detectable effect size\n",
        "    min_detectable_r = []\n",
        "    for n in sample_sizes:\n",
        "        # Binary search for minimum r that achieves desired power\n",
        "        low, high = 0.01, 0.99\n",
        "        while high - low > 0.001:\n",
        "            mid = (low + high) / 2\n",
        "            if calculate_power(mid, n, alpha) >= power:\n",
        "                high = mid\n",
        "            else:\n",
        "                low = mid\n",
        "        min_detectable_r.append(high)\n",
        "\n",
        "    ax4.plot(sample_sizes, min_detectable_r, 'g-', linewidth=2, marker='s')\n",
        "    ax4.set_xlabel('Sample Size')\n",
        "    ax4.set_ylabel('Minimum Detectable Effect Size')\n",
        "    ax4.set_title(f'Minimum Detectable |r| for {power*100:.0f}% Power')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add Cohen's guidelines\n",
        "    ax4.axhline(y=0.1, color='green', linestyle=':', alpha=0.7, label='Small')\n",
        "    ax4.axhline(y=0.3, color='orange', linestyle=':', alpha=0.7, label='Medium')\n",
        "    ax4.axhline(y=0.5, color='red', linestyle=':', alpha=0.7, label='Large')\n",
        "    ax4.legend(title='Effect Size')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'effect_sizes': effect_sizes,\n",
        "        'sample_sizes': sample_sizes,\n",
        "        'required_sample_sizes': dict(zip([0.1, 0.2, 0.3, 0.4, 0.5], required_ns)),\n",
        "        'power_function': calculate_power,\n",
        "        'sample_size_function': calculate_required_n\n",
        "    }\n",
        "\n",
        "def practical_correlation_reporting_guide(correlation_analysis):\n",
        "    \"\"\"\n",
        "    Provide practical guide for reporting correlation results.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"PRACTICAL GUIDE FOR REPORTING CORRELATIONS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    detailed_results = correlation_analysis['detailed_results']\n",
        "\n",
        "    print(f\"Essential elements for reporting correlations:\")\n",
        "    print(\"-\" * 45)\n",
        "    print(f\"1. Correlation coefficient type (Pearson/Spearman)\")\n",
        "    print(f\"2. Sample size and degrees of freedom\")\n",
        "    print(f\"3. Confidence intervals\")\n",
        "    print(f\"4. Statistical significance\")\n",
        "    print(f\"5. Effect size interpretation\")\n",
        "    print(f\"6. Multiple comparisons correction (if applicable)\")\n",
        "\n",
        "    print(f\"\\nExample reporting formats:\")\n",
        "    print(\"-\" * 27)\n",
        "\n",
        "    # Select a few examples for demonstration\n",
        "    example_results = detailed_results[:3]  # First 3 correlations\n",
        "\n",
        "    for i, result in enumerate(example_results, 1):\n",
        "        var1 = result['var1'].replace('_', ' ')\n",
        "        var2 = result['var2'].replace('_', ' ')\n",
        "        r = result['correlation']\n",
        "        p = result['p_value']\n",
        "        ci_lower = result['ci_lower']\n",
        "        ci_upper = result['ci_upper']\n",
        "        n = result['sample_size']\n",
        "\n",
        "        # Determine significance stars\n",
        "        if p < 0.001:\n",
        "            sig_text = \"p < .001\"\n",
        "        elif p < 0.01:\n",
        "            sig_text = f\"p = {p:.3f}\"\n",
        "        elif p < 0.05:\n",
        "            sig_text = f\"p = {p:.3f}\"\n",
        "        else:\n",
        "            sig_text = f\"p = {p:.3f}\"\n",
        "\n",
        "        # Effect size interpretation\n",
        "        if abs(r) >= 0.5:\n",
        "            effect_desc = \"large\"\n",
        "        elif abs(r) >= 0.3:\n",
        "            effect_desc = \"medium\"\n",
        "        elif abs(r) >= 0.1:\n",
        "            effect_desc = \"small\"\n",
        "        else:\n",
        "            effect_desc = \"negligible\"\n",
        "\n",
        "        direction = \"positive\" if r > 0 else \"negative\"\n",
        "\n",
        "        print(f\"\\nExample {i}:\")\n",
        "        print(f\"Standard format:\")\n",
        "        print(f\"  'There was a {effect_desc} {direction} correlation between\")\n",
        "        print(f\"   {var1} and {var2}, r({n-2}) = {r:.3f},\")\n",
        "        print(f\"   95% CI [{ci_lower:.3f}, {ci_upper:.3f}], {sig_text}.'\")\n",
        "\n",
        "        print(f\"APA format:\")\n",
        "        print(f\"  '{var1.title()} and {var2} were significantly correlated,\")\n",
        "        print(f\"   r = {r:.2f}, {sig_text}, 95% CI [{ci_lower:.2f}, {ci_upper:.2f}].'\")\n",
        "\n",
        "    # Create summary table for all results\n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"SUMMARY TABLE FOR ALL CORRELATIONS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    summary_data = []\n",
        "    for result in detailed_results:\n",
        "        summary_data.append({\n",
        "            'Variables': f\"{result['var1']} ↔ {result['var2']}\",\n",
        "            'r': f\"{result['correlation']:.3f}\",\n",
        "            '95% CI': f\"[{result['ci_lower']:.3f}, {result['ci_upper']:.3f}]\",\n",
        "            'p-value': f\"{result['p_value']:.4f}\",\n",
        "            'n': result['sample_size'],\n",
        "            'Significant': 'Yes' if result['p_value'] < 0.05 else 'No'\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    # Multiple comparisons impact\n",
        "    print(f\"\\nMultiple comparisons correction impact:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    uncorrected_sig = sum(1 for result in detailed_results if result['p_value'] < 0.05)\n",
        "    bonferroni_sig = sum(1 for result in detailed_results if result.get('significant_bonferroni', False))\n",
        "    fdr_sig = sum(1 for result in detailed_results if result.get('significant_fdr_bh', False))\n",
        "\n",
        "    print(f\"Uncorrected significant: {uncorrected_sig}/{len(detailed_results)}\")\n",
        "    print(f\"Bonferroni significant:  {bonferroni_sig}/{len(detailed_results)}\")\n",
        "    print(f\"FDR (BH) significant:    {fdr_sig}/{len(detailed_results)}\")\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "# Execute Part 4 Analysis\n",
        "print(\"\\n EXECUTING ADVANCED CORRELATION ANALYSIS\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# For demonstration, let's use the penguin data\n",
        "test_data = penguins_clean.head()\n",
        "\n",
        "\n",
        "# Step 1: Comprehensive correlation matrix analysis\n",
        "print(\"\\n Step 1: Comprehensive correlation matrix analysis with confidence intervals\")\n",
        "comprehensive_analysis = comprehensive_correlation_matrix_analysis(penguins_clean, continuous_vars)\n",
        "\n",
        "# Step 2: Advanced visualizations\n",
        "print(\"\\n Step 2: Creating advanced correlation visualizations\")\n",
        "create_advanced_correlation_visualizations(comprehensive_analysis, continuous_vars)\n",
        "\n",
        "# Step 3: Hierarchical clustering\n",
        "print(\"\\n Step 3: Hierarchical clustering of correlation matrix\")\n",
        "clustering_results = hierarchical_clustering_correlations(\n",
        "    comprehensive_analysis['correlation_matrix'], continuous_vars\n",
        ")\n",
        "\n",
        "# Step 4: Bootstrap confidence intervals for a key correlation\n",
        "print(\"\\n Step 4: Bootstrap confidence intervals demonstration\")\n",
        "bootstrap_results = bootstrap_correlation_confidence_intervals(\n",
        "    penguins_clean, 'flipper_length_mm', 'body_mass_g', n_bootstrap=1000\n",
        ")\n",
        "\n",
        "# Step 5: Power analysis\n",
        "print(\"\\n Step 5: Power analysis for correlation studies\")\n",
        "power_analysis_results = power_analysis_correlation()\n",
        "\n",
        "# Step 6: Practical reporting guide\n",
        "print(\"\\n Step 6: Practical correlation reporting guide\")\n",
        "summary_table = practical_correlation_reporting_guide(comprehensive_analysis)\n"
      ],
      "metadata": {
        "id": "JQmTPFvWlkPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Correlation Analysis Results Summary\n",
        "\n",
        "## Key Findings: Statistical Rigor Reveals Robust Relationships\n",
        "\n",
        "### All Correlations Remain Significant After Multiple Corrections\n",
        "\n",
        "**Comprehensive Testing Results**:\n",
        "- **6 out of 6 correlations** significant across all correction methods\n",
        "- **Bonferroni, Holm, and FDR**: All maintained significance\n",
        "- **Interpretation**: These are robust, replicable relationships, not false discoveries\n",
        "\n",
        "This is remarkable - it means the penguin morphological relationships are so strong they survive even the most conservative statistical corrections.\n",
        "\n",
        "---\n",
        "\n",
        "## Individual Correlation Strengths with Confidence Intervals\n",
        "\n",
        "### Strongest Relationship: Flipper Length ↔ Body Mass\n",
        "- **r = 0.873, 95% CI [0.845, 0.896]**\n",
        "- **Effect size**: Very large (77% shared variance)\n",
        "- **Confidence interval**: Extremely narrow, indicating high precision\n",
        "- **Biological interpretation**: Body size scaling - larger penguins have proportionally longer flippers\n",
        "\n",
        "### Moderate-Strong Relationships\n",
        "**Bill Length ↔ Flipper Length**: r = 0.653, CI [0.587, 0.711]\n",
        "- Large positive relationship, consistent with overall body size scaling\n",
        "\n",
        "**Bill Length ↔ Body Mass**: r = 0.589, CI [0.515, 0.655]  \n",
        "- Large positive relationship, supporting the body size hypothesis\n",
        "\n",
        "### Moderate Negative Relationships\n",
        "**Bill Depth ↔ Flipper Length**: r = -0.578, CI [-0.645, -0.501]\n",
        "**Bill Depth ↔ Body Mass**: r = -0.472, CI [-0.552, -0.384]\n",
        "- These negative correlations reflect species differences in bill morphology\n",
        "\n",
        "### Surprising Negative Relationship\n",
        "**Bill Length ↔ Bill Depth**: r = -0.229, CI [-0.328, -0.124]\n",
        "- Small but significant negative correlation\n",
        "- Reflects the earlier finding about species-level confounding effects\n",
        "\n",
        "---\n",
        "\n",
        "## Hierarchical Clustering: Variable Structure Revealed\n",
        "\n",
        "### Clear Clustering Pattern Emerges\n",
        "\n",
        "**2-Cluster Solution**:\n",
        "- **Cluster 1**: Bill length, flipper length, body mass (size-related variables)\n",
        "- **Cluster 2**: Bill depth (stands alone)\n",
        "\n",
        "**3-Cluster Solution**:\n",
        "- **Cluster 1**: Flipper length + body mass (closest relationship)\n",
        "- **Cluster 2**: Bill length (intermediate)\n",
        "- **Cluster 3**: Bill depth (most distinct)\n",
        "\n",
        "**Biological Insight**: Bill depth represents a fundamentally different aspect of penguin morphology compared to general body size scaling.\n",
        "\n",
        "---\n",
        "\n",
        "## Bootstrap Validation: Robust Statistical Inference\n",
        "\n",
        "### Bootstrap vs. Fisher's Z Comparison\n",
        "**Flipper Length ↔ Body Mass** (strongest correlation):\n",
        "- **Original correlation**: r = 0.873\n",
        "- **Bootstrap mean**: r = 0.873 (virtually identical)\n",
        "- **Bootstrap bias**: -0.0001 (negligible)\n",
        "- **Bootstrap CI**: [0.847, 0.895]\n",
        "- **Fisher's Z CI**: [0.845, 0.896]\n",
        "\n",
        "**Key Finding**: Bootstrap and Fisher's Z methods produce nearly identical results, validating the traditional approach for this dataset.\n",
        "\n",
        "### Bootstrap Distribution Properties\n",
        "- **Standard error**: 0.012 (very small)\n",
        "- **Distribution shape**: Normal (confirmed by Q-Q plot)\n",
        "- **Sampling stability**: High precision with n = 333\n",
        "\n",
        "---\n",
        "\n",
        "## Power Analysis: Study Design Validation\n",
        "\n",
        "### Sample Size Adequacy Assessment\n",
        "\n",
        "**Current Study (n = 333)**:\n",
        "- **Power for large effects (r = 0.5)**: >99% (excellent)\n",
        "- **Power for medium effects (r = 0.3)**: >99% (excellent)  \n",
        "- **Power for small effects (r = 0.1)**: ~40% (underpowered)\n",
        "\n",
        "**Sample Size Requirements for 80% Power**:\n",
        "- **Small effects (r = 0.1)**: 783 participants needed\n",
        "- **Medium effects (r = 0.3)**: 85 participants needed\n",
        "- **Large effects (r = 0.5)**: 30 participants needed\n",
        "\n",
        "**Study Evaluation**: This study is excellently powered to detect medium and large effects, explaining why all significant correlations were found.\n",
        "\n",
        "---\n",
        "\n",
        "## Statistical Robustness: Multiple Lines of Evidence\n",
        "\n",
        "### Confidence Interval Patterns\n",
        "- **Narrow intervals**: High precision due to adequate sample size\n",
        "- **No intervals crossing zero**: All relationships are statistically robust\n",
        "- **Consistent effect sizes**: Similar CI widths across correlations\n",
        "\n",
        "### Effect Size Distribution\n",
        "Looking at the correlation strength distribution:\n",
        "- **16.7% Large effects** (|r| > 0.5): 1 correlation\n",
        "- **50.0% Medium effects** (0.3 < |r| < 0.5): 3 correlations  \n",
        "- **33.3% Small-Medium effects** (0.1 < |r| < 0.3): 2 correlations\n",
        "- **0% Negligible effects**: No weak relationships detected\n",
        "\n",
        "---\n",
        "\n",
        "## Multiple Comparisons: Conservative Testing Success\n",
        "\n",
        "### Why All Correlations Survived Correction\n",
        "\n",
        "**Bonferroni Correction** (most conservative):\n",
        "- **Adjusted α**: 0.05/6 = 0.008\n",
        "- **Result**: All p-values < 0.001, well below threshold\n",
        "- **Interpretation**: Effects are so strong they survive harsh correction\n",
        "\n",
        "**FDR Correction** (controls false discovery rate):\n",
        "- **More liberal**: Allows some false positives\n",
        "- **Result**: All correlations still significant\n",
        "- **Interpretation**: High confidence in all discovered relationships\n",
        "\n",
        "---\n",
        "\n",
        "## Practical Implications\n",
        "\n",
        "### Research Design Lessons\n",
        "\n",
        "**Sample Size Planning**:\n",
        "- **n = 333 is excellent** for detecting medium-to-large correlations\n",
        "- **For small effects**: Would need ~800 participants\n",
        "- **Cost-benefit**: Current design efficiently captures meaningful relationships\n",
        "\n",
        "**Statistical Approach Validation**:\n",
        "- **Traditional methods work well** (Fisher's Z ≈ Bootstrap)\n",
        "- **Multiple comparisons matter** but effects were strong enough to survive\n",
        "- **Confidence intervals provide crucial precision estimates**\n",
        "\n",
        "### Biological Insights Confirmed\n",
        "\n",
        "**Body Size Scaling**: The strongest relationships (flipper-mass, bill length-mass) reflect coordinated growth patterns in penguin morphology.\n",
        "\n",
        "**Bill Depth Uniqueness**: Clustering analysis confirms bill depth measures something fundamentally different from general body size.\n",
        "\n",
        "**Species Effects**: The negative bill length-depth correlation likely reflects species-level trade-offs in bill morphology.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lBV8Mdddg7pb"
      }
    }
  ]
}